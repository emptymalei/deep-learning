{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Time Series Forecasting using Deep Learning", "text": "<p>Pre-order my new book: Time Series with PyTorch: Modern Deep Learning Toolkit for Real-World Forecasting Challenges.</p> <p>Forecasting the future is an extremely valuable superpower. The forecasting game has been dominated by statisticians who are real experts in time series problems. As the amount of data increases, many of the statistical methods are not squeezing enough out of the massive datasets. Consequently, time series forecasting using deep learning emerges and became a fast-growing field. It is trendy, not only in LinkedIn debates but also in academic papers. We plotted the number of related publications per year using the keyword \"deep learning forecasting\" on dimensions.ai) <sup>2</sup>.</p> <p> </p> This chart is obtained on 2022-08-06, from Digital Science\u2019s Dimensions platform, available at https://app.dimensions.ai <p>On the other hand, deep learning methods are not yet winning all the games of forecasting. Time series forecasting is a complicated problem with a great variety of data generating processes (DGP). Some argue that we don't need deep learning to forecast since well tuned statistical models and trees are already performing well and are faster and more interpretable than deep neural networks<sup>3</sup> <sup>4</sup>. Ensembles of statistical models performing great, even outperforming many deep learning models on the M3 data<sup>1</sup>.</p> <p>However, deep learning models are picking up speed. In the M5 competition, deep learning \"have shown forecasting potential, motivating further research in this direction\"<sup>5</sup>. As the complexity and size of time series data are growing and more and more deep learning forecasting models are being developed, forecasting with deep learning is on the path to be an important alternative to statistical forecasting methods.</p> <p>In Coding Tips, we provide coding tips to help some readers set up the development environment. In Deep Learning Fundamentals, we introduce the fundamentals of deep neural networks and their practices. For completeness, we also provide code and derivations for the models. With these two parts, we introduce time series data and statistical forecasting models in Time Series Forecasting Fundamentals, where we discuss methods to analyze time series data, several universal data generating processes of time series data, and some statistical forecasting methods. Finally, we fulfill our promise in the title in Time Series Forecasting with Deep Learning.</p>"}, {"location": "#blueprint", "title": "Blueprint", "text": "<p>The following is my first version of the blueprint.</p> <ul> <li> Engineering Tips<ul> <li> Environment, VSCode, Git, ...</li> <li> Python Project Tips</li> </ul> </li> <li> Fundamentals of Time Series Forecasting<ul> <li> Time Series Data and Terminologies</li> <li> Transformation of Time Series</li> <li> Two-way Fixed Effects</li> <li> Time Delayed Embedding</li> <li> Data Generating Process (DGP)</li> <li> DGP: Langevin Equation</li> <li> Kindergarten Models for Time Series Forecasting<ul> <li> Statistical Models</li> <li> Statistical Model: AR</li> <li> Statistical Model: VAR</li> </ul> </li> <li> Synthetic Datasets<ul> <li> Synthetic Time Series</li> <li> Creating Synthetic Dataset</li> </ul> </li> <li> Data Augmentation</li> <li> Forecasting<ul> <li> Time Series Forecasting Tasks</li> <li> Naive Forecasts</li> </ul> </li> <li> Evaluation and Metrics<ul> <li> Time Series Forecasting Evaluation</li> <li> Time Series Forecasting Metrics<ul> <li> CRPS</li> </ul> </li> </ul> </li> <li> Hierarchical Time Series<ul> <li> Hierarchical Time Series Data</li> <li> Hierarchical Time Series Reconciliation</li> </ul> </li> <li> Some Useful Datasets</li> </ul> </li> <li> Trees<ul> <li> Tree-based Models</li> <li> Random Forest</li> <li> Gradient Boosted Trees</li> <li> Forecasting with Trees</li> </ul> </li> <li> Fundamentals of Deep Learning<ul> <li> Deep Learning Introduction</li> <li> Learning from Data</li> <li> Neural Networks</li> <li> Recurrent Neural Networks</li> <li> Convolutional Neural Networks</li> <li> Transformers</li> <li> Dynamical Systems<ul> <li> Why Dynamical Systems</li> <li> Neural ODE</li> </ul> </li> <li> Energy-based Models<ul> <li> Diffusion Models</li> </ul> </li> <li> Generative Models<ul> <li> Autoregressive Model</li> <li> Auto-Encoder</li> <li> Variational Auto-Encoder</li> <li> Flow</li> <li> Generative Adversarial Network (GAN)</li> </ul> </li> </ul> </li> <li> Time Series Forecasting with Deep Learning<ul> <li> A Few Datasets</li> <li> Forecasting with MLP</li> <li> Forecasting with RNN</li> <li> Forecasting with Transformers<ul> <li> TFT</li> <li> DLinear</li> <li> NLinear</li> </ul> </li> <li> Forecasting with CNN</li> <li> Forecasting with VAE</li> <li> Forecasting with Flow</li> <li> Forecasting with GAN</li> <li> Forecasting with Neural ODE</li> <li> Forecasting with Diffusion Models</li> </ul> </li> <li> Extras Topics, Supplementary Concepts, and Code<ul> <li> DTW and DBA</li> <li> f-GAN</li> <li> Info-GAN</li> <li> Spatial-temporal Models, e.g., GNN</li> <li> Conformal Prediction</li> <li> Graph Neural Networks</li> <li> Spiking Neural Networks</li> <li> Deep Infomax</li> <li> Contrastive Predictive Coding</li> <li> MADE</li> <li> MAF</li> <li> ...</li> </ul> </li> </ul> <ol> <li> <p>Nixtla. statsforecast/experiments/m3 at main \u00b7 Nixtla/statsforecast. In: GitHub [Internet]. [cited 12 Dec 2022]. Available: https://github.com/Nixtla/statsforecast/tree/main/experiments/m3 \u21a9</p> </li> <li> <p>Hook DW, Porter SJ, Herzog C. Dimensions: Building context for search and evaluation. Frontiers in Research Metrics and Analytics 2018; 3: 23.\u00a0\u21a9</p> </li> <li> <p>Elsayed S, Thyssens D, Rashed A, Jomaa HS, Schmidt-Thieme L. Do we really need deep learning models for time series forecasting? 2021. doi:10.48550/ARXIV.2101.02118.\u00a0\u21a9</p> </li> <li> <p>Grinsztajn L, Oyallon E, Varoquaux G. Why do tree-based models still outperform deep learning on tabular data? 2022. doi:10.48550/ARXIV.2207.08815.\u00a0\u21a9</p> </li> <li> <p>Makridakis S, Spiliotis E, Assimakopoulos V. M5 accuracy competition: Results, findings, and conclusions. International journal of forecasting 2022; 38: 1346\u20131364.\u00a0\u21a9</p> </li> </ol>"}, {"location": "supplementary/", "title": "Supplementary Materials", "text": "<p>In this part, we showcase some supplementary materials such as jupyter notebooks.</p>"}, {"location": "tags/", "title": "Tags", "text": "<p>Following is a list of tags:</p>"}, {"location": "tags/#wip", "title": "WIP", "text": "<ul> <li>Convolutional Neural Networks</li> <li>f-GAN</li> <li>GAN</li> <li>InfoGAN</li> <li>Adversarial Models</li> <li>Contrastive Predictive Coding</li> <li>Deep Infomax</li> <li>Introduction</li> <li>AE</li> <li>Autoregressive</li> <li>Flow</li> <li>Introduction</li> <li>MADE</li> <li>MAF</li> <li>VAE</li> <li>Creating Synthetic Dataset</li> <li>Forecasting with CNN</li> <li>Pendulum Dataset</li> <li>Forecasting with Diffusion Models</li> <li>Forecasting with MLP</li> <li>Forecasting with Flow</li> <li>Forecasting with GAN</li> <li>Forecasting with Neural ODE</li> <li>Forecasting with RNN</li> <li>Forecasting with Transformers</li> <li>Forecasting with VAE</li> </ul>"}, {"location": "concepts/", "title": "Appendices", "text": "<p>Many formulae and concepts are reused again and again in different models. In this chapter, we will provide support for such concepts.</p>"}, {"location": "concepts/alignment-and-uniformity/", "title": "Alignment and Uniformity", "text": "<p>A good representation should be able to</p> <ul> <li>separate different instances, and</li> <li>cluster similar instances.</li> </ul> <p>Wang et al proposed two concepts that matches the above two ideas, alignment and uniformity, on a hypersphere<sup>1</sup>.</p> <p></p> <p>From Wang et al<sup>1</sup>.</p> <ol> <li> <p>Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242 \u21a9\u21a9</p> </li> </ol>"}, {"location": "concepts/elbo/", "title": "ELBO", "text": "<p>Given a probability distribution density \\(p(x)\\) and a latent variable \\(z\\), the marginalization of the joint probability is</p> \\[ \\int \\mathrm dz p(x, z) = p(x). \\]"}, {"location": "concepts/elbo/#using-jensens-inequality", "title": "Using Jensen's Inequality", "text": "<p>In many models, we are interested in the log probability density \\(\\log p(X)\\) which can be decomposed using an auxiliary density of the latent variable \\(q(Z)\\),</p> \\[ \\begin{align} \\log p(x) =&amp; \\log \\int \\mathrm d z p(x, z) \\\\ =&amp; \\log \\int \\mathrm d z p(x, z) \\frac{q(z)}{q(z)} \\\\ =&amp; \\log \\int \\mathrm d z q(x) \\frac{p(x, z)}{q(z)} \\\\ =&amp; \\log \\mathbb E_q \\left[ \\frac{p(x, z)}{q(z)} \\right]. \\end{align} \\] <p>Jensen's Inequality</p> <p>Jensen's inequality shows that<sup>1</sup></p> \\[ \\log \\mathbb E_q \\left[ \\frac{p(x, z)}{q(Z)} \\right] \\geq  \\mathbb E_q \\left[ \\log\\left(\\frac{p(x, z)}{q(Z)}\\right) \\right], \\] <p>as \\(\\log\\) is a concave function.</p> <p>Applying Jensen's inequality,</p> \\[ \\begin{align} \\log p(x) =&amp; \\log \\mathbb E_q \\left[ \\frac{p(x, z)}{q(z)} \\right] \\\\ \\geq&amp;  \\mathbb E_q \\left[ \\log\\left(\\frac{p(x, z)}{q(z)}\\right) \\right] \\\\ =&amp; \\mathbb E_q \\left[ \\log p(x, z)- \\log q(z) \\right] \\\\ =&amp; \\mathbb E_q \\left[ \\log p(x, z) \\right] - \\mathbb E_q \\left[ \\log q(z) \\right] . \\end{align} \\] <p>Using the definition of entropy and cross entropy, we know that</p> \\[ H(q(z)) = - \\mathbb E_q \\left[ \\log q(z) \\right] \\] <p>is the entropy of \\(q(z)\\), and</p> \\[ H(q(z);p(x,z)) = -\\mathbb E_q \\left[ \\log p(x, z) \\right] \\] <p>is the cross entropy. We define</p> \\[ L = \\mathbb E_q \\left[ \\log p(x, z) \\right] - \\mathbb E_q \\left[ \\log q(z) \\right] = - H(q(z);p(x,z)) + H(q(z)), \\] <p>which is called the evidence lower bound (ELBO). It is a lower bound because</p> \\[ \\log p(x) \\geq L. \\]"}, {"location": "concepts/elbo/#using-kl-divergence", "title": "Using KL Divergence", "text": "<p>In a latent variable model, we need the posterior \\(p(z|x)\\). When this is intractable, we find an approximation \\(q(z|\\theta)\\) where \\(\\theta\\) is the parametrization, e.g., neural network parameters. To make sure we have a good approximation of the posterior, we require the KL divergence of \\(q(z|\\theta)\\) and \\(p(z|z)\\) to be small. The KL divergence in this situation is<sup>2</sup></p> \\[ \\begin{align} &amp;\\operatorname{ D}_\\text{KL}(q(z|\\theta)\\parallel p(z|x)) \\\\ =&amp; -\\mathbb E_q \\log\\frac{p(z|x)}{q(z|\\theta)} \\\\ =&amp; -\\mathbb E_q \\log\\frac{p(x, z)/p(x)}{q(z|\\theta)} \\\\ =&amp; -\\mathbb E_q \\log\\frac{p(x, z)}{q(z|\\theta)} - \\mathbb E_q \\log\\frac{1}{p(x)} \\\\ =&amp; - L + \\log p(x). \\end{align} \\] <p>Since \\(\\operatorname{D}_{\\text{KL}}(q(z|\\theta)\\parallel p(z|x))\\geq 0\\), we have</p> \\[ \\log p(x) \\geq L, \\] <p>which also indicates that \\(L\\) is the lower bound of \\(\\log p(x)\\).</p> <p>Jensen gap</p> <p>The difference between \\(\\log p(x)\\) and \\(L\\) is the Jensen gap, i.e.,</p> \\[ L - \\log p(x) = - \\operatorname{D}_\\text{KL}(q(z|\\theta)\\parallel p(z|x)). \\] <ol> <li> <p>Contributors to Wikimedia projects. Jensen\u2019s inequality. In: Wikipedia [Internet]. 27 Aug 2021 [cited 5 Sep 2021]. Available: https://en.wikipedia.org/wiki/Jensen%27s_inequality \u21a9</p> </li> <li> <p>Yang X. Understanding the Variational Lower Bound. 14 Apr 2017 [cited 5 Sep 2021]. Available: https://xyang35.github.io/2017/04/14/variational-lower-bound/ \u21a9</p> </li> </ol>"}, {"location": "concepts/entropy/", "title": "Entropy", "text": ""}, {"location": "concepts/entropy/#shannon-entropy", "title": "Shannon Entropy", "text": "<p>Shannon entropy \\(S\\) is the expectation of information content \\(I(X)=-\\log \\left(p\\right)\\)<sup>1</sup>,</p> \\[\\begin{equation} H(p) = \\mathbb E_{p}\\left[ -\\log \\left(p\\right) \\right]. \\end{equation}\\]"}, {"location": "concepts/entropy/#cross-entropy", "title": "Cross Entropy", "text": "<p>Cross entropy is<sup>2</sup></p> \\[ H(p, q) = \\mathbb E_{p} \\left[ -\\log q \\right]. \\] <p>Cross entropy \\(H(p, q)\\) can also be decomposed,</p> \\[ H(p, q) = H(p) + \\operatorname{D}_{\\mathrm{KL}} \\left( p \\parallel q \\right), \\] <p>where \\(H(p)\\) is the entropy of \\(P\\) and \\(\\operatorname{D}_{\\mathrm{KL}}\\) is the KL Divergence.</p> <p>Cross entropy is widely used in classification problems, e.g., logistic regression.</p> <ol> <li> <p>Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory)\u00a0\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Cross entropy. In: Wikipedia [Internet]. 4 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Cross_entropy \u21a9</p> </li> </ol>"}, {"location": "concepts/f-divergence/", "title": "f-Divergence", "text": "<p>The f-divergence is defined as<sup>1</sup></p> \\[ \\operatorname{D}_f = \\int f\\left(\\frac{p}{q}\\right) q\\mathrm d\\mu, \\] <p>where \\(p\\) and \\(q\\) are two densities and \\(\\mu\\) is a reference distribution.</p> <p>Requirements on the generating function</p> <p>The generating function \\(f\\) is required to</p> <ul> <li>be convex, and</li> <li>\\(f(1) =0\\).</li> </ul> <p>For \\(f(x) = x \\log x\\) with \\(x=p/q\\), f-divergence is reduced to the KL divergence</p> \\[ \\begin{align} &amp;\\int f\\left(\\frac{p}{q}\\right) q\\mathrm d\\mu \\\\ =&amp; \\int \\frac{p}{q} \\log \\left( \\frac{p}{q} \\right) \\mathrm d\\mu \\\\ =&amp;  \\int p \\log \\left( \\frac{p}{q} \\right) \\mathrm d\\mu. \\end{align} \\] <p>For more special cases of f-divergence, please refer to wikipedia<sup>1</sup>. Nowozin 2016 also provides a concise review of f-divergence<sup>2</sup>.</p> <ol> <li> <p>Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence \u21a9\u21a9</p> </li> <li> <p>Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709 \u21a9</p> </li> </ol>"}, {"location": "concepts/generalization/", "title": "Generalization", "text": "<p>To measure the generalization, we define a generalization error<sup>1</sup>,</p> \\[ \\mathcal G = \\mathcal L_{P}(\\hat f) - \\mathcal L_E(\\hat f), \\] <p>where \\(\\mathcal L_{P}\\) is the population loss, \\(\\mathcal L_E\\) is the empirical loss, and \\(\\hat f\\) is our model by minimizing the empirical loss.</p> <p>However, we do not know the actual joint probability \\(p(x, y)\\) of our dataset \\(\\\\{x_i, y_i\\\\}\\). Thus the population loss is not known. In machine learning, we usually use cross validation where we split our dataset into train and test dataset. We approximate the population loss using the test dataset.</p> <ol> <li> <p>Roelofs R. Measuring generalization and overfitting in machine learning. 2019.https://escholarship.org/uc/item/6j01x9mz.\u00a0\u21a9</p> </li> </ol>"}, {"location": "concepts/gini-impurity/", "title": "Gini Impurity", "text": "<p>Suppose we have a dataset \\(\\{0,1\\}^{10}\\), which has 10 records and 2 possible classes of objects \\(\\{0,1\\}\\) in each record.</p> <p>The first example we investigate is a pure 0 dataset.</p> object 0 0 0 0 0 0 0 0 0 0 0 0 <p>For such an all-0 dataset, we would like to define its impurity as 0. Same with an all-1 dataset. For a dataset with 50% of 1 and 50% of 0, we would define its impurity as max due to the symmetries between 0 and 1.</p>"}, {"location": "concepts/gini-impurity/#definition", "title": "Definition", "text": "<p>Given a dataset \\(\\{0,1,...,d\\}^n\\), the Gini impurity is calculated as</p> \\[ G = \\sum_{i \\in \\{0,1,...,d\\} } p(i)(1-p(i)), \\] <p>where \\(p(i)\\) is the probability of a randomly picked record being class \\(i\\).</p> <p>In the above example, we have two classes, \\(\\{0,1\\}\\). The probabilities are</p> \\[ \\begin{align} p(0) =&amp; 1\\\\ p(1) =&amp; 0 \\end{align}. \\] <p>The Gini impurity is</p> \\[ G = p(0)(1-p(0)) + p(1)(1-p(1)) = 0+0 = 0. \\]"}, {"location": "concepts/gini-impurity/#examples", "title": "Examples", "text": "<p>Suppose we have another dataset with 50% of the values being 50%.</p> object 0 0 1 0 0 1 1 1 0 0 0 1 <p>The Gini impurity is</p> \\[ G = p(0)(1-p(0)) + p(1)(1-p(1)) = 0.5 * 0.5+ 0.5*0.5 = 0.5. \\] <p>For data with two possible values \\(\\{0,1\\}\\), the maximum Gini impurity is 0.25. The following chart shows all the possible values of the Gini impurity for a two-value dataset.</p> <p>The following heatmap shows the Gini impurity for data with two possible values. The color indicates the Gini impurity.</p> <p></p> <p>For data with three possible values, the Gini impurity is also visualized using the same chart given the condition that \\(p_3 = 1 - p_1 - p_2\\).</p> <p>The following chart shows the Gini impurity for data with three possible values. The color indicates the Gini impurity.</p> <p></p>"}, {"location": "concepts/information-gain/", "title": "Information Gain", "text": "<p>Information gain is a frequently used metric in calculating the gain during a split in tree-based methods.</p> <p>First of all, the entropy of a dataset is defined as</p> \\[ S = - \\sum_i p_i \\log p_i - \\sum_i (1-p_i)\\log p_i, \\] <p>where \\(p_i\\) is the probability of a class.</p> <p>The information gain is the change of entropy.</p> <p>To illustrate this idea, we use decision tree as an example. In a decision tree algorithm, we would split a node. Before splitting, we assign a label \\(m\\) to the node, the entropy is</p> \\[ S_m = - p_m \\log p_m - (1-p_m)\\log p_m. \\] <p>After the splitting, we have two groups that contributes to the entropy, group \\(L\\) and group \\(R\\) <sup>1</sup>,</p> \\[ S'_m = p_L (- p_m \\log p_m - (1-p_m)\\log p_m) + p_R (- p_m \\log p_m - (1-p_m)\\log p_m), \\] <p>where \\(p_L\\) and \\(p_R\\) are the probabilities of the two groups. Suppose we have 100 samples before splitting and 29 samples in the left group and 71 samples in the right group, we have \\(p_L = 29/100\\) and \\(p_R = 71/100\\).</p> <p>The information gain is the difference between \\(S_m\\) and \\(S'_m\\),</p> \\[ \\mathrm{Gain} = S_m - S'_m. \\] <ol> <li> <p>Shalev-Shwartz S, Ben-David S. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014 doi:10.1017/CBO9781107298019.\u00a0\u21a9</p> </li> </ol>"}, {"location": "concepts/kl-divergence/", "title": "KL Divergence", "text": "<p>The Kullback\u2013Leibler (KL) divergence is defined as</p> \\[ \\begin{align} \\operatorname{D}_\\mathrm{KL}(p \\parallel q ) =&amp; \\mathbb E_{p} \\left[\\log\\left(\\frac{p}{q}\\right) \\right] \\\\ =&amp; \\int_{-\\infty}^\\infty p \\log\\left(\\frac{p}{q}\\right)\\, dx . \\end{align} \\] <p>Suppose \\(p\\) is a Gaussian distribution and \\(q\\) is a bimodal Gaussian mixture, the KL divergence \\(\\operatorname{D}_\\mathrm{KL}(p \\parallel q )\\) and \\(\\operatorname{D}_\\mathrm{KL}(q \\parallel p )\\) are different as KL divergence is not necessarily symmetric. Thus the KL divergence is not a proper distance definition.</p> <p></p> <p>KL divergence is a special case of f-divergence.</p>"}, {"location": "concepts/mutual-information/", "title": "Mutual Information", "text": "<p>Mutual information is</p> \\[ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}. \\] <p>Mutual information is closed related to KL divergence,</p> \\[ I(X;Y) = D_{\\mathrm{KL}} \\left(  P_{XY}(x,y) \\parallel  P_X(x) P_{Y}(y) \\right). \\]"}, {"location": "concepts/nce/", "title": "NCE", "text": ""}, {"location": "concepts/timeseries-data.dtw-barycenter-averaging/", "title": "DTW Barycenter Averaging", "text": "<p>DTW Barycenter Averaging (DBA) constructs a series \\(\\bar{\\mathcal S}\\) out of a set of series \\(\\{\\mathcal S^{(\\alpha)}\\}\\) so that \\(\\bar{\\mathcal S}\\) is the barycenter of \\(\\{\\mathcal S^{(\\alpha)}\\}\\) measured by Dynamic Time Warping (DTW) distance <sup>2</sup>.</p>"}, {"location": "concepts/timeseries-data.dtw-barycenter-averaging/#barycenter-averaging-based-on-dtw-distance", "title": "Barycenter Averaging Based on DTW Distance", "text": "<p>Petitjean et al proposed a time series averaging algorithm based on DTW distance which is dubbed DTW Barycenter Averaging (DBA).</p> <p>DBA Implementation</p> <p>https://github.com/fpetitjean/DBA</p> <ol> <li> <p>trekhleb. javascript-algorithms/src/algorithms/string/levenshtein-distance at master \u00b7 trekhleb/javascript-algorithms. In: GitHub [Internet]. [cited 27 Jul 2022]. Available: https://github.com/trekhleb/javascript-algorithms/tree/master/src/algorithms/string/levenshtein-distance \u21a9</p> </li> <li> <p>Petitjean F, Ketterlin A, Gan\u00e7arski P. A global averaging method for dynamic time warping, with applications to clustering. Pattern recognition 2011; 44: 678\u2013693.\u00a0\u21a9</p> </li> </ol>"}, {"location": "concepts/timeseries-data.dtw/", "title": "Dynamic Time Warping (DTW)", "text": "<p>Given two sequences, \\(S^{(1)}\\) and \\(S^{(2)}\\), the Dynamic Time Warping (DTW) algorithm finds the best way to align two sequences. During this alignment process, we quantify the misalignment using a distance similar to the Levenshtein distance, where the distance between two series \\(S^{(1)}_{1:i}\\) (with \\(i\\) elements) and \\(S^{(2)}_{1:j}\\) (with \\(j\\) elements) is<sup>3</sup></p> \\[ \\begin{align} D(S^{(1)}_{1:i}, S^{(2)}_{1:j}) =&amp; d(S^{(1)}_i, S^{(2)}_j)\\\\ &amp; + \\operatorname{min}\\left[ D(S^{(1)}_{1:i-1}, S^{(2)}_{1:j-1}), D(S^{(1)}_{1:i}, S^{(2)}_{1:j-1}), D(S^{(1)}_{1:i-1}, S^{(2)}_{1:j}) \\right], \\end{align} \\] <p>where \\(S^{(1)}_i\\) is the \\(i\\)the element of the series \\(S^{(1)}\\), \\(d(x,y)\\) is a predetermined distance, e.g., Euclidean distance. This definition reveals the recursive nature of the DTW distance.</p> Notations in the Definition: \\(S_{1:i}\\) and \\(S_{i}\\) <p>The notation \\(S_{1:i}\\) stands for a series that contains the elements starting from the first to the \\(i\\)th in series \\(S\\). For example, we have a series</p> \\[ S^1 = [s^1_1, s^1_2, s^1_3, s^1_4, s^1_5, s^1_6]. \\] <p>The notation \\(S^1_{1:4}\\) represents</p> \\[ S^1_{1:4} = [s^1_1, s^1_2, s^1_3, s^1_4]. \\] <p>The notation \\(S_i\\) indicates the \\(i\\)th element in \\(S\\). For example,</p> \\[ S^1_4 = s^1_4. \\] <p>If we map these two notations to Python,</p> <ul> <li>\\(S_{1:i}\\) is equivalent to <code>S[0:i]</code>, and</li> <li>\\(S_i\\) is equivalent to <code>S[i-1]</code>.</li> </ul> <p>Note that the indices in Python look strange. This is also the reason we choose to use subscripts not square brackets in our definition.</p> Levenshtein Distance <p>Given two words, e.g., \\(w^{a} = \\mathrm{cats}\\) and \\(w^{b} = \\mathrm{katz}\\). Suppose we can only use three operations: insertions, deletions and substitutions. The Levenshtein distance calculates the number of such operations needed to change from the first word \\(w^a\\) to the second one \\(w^b\\) by applying single-character edits. In this example, we need two replacements, i.e., <code>\"c\" -&gt; \"k\"</code> and <code>\"s\" -&gt; \"z\"</code>.</p> <p>The Levenshtein distance can be solved using recursive algorithms <sup>1</sup>.</p> <p>DTW is very useful when comparing series with different lengths. For example, most error metrics require the actual time series and predicted series to have the same length. In the case of different lengths, we can perform DTW when calculating these metrics<sup>2</sup>.</p>"}, {"location": "concepts/timeseries-data.dtw/#examples", "title": "Examples", "text": "<p>The forecasting package darts provides a demo of DTW.</p> <ol> <li> <p>trekhleb. javascript-algorithms/src/algorithms/string/levenshtein-distance at master \u00b7 trekhleb/javascript-algorithms. In: GitHub [Internet]. [cited 27 Jul 2022]. Available: https://github.com/trekhleb/javascript-algorithms/tree/master/src/algorithms/string/levenshtein-distance \u21a9</p> </li> <li> <p>Unit8. Metrics \u2014 darts\u00a0 documentation. In: Darts [Internet]. [cited 7 Mar 2023]. Available: https://unit8co.github.io/darts/generated_api/darts.metrics.metrics.html?highlight=dtw#darts.metrics.metrics.dtw_metric \u21a9</p> </li> <li> <p>Petitjean F, Ketterlin A, Gan\u00e7arski P. A global averaging method for dynamic time warping, with applications to clustering. Pattern recognition 2011; 44: 678\u2013693.\u00a0\u21a9</p> </li> </ol>"}, {"location": "deep-learning-fundamentals/", "title": "Deep Learning Fundamentals", "text": "<p>Deep learning, as the rising method for time series forecasting, requires the knowledge of some fundamental principles.</p> <p>In this part, we explain and demonstrate some popular deep learning models. Note that we do not intend to cover all models but only discuss a few popular principles.</p> <p>The simplest deep learning model, is a fully connected Feedforward Neural Network (FFNN). A FFNN might work for in-distribution predictions, it is likely to overfit and perform poorly for out-of-distribution predictions. In reality, most of the deep learning models are much more complicated than a FFNN, and a large population of deep learning models are utilizing the self-supervised learning concept, providing better generalizations<sup>1</sup>.</p> <p>In the following chapters, we provide some popular deep learning architectures and cool ideas.</p> <p>Notations</p> <p>In this document, we use the following notations.</p> <ul> <li>Sets, domains, abstract variables, \\(X\\), \\(Y\\);</li> <li>Probability distribution \\(P\\), \\(Q\\);</li> <li>Probability density \\(p\\), \\(q\\);</li> <li>Slicing arrays from index \\(i\\) to index \\(j\\) using \\({}_{i:j}\\).</li> </ul> <ol> <li> <p>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J et al. Self-supervised learning: Generative or contrastive. 2020.http://arxiv.org/abs/2006.08218.\u00a0\u21a9</p> </li> </ol>"}, {"location": "deep-learning-fundamentals/convolutional-neural-networks/", "title": "Convolutional Neural Networks", "text": "", "tags": ["WIP"]}, {"location": "deep-learning-fundamentals/learning/", "title": "Learning from Data", "text": "<p>Learning from data is a practice of extracting compressed knowledge about the world from data. There are many frameworks of learning. For example, the induction, deduction, and transduction schema shows different possible paths to produce predictions.</p> <pre><code>graph LR\n    P[Prediction]\n    D[Data]\n    M[Model]\n    D --\"Induction\"--&gt; M\n    M --\"Deduction\"--&gt; P\n    D --\"Transduction\"--&gt; P</code></pre> <p>There are two different approaches to making predictions based on some given data.</p> <ol> <li>Perform induction to find a good model from the data, this is called induction. Once we have a model, we can use it to make predictions, this is called deduction.</li> <li>Directly make predictions from the data, this is called transduction.</li> </ol> <p>The Nature of Statistical Learning Theory</p> <p>Vapnik's seminal book The Nature of Statistical Learning Theory is a very good read for the fundamentals of learning theories<sup>1</sup>.</p> <p>Vapnik also discussed some of the key ideas in a book chapter Estimation of Dependences Based on Empirical Data<sup>2</sup>.</p> <p>In the context of machine learning, Abu-Mostafa, Magdon-Ismail, and Lin summarized the machine learning problem using the following chart <sup>3</sup>. Ultimately, we need to find an approximation \\(g\\) of the true map \\(f\\) from features \\(\\mathcal X\\) to targets \\(\\mathcal Y\\) on a specific probability distribution of features \\(P\\). This process is done by using an algorithm to select a hypothesis that works.</p> <pre><code>flowchart LR\n    X[Data Samples]\n    A[Algorithm]\n    H[Hypotheses Set]\n    SH[Selected Hypothesis]\n\n    X --&gt; A\n    H --&gt; A\n    A --&gt; SH</code></pre> <p>Based on this framework, a machine learning process usually consists of three core components<sup>4</sup>.</p> <ol> <li>Representation: Encoded data and the problem representation.</li> <li>Evaluation: An objective function to be evaluated that guides the model.</li> <li>Optimization: An algorithm to optimize the model so it learns what we want it to do.</li> </ol> <p>We will reuse this framework again and again in the following sections of this chapter.</p> <ol> <li> <p>Vapnik V. The nature of statistical learning theory. Springer: New York, NY, 2010 doi:10.1007/978-1-4757-3264-1.\u00a0\u21a9</p> </li> <li> <p>Vapnik V. Estimation of dependences based on empirical data. 1st ed. Springer: New York, NY, 2006 doi:10.1007/0-387-34239-7.\u00a0\u21a9</p> </li> <li> <p>Abu-Mostafa YS, Magdon-Ismail M, Lin H-T. Learning from data: A short course. AMLBook, 2012https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698.\u00a0\u21a9</p> </li> <li> <p>Domingos P. A few useful things to know about machine learning. Communications of the ACM 2012; 55: 78\u201387.\u00a0\u21a9</p> </li> </ol>"}, {"location": "deep-learning-fundamentals/neural-net/", "title": "Neural Networks", "text": "<p>Neural networks have been a buzzword for machine learning in recent years. As indicated in the name, artificial neural networks are artificial neurons connected in a network. In this section, we discuss some intuitions and theories of neural networks.</p> Artificial vs Biological <p>Neuroscientists also discuss neural networks, or neuronal networks in their research. Those are different concepts from the artificial neural networks we are going to discuss here. In this book, we use the term neural networks to refer to artificial neural networks, unless otherwise specified.</p>"}, {"location": "deep-learning-fundamentals/neural-net/#intuitions", "title": "Intuitions", "text": "<p>We start with some intuitions of neural networks before discussing the theoretical implications.</p>"}, {"location": "deep-learning-fundamentals/neural-net/#what-is-an-artificial-neuron", "title": "What is an Artificial Neuron", "text": "<p>What an artificial neuron does is respond to stimulations. This response could be strong or weak depending on the strength of the stimulations. Here is an example.</p> <p></p> <p>Using one simple single neuron, we do not have much to build. It is just the function we observe above. However, by connecting multiple neurons in a network, we could compose complicated functions and generalize the scaler function to multi-dimensional functions.</p> <p>Before we connect this neuron to a network, we study a few transformations first. The response function can be shifted, scaled, or inverted. The following figure shows the effect of these transformations.</p> <p></p>"}, {"location": "deep-learning-fundamentals/neural-net/#artificial-neural-network", "title": "Artificial Neural Network", "text": "<p>A simple network is a collection of neurons that respond to stimulations, which could come from the responses of other neurons.</p> <p></p> <p>A given input signal is spread onto three different neurons. The neurons respond to this signal separately before being combined with different weights. In the language of math, given input \\(x\\), output \\(y(x)\\) is</p> \\[y(x) = \\sum_{k=1}^{3} x v_k * \\mathrm{activation}( w_k * x + u_k )\\] <p>where \\(\\mathrm{activation}\\) is the activation function, i.e., the response behavior of the neuron. This is a single-layer structure.</p> <p>\\(\\mathrm{activation} \\to \\sigma\\)</p> <p>In the following discussions, we will use \\(\\sigma\\) as a drop in replacement for \\(\\mathrm{activation}\\).</p> <p>To extend this naive and shallow network, we could</p> <ul> <li>increase the number of neurons on one layer, i.e., go wider, or</li> <li>extend the number of layers, i.e., go deeper, or</li> <li>add interactions between neurons, or</li> <li>include recurrent connections in the network.</li> </ul> <p></p>"}, {"location": "deep-learning-fundamentals/neural-net/#composition-effects", "title": "Composition Effects", "text": "<p>To build up intuitions of how multiple neurons work together, we take an example of a network with two neurons. We will solve two problems:</p> <ol> <li>Find out if a hotel room is hot or cold.</li> <li>Find out if the hotel room is comfortable to stay.</li> </ol> <p>The first task can be solved using o single neuron. Suppose our input to the neuron is the temperature of the room. The output of the neuron is a binary value, 1 for hot and 0 for cold. The following figure shows the response of the neuron.</p> <p></p> <p>In the figure above, we use red for \"hot\" and blue for \"cold\". In this example, the temperature being \\(T_1\\) means the room is cold, while that being \\(T_2\\) and \\(T_3\\) indicate hot rooms.</p> <p>However, moving on the the second problem, such monotonic functions won't work. It is only comfortable to stay in the hotel room if the temperature is neither too high nor too low. Now consider two neurons in a network. One neuron has a monotonic increasing response to the temperature, while the other has a monotonic decreasing response. The following figure shows the combined response of the two neurons. We observe that the combined response is high only when the temperature is in a certain range.</p> <p></p> <p>Suppose we have three rooms with temperatures \\(T_1\\), \\(T_2\\), \\(T_3\\) respectively. Only \\(T_2\\) falls into the region of large output value which corresponds to the habitable temperature.</p> <p></p> <p>Mathematical Formulation</p> <p>The above example can be formulated as $$ f(x) = \\sum_k v_k \\sigma(w_k x + u_k) $$ where \\(\\sigma\\) is some sort of monotonic activation function.</p> <p>It is a form of single hidden layer feedforward network. We will discuss this in more detail in the following sections.</p> <p>These two examples hint that neural networks are good at classification tasks. Neural networks excel at a variety of tasks. Since this book is about time series, we will demonstrate the power of neural networks in time series analysis.</p>"}, {"location": "deep-learning-fundamentals/neural-net/#universal-approximators", "title": "Universal Approximators", "text": "<p>Even a single hidden layer feedforward network can approximate any measurable function, regardless of the activation function<sup>2</sup>. In the case of the commonly discussed sigmoid function as an activation function, a neural network for real numbers becomes</p> \\[ \\sum_k v_k \\sigma(w_k x + u_k) \\] <p>It is a good approximation of continuous functions<sup>3</sup>.</p> Kolmogorov's Theorem <p>Kolmogorov's theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set<sup>4</sup>.</p>"}, {"location": "deep-learning-fundamentals/neural-net/#neural-networks-can-be-complicated", "title": "Neural Networks Can be Complicated", "text": "<p>In practice, we observe a lot of problems when the number of neurons grows, e.g., the convergence during the training slows down if we have too many layers in the network (the vanishing gradient problem) <sup>5</sup>.</p> <p>Training</p> <p>We have not yet discussed how to adjust the parameters in a neural network. The process is called training. The most popular method is backpropagation<sup>1</sup>.</p> <p>The reader should understand that a good neural network model is not only about these naive examples but is about many different topics. For example, to solve the vanishing gradient problem, new architectures are proposed, e.g., residual blocks<sup>6</sup>, new optimization techniques were proposed<sup>7</sup>, and theories such as information highway also became the key to the success of deep neural networks<sup>8</sup>.</p> <ol> <li> <p>Nielsen MA. How the backpropagation algorithm works. In: Neural networks and deep learning [Internet]. [cited 22 Nov 2023]. Available: http://neuralnetworksanddeeplearning.com/chap2.html \u21a9</p> </li> <li> <p>Hornik K, Stinchcombe M, White H. Multilayer feedforward networks are universal approximators. Neural networks: the official journal of the International Neural Network Society 1989; 2: 359\u2013366.\u00a0\u21a9</p> </li> <li> <p>Cybenko G. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems 1989; 2: 303\u2013314.\u00a0\u21a9</p> </li> <li> <p>Hassoun M. Fundamentals of artificial neural networks. The MIT Press, Massachusetts Institute of Technology, 2021https://mitpress.mit.edu/9780262514675/fundamentals-of-artificial-neural-networks/.\u00a0\u21a9</p> </li> <li> <p>Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J. Gradient flow in recurrent nets: The difficulty of learning long-term dependencies. In: Kremer SC, Kolen JF (eds). A field guide to dynamical recurrent neural networks. IEEE Press, 2001.\u00a0\u21a9</p> </li> <li> <p>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. 2015.http://arxiv.org/abs/1512.03385.\u00a0\u21a9</p> </li> <li> <p>Huang G, Sun Y, Liu Z, Sedra D, Weinberger K. Deep networks with stochastic depth. 2016.http://arxiv.org/abs/1603.09382.\u00a0\u21a9</p> </li> <li> <p>Srivastava RK, Greff K, Schmidhuber J. Highway networks. 2015.http://arxiv.org/abs/1505.00387.\u00a0\u21a9</p> </li> </ol>"}, {"location": "deep-learning-fundamentals/recurrent-neural-networks/", "title": "Recurrent Neural Networks", "text": "<p>In the section Neural Networks, we discussed the feedforward neural network.</p> Biological Neural Networks <p>Biological neural networks contain recurrent units. There are theories that employ recurrent networks to explain our memory<sup>3</sup>.</p>"}, {"location": "deep-learning-fundamentals/recurrent-neural-networks/#recurrent-neural-network-architecture", "title": "Recurrent Neural Network Architecture", "text": "<p>A recurrent neural network (RNN) can be achieved by including loops in the network, i.e., the output of a unit is fed back to itself. As an example, we show a single unit in the following figure.</p> <p></p> <p>On the left, we have the unfolded (unrolled) RNN, while the representation on the right is the compressed form. A simplified mathematical representation of the RNN is as follows:</p> \\[ \\begin{equation} h(t) = f( W_h h(t-1) + W_x x(t) + b) \\label{eq-rnn-vanilla} \\end{equation} \\] <p>where \\(h(t)\\) represents the state of the unit at time \\(t\\), \\(x(t)\\) is the input at time \\(t\\).</p> <p>RNN and First-order Differential Equation</p> <p>There are different views of the nature of time series data. Many of the time series datasets are generated by physical systems that follow the laws of physics. Mathematicians and physicists already studied and built up the theories of such systems and the framework we are looking into is dynamical systems.</p> <p>The vanilla RNN described in \\(\\eqref{eq-rnn-vanilla}\\) is quite similar to a first-order differential equation. For simplicity we use RELU for \\(f(\\cdot)\\).</p> <p>Note that</p> \\[ h(t - 1) = h(t) + \\sum_{n=1}^{\\infty} \\frac{h^{(n)} (t)}{n!} (-1)^n, \\] <p>where \\(h^{(n)}(t)\\) is the \\(n\\)th derivative of \\(h(t)\\). Assuming that it converges and higher order doesn't contribute much, we rewrite \\(\\eqref{eq-rnn-vanilla}\\) as</p> \\[ \\begin{equation} \\frac{\\mathrm d h(t)}{\\mathrm d t} = x'(t) + W_h h(t), \\label{eq-rnn-1st-order-diff-eq} \\end{equation} \\] <p>where</p> \\[ \\begin{align} x'(t) =&amp; W_h^{-1} W_x x(t) + b' \\\\ b' = &amp; W_h^{-1} b. \\end{align} \\] <p>We have reduced the RNN formula to a first-order differential equation. Without discussing the details, we know that an exponential component \\(e^{W_h t}\\) will rise in the solution. The component may explode or shrink.</p> <p>Based on our intuition of differential equations, such a dynamical system usually just blows up or diminishes for a large number of iterations. It can also be shown explicitly if we write down the backprogation formula where the state in the far past contributes little to the gradient. This is the famous vanishing gradient problem in RNN<sup>4</sup>. One solution to this is to introduce memory in the iterations, e.g., long short-term memory (LSTM) <sup>5</sup>.</p> <p>In the basic example of RNN shown above, the output of the hidden state is fed to itself in the next iteration. In theory, the value to feed back to the unit and how the input and output are calculated can be quite different in different setups <sup>1</sup><sup>2</sup>. In the section Forecasting with RNN, we will show some examples of the different setups.</p> <ol> <li> <p>Amidi A, Amidi S. CS 230. In: Recurrent Neural Networks Cheatsheet [Internet]. [cited 22 Nov 2023]. Available: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks \u21a9</p> </li> <li> <p>Karpathy A. The Unreasonable Effectiveness of Recurrent Neural Networks. In: Andrej Karpathy blog [Internet]. 2015 [cited 22 Nov 2023]. Available: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ \u21a9</p> </li> <li> <p>Grossberg S. Recurrent neural networks. Scholarpedia 2013; 8: 1888.\u00a0\u21a9</p> </li> <li> <p>Pascanu R, Mikolov T, Bengio Y. On the difficulty of training recurrent neural networks. 2012.http://arxiv.org/abs/1211.5063.\u00a0\u21a9</p> </li> <li> <p>Hochreiter S, Schmidhuber J. Long short-term memory. Neural computation 1997; 9: 1735\u20131780.\u00a0\u21a9</p> </li> </ol>"}, {"location": "dynamical-systems/", "title": "Dynamical Systems", "text": "<p>A lot of time series data are generated by dynamical systems. One of the most cited examples is the coordinates \\(x(t)\\), \\(y(t)\\), \\(z(t)\\) as functions of time \\(t\\) in a Lorenz system.</p> <p>Lorenz System</p> <p>A Lorenz system is defined by the Lorenz equations<sup>1</sup></p> \\[ \\begin{align} \\frac{\\mathrm{d}x}{\\mathrm{d}t} &amp;= \\sigma (y - x), \\\\ \\frac{\\mathrm{d}y}{\\mathrm{d}t} &amp;= x (\\rho - z) - y, \\\\ \\frac{\\mathrm{d}z}{\\mathrm{d}t} &amp;= x y - \\beta z, \\end{align} \\] <p>where \\(x\\), \\(y\\), and \\(z\\) are the coordinates of a particle.</p> <p>It is a chaotic system that is very sensitive to the initial conditions.</p> <p>Dynamical Systems</p> <p>Many real-world systems are dynamical systems. Differential equation is a handy tool to model a dynamical system. For example, the action potentials of a squid giant axon can be modeled by the famous Hodgkin-Huxley model.</p> <p>A naive philosophy to model time series is to come up with a set of differential equations to model the time series. However, finding clean and interpretable differential equations is not easy. It has been the top game in physics for centuries.</p> <p>In the following sections, we will discuss a few solutions to model data as dynamical systems.</p> <ol> <li> <p>Wikipedia contributors. Lorenz system \u2014 Wikipedia, the free encyclopedia. 2023.https://en.wikipedia.org/w/index.php?title=Lorenz\\system\\oldid=1186188179.\u00a0\u21a9</p> </li> </ol>"}, {"location": "dynamical-systems/neural-ode/", "title": "Neural ODE", "text": "<p>Neural ODE is an elegant idea of combining neural networks and differential equations<sup>1</sup>. In this section, we will first show some examples of differential equations and then discuss how to combine neural networks and differential equations.</p>"}, {"location": "dynamical-systems/neural-ode/#ordinary-differential-equations", "title": "Ordinary Differential Equations", "text": "<p>A first-order ordinary differential equation is as simple as</p> \\[ \\begin{equation} \\frac{\\mathrm d h(t)}{\\mathrm dt} = f_\\theta(h(t), t), \\label{eq:1st-order-ode} \\end{equation} \\] <p>where \\(h(t)\\) is the function that describes the state of a dynamical system. To build up intuitions, we show a few examples of differential equations below.</p> <p>Examples of Differential Equations</p>  Utility Code for the Following ODE (Run this first) <pre><code>from abc import ABC, abstractmethod\nimport numpy as np\nfrom scipy.integrate import odeint\nimport inspect\n\nfrom typing import Optional, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nclass DiffEqn(ABC):\n    \"\"\"A first order differential equation and the corresponding solutions.\n\n    :param t: time steps to solve the differential equation for\n    :param y_0: initial condition of the ode\n    \"\"\"\n    def __init__(self, t: np.ndarray, y_0: float, **fn_args: Optional[Any]):\n        self.t = t\n        self.y_0 = y_0\n        self.fn_args = fn_args\n\n    @abstractmethod\n    def fn(self, y: float, t: np.ndarray) -&gt; np.ndarray:\n        pass\n\n    def solve(self) -&gt; np.ndarray:\n        return odeint(self.fn, self.y_0, self.t)\n\n    @abstractmethod\n    def _formula(self) -&gt; str:\n        pass\n\n    def __str__(self):\n        return f\"{self._formula()}\"\n\n    def __repr__(self):\n        return f\"{self._formula()}\"\n\n    def plot(self, ax: Optional[plt.Axes]=None):\n\n        if ax is None:\n            _, ax = plt.subplots(figsize=(10, 6.18))\n\n        sns.lineplot(\n            x=self.t, y=self.solve()[:, 0],\n            ax=ax\n        )\n\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"y\")\n        ax.set_title(f\"Solution for ${self}$ with {self.fn_args}\")\n</code></pre>  Infection Model Exponential Growth Oscillations Receprocal <p>The logistic model of infectious disease is</p> \\[ \\frac{dh(t)}{d t} = r * y * (1-y). \\] <pre><code>class Infections(DiffEqn):\n    def fn(self, y: float, t: np.ndarray) -&gt; np.ndarray:\n        r = self.fn_args[\"r\"]\n        dydt = r * y * (1 - y)\n        return dydt\n\n    def _formula(self):\n        return r\"\\frac{dh(t)}{d t} = r * y * (1-y)\"\n\ninfections_s = Infections(t, 0.1, r=0.9)\ninfections_s.plot()\n</code></pre> <p></p> <p>The following equation describes an exponentially growing \\(h(t)\\).</p> \\[ \\frac{\\mathrm d h(t)}{\\mathrm d t} = \\lambda h(t), \\] <p>with \\(\\lambda &gt; 0\\).</p> <pre><code>class Exponential(DiffEqn):\n    def fn(self, y: float, t: np.ndarray) -&gt; np.ndarray:\n        lbd = self.fn_args[\"lbd\"]\n        dydt = lbd * y\n\n        return dydt\n\n    def _formula(self):\n        return r\"\\frac{dh(t)}{d t} = \\lambda h(t)\"\n\ny0_exponential = 1\nt = np.linspace(0, 10, 101)\nlbd = 2\n\nexponential = Exponential(t, y0_exponential, lbd=lbd)\nexponential.plot()\n</code></pre> <p></p> <p>We construct an oscillatory system using sinusoid,</p> \\[ \\frac{dh(t)}{d t} = \\sin(\\lambda t) t. \\] <p>Naively, we expect the oscillations to be large for larger \\(t\\). Taking the limit \\(t\\to\\infty\\), the first order derivative \\(\\frac{\\mathrm dy}{\\mathrm d t}\\to\\infty\\). With this limit, we expect the oscillation amplitude to be infinite.</p> <pre><code>class SinMultiplyT(DiffEqn):\n    def fn(self, y: float, t: np.ndarray) -&gt; np.ndarray:\n        lbd = self.fn_args[\"lbd\"]\n        dydt = np.sin(lbd * t) * t\n\n        return dydt\n\n    def _formula(self):\n        return r\"\\frac{dh(t)}{d t} = \\sin(\\lambda t) t\"\n\ny0_sin = 1\nt = np.linspace(0, 10, 101)\nlbd = 2\nsin_multiply_t = SinMultiplyT(t, y0_sin, lbd=lbd)\nsin_multiply_t.plot()\n</code></pre> <p></p> <p>We design a system that grows according to the receprocal of its value,</p> \\[ \\frac{dh(t)}{d t} = \\frac{1}{a * y + b}. \\] <pre><code>class Receprocal(DiffEqn):\n    def fn(self, y: float, t: np.ndarray) -&gt; np.ndarray:\n        shift = self.fn_args[\"shift\"]\n        scale = self.fn_args[\"scale\"]\n        dydt = 1/(shift + scale * y)\n        return dydt\n\n    def _formula(self):\n        return r\"\\frac{dh(t)}{d t} = \\frac{1}{shift + scale * y}\"\n\nreceprocal = Receprocal(t, 1, shift=-5, scale=-10)\nreceprocal.plot()\n</code></pre> <p></p>"}, {"location": "dynamical-systems/neural-ode/#finite-difference-form-of-differential-equations", "title": "Finite Difference Form of Differential Equations", "text": "<p>Eq. \\(\\eqref{eq:1st-order-ode}\\) can be rewritten in the finite difference form as</p> \\[ \\frac{h(t+\\Delta t) - h(t)}{\\Delta t} = f_\\theta(h(t), t), \\] <p>with \\(\\Delta t\\) small enough.</p> <p>Derivatives</p> <p>The definition of the first-order derivative is $$ h'(t) = \\lim_{\\Delta t\\to 0} \\frac{h(t+\\Delta t) - h(t)}{\\Delta t}. $$</p> <p>In a numerical computation, \\(\\lim\\) is approached by taking a small \\(\\Delta t\\).</p> <p>If we take \\(\\Delta t = 1\\), the equation becomes</p> \\[ \\begin{equation} h(t+1) = h(t) + f_\\theta(h(t), t) \\label{eq:1st-order-ode-finite-difference-deltat-1} \\end{equation} \\] <p>\\(\\Delta t = 1\\)? Rescale of Time \\(t\\).</p> <p>As there is no absolute measure of time \\(t\\), we can always rescale \\(t\\) to \\(\\hat t\\) so that \\(\\Delta t = 1\\). However, for the sake of clarity, we will keep \\(t\\) as the time variable.</p> <p>Coming back to neural networks, if \\(h(t)\\) represents the state of a neural network block at depth \\(t\\), Eq. \\(\\eqref{eq:1st-order-ode-finite-difference-deltat-1}\\) is nothing fancy but the residual connection in ResNet<sup>2</sup>. This connection between the finite difference form of a first-order differential equation and the residual connection leads to the new family of deep neural network models called neural ode<sup>1</sup>.</p> <p>In a neural ODE, we treat each layer of a neural network as a function \\(h\\) of depth \\(t\\), i.e., the state of the layer is equivalent to a function \\(h(t)\\). However, we are not obliged to use \\(h\\) as the function to directly take in the raw input and output the raw output. NeuralODE is extremely flexible and we could build latent dynamical systems to represent some intrinsic dynamics.</p> <p></p> Solving Differential Equations <p>The right side of the Eq. \\(\\eqref{eq:1st-order-ode}\\) is called the Jacobian. Given any Jacobian \\(f(h(t), \\theta(t), t)\\), we can, at least numerically, perform integration to find out the value of \\(h(t)\\) at any \\(t=t_N\\),</p> \\[ h(t_N) = \\int_{t_0}^{t_i} f(h(t), \\theta(t), t) \\mathrm dt. \\] <p>In this formalism, we find out the transformed input \\(h(t_0)\\) by solving this differential equation. In traditional neural networks, we achieve this by stacking many layers of neural networks using skip connections.</p> <p>In the original Neural ODE paper, the authors used the so-called reverse-model derivative method<sup>1</sup>.</p> <p>We will not dive deep into solving differential equations in this section. Instead, we will show some applications of neural ODEs in section Time Series Forecasting with Neural ODE.</p> <ol> <li> <p>Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. Neural ordinary differential equations. 2018.http://arxiv.org/abs/1806.07366.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. 2015.http://arxiv.org/abs/1512.03385.\u00a0\u21a9</p> </li> </ol>"}, {"location": "energy-based-models/ebm.diffusion/", "title": "Denoising Diffusion Probabilistic Models", "text": "<p>Many philosophically beautiful deep learning ideas face the tractability problem. Many deep learning models utilize the concept of latent space, e.g., \\(\\mathbf z\\), which is usually a compression of the real data space, e.g., \\(\\mathbf x\\), to enable easier computations for our task.</p> <p>However, such models usually require the computation of an intractable marginalization of the joint distribution \\(p(\\mathbf x, \\mathbf z)\\) over the latent space<sup>3</sup>. To make such computations tractable, we have to apply approximations or theoretical assumptions. Diffusion models in deep learning establish the connection between the real data space \\(\\mathbf x\\) and the latent space \\(\\mathbf z\\) assuming invertible diffusion processes <sup>4</sup> <sup>5</sup>.</p>"}, {"location": "energy-based-models/ebm.diffusion/#objective", "title": "Objective", "text": "<p>In a denoising diffusion model, given an input \\(\\mathbf x^0\\) drawn from a complicated and unknown distribution \\(q(\\mathbf x^0)\\), we find</p> <ul> <li>a latent space with a simple and manageable distribution, e.g., normal distribution, and</li> <li>the transformations from \\(\\mathbf x^0\\) to \\(\\mathbf x^n\\), as well as</li> <li>the transformations from \\(\\mathbf x^n\\) to \\(\\mathbf x^0\\).</li> </ul> <p>Image Data Example</p> <p>The following figure is taken from Sohl-Dickstein et al. (2015)<sup>5</sup>.</p> <p></p> <p>The forward process, shown in the first row, diffuses the original spiral data at \\(t=0\\) into a Gaussian noise at \\(t=T\\). The reverse process, shown in the second row, recovers the original data from \\(t=T\\) into the image at \\(t=0\\).</p> <p>In the following texts, we use \\(n\\) instead of \\(t\\).</p>"}, {"location": "energy-based-models/ebm.diffusion/#an-example-with-n5", "title": "An Example with \\(N=5\\)", "text": "<p>For example, with \\(N=5\\), the forward process is</p> <pre><code>flowchart LR\nx0 --&gt; x1 --&gt; x2 --&gt; x3 --&gt; x4 --&gt; x5</code></pre> <p>and the reverse process is</p> <pre><code>flowchart LR\nx5 --&gt; x4 --&gt; x3 --&gt; x2 --&gt; x1 --&gt; x0</code></pre> <p>The joint distribution we are searching for is</p> \\[ q(\\mathbf x^1, \\mathbf x^2, \\mathbf x^3, \\mathbf x^4, \\mathbf x^5 \\vert \\mathbf x^0) = q(\\mathbf x^5\\vert \\mathbf x^4) q(\\mathbf x^4\\vert \\mathbf x^3) q(\\mathbf x^3\\vert \\mathbf x^2)q(\\mathbf x^2\\vert \\mathbf x^1)q(\\mathbf x^1\\vert \\mathbf x^0), \\] <p>A diffusion model assumes a simple diffusion process, e.g.,</p> \\[ \\begin{equation} q(\\mathbf x^n \\vert \\mathbf x^{n-1}) \\equiv \\mathcal N (\\mathbf x^n ; \\sqrt{ 1 - \\beta_n} \\mathbf x ^{n -1}, \\beta_n\\mathbf I). \\label{eq-guassian-noise} \\end{equation} \\] <p>This simulates an information diffusion process. The information in the original data is gradually smeared.</p> <p>If the chosen diffusion process is reversible, the reverse process of it can be modeled by a similar Markov process</p> \\[ p_\\theta (\\mathbf x^0, \\mathbf x^1, \\mathbf x^2, \\mathbf x^3, \\mathbf x^4, \\mathbf x^5) = p_\\theta (\\mathbf x^0 \\vert \\mathbf x^1) p_\\theta (\\mathbf x^1 \\vert \\mathbf x^2) p_\\theta (\\mathbf x^2 \\vert \\mathbf x^3) p_\\theta (\\mathbf x^3 \\vert \\mathbf x^4) p_\\theta (\\mathbf x^4 \\vert \\mathbf x^5) p(\\mathbf x^5). \\] <p>This reverse process is the denoising process.</p> <p>As long as our model estimates \\(p_\\theta (\\mathbf x^n \\vert \\mathbf x^{n-1})\\) nicely, we can go \\(\\mathbf x^0 \\to \\mathbf x^N\\) and \\(\\mathbf x^N \\to \\mathbf x^0\\).</p>"}, {"location": "energy-based-models/ebm.diffusion/#the-reverse-process-a-gaussian-example", "title": "The Reverse Process: A Gaussian Example", "text": "<p>With Eq \\(\\eqref{eq-guassian-noise}\\), the reverse process is</p> \\[ \\begin{equation} p_\\theta (\\mathbf x^{n-1} \\vert \\mathbf x^n) = \\mathcal N ( \\mathbf x^{n-1} ; \\mu_\\theta(\\mathbf x^n, n), \\Sigma_\\theta(\\mathbf x^n, n)\\mathbf I). \\label{eqn-guassian-reverse-process} \\end{equation} \\]"}, {"location": "energy-based-models/ebm.diffusion/#summary", "title": "Summary", "text": "<ul> <li>Forward: perturbs data to noise, step by step;</li> <li>Reverse: converts noise to data, step by step.</li> </ul> <pre><code>flowchart LR\nprior[\"prior distribution\"]\n    data --\"forward Markov chain\"--&gt; noise\n    noise --\"reverse Markov chain\"--&gt; data\n        prior --\"sampling\"--&gt; noise</code></pre>"}, {"location": "energy-based-models/ebm.diffusion/#optimization", "title": "Optimization", "text": "<p>The forward chain is predefined. To close the loop, we have to find \\(p_\\theta\\). A natural choice for our loss function is the negative log-likelihood,</p> \\[ \\mathbb E_{q(\\mathbf x^0)} \\left( - \\log ( p_\\theta (\\mathbf x^0) ) \\right). \\] <p>(Ho et al., 2020) proved that the above loss has an upper bound related to the diffusion process defined in Eq \\(\\eqref{eq-guassian-noise}\\)<sup>1</sup></p> \\[ \\begin{align} &amp;\\operatorname{min}_\\theta \\mathbb E_{q(\\mathbf x^0)} \\\\ \\leq &amp; \\operatorname{min}_\\theta \\mathbb E_{q(\\mathbf x^{0:N})} \\left[ -\\log p(\\mathbf x^N) - \\sum_{n=1}^{N} \\log \\frac{p_\\theta (\\mathbf x^{n-1}\\vert \\mathbf x^n)}{q(\\mathbf x^n \\vert \\mathbf x^{n-1})} \\right] \\\\ =&amp; \\operatorname{min}_\\theta \\mathbb E_{\\mathbf x^0, \\epsilon} \\left[ \\frac{\\beta_n^2}{2\\Sigma_\\theta \\alpha_n (1 - \\bar \\alpha_n)} \\lVert \\epsilon - \\epsilon_\\theta ( \\sqrt{ \\bar \\alpha_n} \\mathbf x^0 + \\sqrt{1-\\bar \\alpha_n} \\epsilon , n ) \\rVert \\right] \\end{align} \\] <p>where \\(\\epsilon\\) is a sample from \\(\\mathcal N(0, \\mathbf I)\\). The second step assumes the Gaussian noise in Eq \\(\\eqref{eq-guassian-noise}\\), which is equivalent to<sup>1</sup></p> \\[ q(\\mathbf x^n \\vert \\mathbf x^0) = \\mathcal N (\\mathbf x^n ; \\sqrt{\\bar \\alpha_n} \\mathbf x^0, (1 - \\bar \\alpha_n)\\mathbf I), \\] <p>with \\(\\alpha_n = 1 - \\beta _ n\\), \\(\\bar \\alpha _ n = \\Pi _ {i=1}^n \\alpha_i\\), and \\(\\Sigma_\\theta\\) in Eq \\(\\eqref{eqn-guassian-reverse-process}\\).</p>"}, {"location": "energy-based-models/ebm.diffusion/#code", "title": "Code", "text": "<p>Rogge &amp; Rasul (2022) wrote a post with detailed annotations of the denoising diffusion probabilistic model<sup>2</sup>.</p> <ol> <li> <p>Rasul K, Seward C, Schuster I, Vollgraf R. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2101.12072 \u21a9\u21a9</p> </li> <li> <p>Rogge N, Rasul K. The Annotated Diffusion Model. In: Hugging Face Blog [Internet]. 7 Jun 2022 [cited 18 Feb 2023]. Available: https://huggingface.co/blog/annotated-diffusion \u21a9</p> </li> <li> <p>Luo C. Understanding diffusion models: A unified perspective. 2022.http://arxiv.org/abs/2208.11970.\u00a0\u21a9</p> </li> <li> <p>Sohl-Dickstein J, Weiss EA, Maheswaranathan N, Ganguli S. Deep unsupervised learning using nonequilibrium thermodynamics. 2015.http://arxiv.org/abs/1503.03585.\u00a0\u21a9</p> </li> <li> <p>Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. 2020.http://arxiv.org/abs/2006.11239.\u00a0\u21a9\u21a9</p> </li> </ol>"}, {"location": "energy-based-models/intro/", "title": "Energy-based Models", "text": "<p>Energy-based models (EBM) establish relations between different possible values of variables using \"energy functions\"<sup>2</sup>. In an EBM, any input data point can be assigned a probability density<sup>1</sup>. Similar to statistical physics, we can create such probability densities using a partition function. As easy as it sounds, such probability densities usually require a scalar function similar to the energy function in statistical physics. When building the objective functions, we require the configurations that should have the same target label to have low energy, or higher probability density, i.e., to be compatible.</p> <ol> <li> <p>Lippe P. Tutorial 9: Deep Autoencoders \u2014 UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html \u21a9</p> </li> <li> <p>Le Cun Y, Chopra S, Hadsell R, Ranzato M, Huang FJ. A tutorial on Energy-Based learning. 2006.\u00a0\u21a9</p> </li> </ol>"}, {"location": "engineering/", "title": "Coding Tips", "text": "<p>In this book, we use Python as our programming language. In the main chapters, we will focus on the theories and actual code and skip the basic concepts. To make sure we are on the same page, we shove all the tech stack related topics into this chapter for future reference. It is not necessary to read this chapter before reading the main chapters. However, we recommend the readers go through this chapter at some point to make sure they are not missing some basic engineering concepts.</p> <p>Info</p> <p>This chapter is not aiming to be a comprehensive note on these technologies but a few key components that may be missing in many research-oriented tech stacks. We assume the readers have worked with the essential technologies in a Python-based deep learning project.</p>"}, {"location": "engineering/#good-references-for-coding-in-research", "title": "Good References for Coding in Research", "text": "<p>Some skills only take a while to learn but people benefit from them for their whole life. Managing code falls exactly into this bucket, for programmers.</p> <p>The Good Research Code Handbook is a very good and concise guide to building good coding habits. This should be a good first read.</p> <p>The Alan Turing Institute also has a Research Software Engineering with Python course. This is a comprehensive generic course for boosting Python coding skills in research.</p> <p>A Checklist of Tech Stack</p> <p>We provide a concise list of tools for coding. Most of them are probably already integrated into most people's workflows. Hence we provide no descriptions but only the list itself.</p> <p>In the following diagrams, we highlight the recommended tools using orange color. Clicking on them takes us to the corresponding website.</p> <p>The first set of checklists is to help us set up a good coding environment.</p> <pre><code>flowchart TD\nclassDef highlight fill:#f96;\n\nenv[\"Setting up Coding Environment\"]\ngit[\"fa:fa-star Git\"]:::highlight\nprecommit[\"pre-commit\"]:::highlight\nide[\"Integrated Development Environment (IDE)\"]\nvscode[\"Visual Studio Code\"]:::highlight\npycharm[\"PyCharm\"]\njupyter[\"Jupyter Notebooks\"]\npython[\"Python Environment\"]\npy_env[\"Python Environment Management\"]\nconda[\"Anaconda\"]\npyenv_venv[\"Pyenv + venv + pip\"]\npyenv_poetry[\"Pyenv + poetry\"]\npoetry[\"Poetry\"]:::highlight\npyenv[\"pyenv\"]:::highlight\nvenv[\"venv\"]\n\nclick git \"https://git-scm.com/\" \"Git\"\nclick precommit \"https://pre-commit.com/\" \"pre-commit\"\nclick vscode \"https://code.visualstudio.com/\" \"Visual Studio Code\"\nclick jupyter \"https://jupyter.org/\" \"Jupyter Lab\"\nclick pycharm \"https://www.jetbrains.com/pycharm/\" \"PyCharm\"\nclick conda \"https://www.anaconda.com/\" \"Anaconda\"\nclick pyenv \"https://github.com/pyenv/pyenv\" \"pyenv\"\nclick venv \"https://docs.python.org/3/library/venv.html\" \"venv\"\nclick poetry \"https://python-poetry.org/\" \"poetry\"\n\nenv --- git\ngit --- precommit\n\nenv --- ide\nide --- vscode\nide --- jupyter\nide --- pycharm\n\nenv --- python\npython --- py_env\npy_env --- conda\npy_env --- pyenv_venv\npy_env --- pyenv_poetry\n\npyenv_venv --- pyenv\npyenv_venv --- venv\n\npyenv_poetry --- pyenv\npyenv_poetry --- poetry</code></pre> <p>The second set of checklists is to boost our code quality.</p> <pre><code>flowchart TD\nclassDef highlight fill:#f96;\n\npython[\"Python Code Quality\"]\ntest[\"Test Your Code\"]\nformatter[\"Formatter\"]\nlinter[\"Linter\"]\npytest[\"pytest\"]:::highlight\nblack[\"black\"]:::highlight\nisort[\"isort\"]:::highlight\npylint[\"pylint\"]\nflake8[\"flake8\"]\npylama[\"pylama\"]\nmypy[\"mypy\"]:::highlight\n\nclick pytest \"https://pytest.org/\" \"pytest\"\nclick black \"https://github.com/psf/black\" \"black\"\nclick isort \"https://github.com/pycqa/isort\"\nclick mypy \"http://mypy-lang.org/\"\nclick pylint \"https://pylint.pycqa.org/\"\nclick flake8 \"https://flake8.pycqa.org/en/latest/\"\nclick pylama \"https://github.com/klen/pylama\"\n\npython --- test\ntest --- pytest\n\npython --- formatter\nformatter --- black\nformatter --- isort\n\npython --- linter\nlinter --- mypy\nlinter --- pylint\nlinter --- flake8\nlinter ---pylama</code></pre> <p>Finally, we also mention the primary python packages used here.</p> <pre><code>flowchart TD\nclassDef highlight fill:#f96;\n\ndataml[\"Data and Machine Learning\"]\npandas[\"Pandas\"]:::highlight\npytorch[\"PyTorch\"]:::highlight\nlightning[\"PyTorch Lightning\"]:::highlight\nmuch_more[\"and more ...\"]\n\nclick pandas \"https://pandas.pydata.org/\"\nclick pytorch \"https://pytorch.org/\"\nclick lightning \"https://www.pytorchlightning.ai/\"\n\ndataml --- pandas\ndataml --- pytorch\ndataml --- lightning\ndataml --- much_more</code></pre>"}, {"location": "engineering/python/", "title": "Python", "text": "<p>We assume the readers have a good understanding of the Python programming language, as Python will be the primary programming language for demos and tutorials in this book. For engineering tips, we will cover a few topics here, including</p> <ul> <li>Environment management;</li> <li>Dependency management;</li> <li><code>pre-commit</code>.</li> </ul> <p>TL;DR</p> <ol> <li>Use pyenv to manage Python versions;</li> <li>Use poetry to manage dependencies;</li> <li>Always set up `pre-commit`` for your git repository.</li> </ol>"}, {"location": "engineering/python/#python-environment-management", "title": "Python Environment Management", "text": "<p>Environment management is never easy, and the same is true for the Python ecosystem. People have developed a lot of tools to make environment management easier. As you could imagine, this also means we have a zoo of tools to choose from.</p> <p>There are three things to manage in a Python project:</p> <ol> <li>Python version,</li> <li>Dependencies of the project, and</li> <li>An environment where we install our dependencies.</li> </ol> <p>Some tools can manage all three, and some tools focus on one or two of them. We discuss two popular sets of tools: <code>conda</code> and <code>pyenv</code> + <code>poetry</code>.</p>"}, {"location": "engineering/python/#conda", "title": "<code>conda</code>", "text": "<p>Many data scientists started with the simple and out-of-the-box choice called <code>conda</code>. <code>conda</code> is an all-in-one toolkit to manage Python versions, environments, and project dependencies.</p> <p><code>conda</code> cheatsheet</p> <p>The most useful commands for conda are the following.</p> <ol> <li>Create an environment: <code>conda create -n my-env-name python=3.9 pip</code>, where<ol> <li><code>my-env-name</code> is the name of the environment,</li> <li><code>python=3.9</code> specifies the version of Python,</li> <li><code>pip</code> at the end is telling <code>conda</code> to install <code>pip</code> in this new environment.</li> </ol> </li> <li>Activate an environment: <code>conda activate my-env-name</code></li> <li>Install new dependency: <code>conda install pandas</code></li> <li>List all available environments: <code>conda env list</code></li> </ol> <p>Anaconda provides a nice cheatsheet.</p>"}, {"location": "engineering/python/#pyenv-poetry", "title": "<code>pyenv</code> + <code>poetry</code>", "text": "<p><code>conda</code> is powerful, but it is too powerful for a simple Python project. As of 2024, if you ask around, many Python developers will recommend <code>poetry</code>.</p> <p><code>poetry</code> manages dependencies and environments. We just need a tool like <code>pyenv</code> to manage Python versions.</p> <p>The <code>poetry</code> workflow</p> <p>To work with poetry in an existing project <code>my_kuhl_project</code></p> <ol> <li><code>poetry init</code> to initialize the project and follow the instructions;</li> <li><code>poetry env use 3.10</code> to specify the Python version. In this example, we use <code>3.10</code>;</li> <li><code>poetry add pandas</code> to add a package called <code>pandas</code>.</li> </ol> <p>Everything we specified will be written into the <code>pyproject.toml</code> file.</p> <p><code>poetry</code> provides a nice tutorial on its website.</p>"}, {"location": "engineering/python/#dependency-specifications", "title": "Dependency Specifications", "text": "<p>We have a few choices to specify the dependencies. The most used method at the moment is <code>requirements.txt</code>. However, specifying dependencies in <code>pyproject.toml</code> is a much better choice.</p> <p>Python introduced <code>pyproject.toml</code> in PEP518 which can be used together with <code>poetry</code> to manage dependencies.</p> <p>While tutorials on how to use <code>poetry</code> are beyond the scope of this book, we highly recommend using <code>poetry</code> in a formal project.</p> <p><code>poetry</code> is sometimes slow</p> <p><code>poetry</code> can be very slow as it has to load many different versions of the packages to try out in some cases<sup>5</sup><sup>6</sup>.</p> <p><code>conda</code> with <code>pip</code></p> <p>If one insists on using <code>conda</code>, here we provide a few tips for <code>conda</code> users.</p> <p><code>conda</code> provides its own requirement specification using <code>environment.yaml</code>. However, many projects still prefer <code>requirements.txt</code> even though <code>conda</code>'s <code>environment.yaml</code> is quite powerful.</p> <p>To use <code>requirements.txt</code> and <code>pip</code>, we always install <code>pip</code> when creating a new environment, e.g., <code>conda create -n my-env-name python=3.9 pip</code>.</p> <p>Once the environment is activated (<code>conda activate my-env-name</code>), we can use <code>pip</code> to install dependencies, e.g., <code>pip install -r requirements.txt</code>.</p>"}, {"location": "engineering/python/#python-styles-and-pre-commit", "title": "Python Styles and <code>pre-commit</code>", "text": "<p>In a Python project, it is important to have certain conventions or styles. To be consistent, one could follow some style guides for Python. There are official proposals, such as PEP8, and \"third party\" style guides, such as Google Python Style Guide <sup>3</sup><sup>4</sup>.</p> <p>We also recommend <code>pre-commit</code>. <code>pre-commit</code> helps us manage git hooks to be executed before each commit. Once installed, every time we run <code>git commit -m \"my commit message here\"</code>, a series of commands will be executed first based on the configurations.</p>  Some pre-commit Configs An Example Config <p><code>pre-commit</code> officially provides some hooks already, e.g., <code>trailing-whitespace</code> <sup>2</sup>.</p> <p>We also recommend the following hooks,</p> <ul> <li><code>black</code>, which formats the code based on pre-defined styles,</li> <li><code>isort</code>, which orders the Python imports<sup>1</sup>,</li> <li><code>mypy</code>, which is a linter for Python.</li> </ul> <p>The following is an example <code>.pre-commit-config.yaml</code> file for a Python project.</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.2.0\n    hooks:\n    - id: check-added-large-files\n    - id: debug-statements\n    - id: detect-private-key\n    - id: end-of-file-fixer\n    - id: requirements-txt-fixer\n    - id: trailing-whitespace\n- repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v0.960\n    hooks:\n    - id: mypy\n        args:\n        - \"--no-strict-optional\"\n        - \"--ignore-missing-imports\"\n- repo: https://github.com/ambv/black\n    rev: 22.6.0\n    hooks:\n    - id: black\n    language: python\n    args:\n        - \"--line-length=120\"\n- repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n    - id: isort\n        name: isort (python)\n        args: [\"--profile\", \"black\"]\n</code></pre>"}, {"location": "engineering/python/#write-docstrings", "title": "Write docstrings", "text": "<p>Writing docstrings for functions and classes can help our future self understand them more easily. There are different styles for docstrings. Two of the popular ones are</p> <ul> <li>reStructuredText Docstring Format, and</li> <li>Google style docstrings.</li> </ul>"}, {"location": "engineering/python/#test-saves-time", "title": "Test Saves Time", "text": "<p>Adding tests to our code can save us time. We will not list all these benefits of having tests. But tests can help us debug our code and ship results more confidently. For example, suppose we are developing a function and spot a bug. One of the best ways of debugging it is to write a test and put a debugger breakpoint at the suspicious line of the code. With the help of IDEs such as Visual Studio Code, this process can save us a lot of time in debugging.</p> Use <code>pytest</code> <p>Use pytest. RealPython provides a good short introduction. The Alan Turing Institue provides some lectures on testing and pytest.</p> <ol> <li> <p>Pre Commit. In: isort [Internet]. [cited 22 Jul 2022]. Available: https://pycqa.github.io/isort/docs/configuration/pre-commit.html \u21a9</p> </li> <li> <p>pre-commit-config-pre-commit-hooks.yaml. In: Gist [Internet]. [cited 22 Jul 2022]. Available: https://gist.github.com/lynnkwong/f7591525cfc903ec592943e0f2a61ed9 \u21a9</p> </li> <li> <p>Guido van Rossum, Barry Warsaw, Nick Coghlan. PEP 8 \u2013 Style Guide for Python Code. In: peps.python.org [Internet]. 5 Jul 2001 [cited 23 Jul 2022]. Available: https://peps.python.org/pep-0008/ \u21a9</p> </li> <li> <p>Google Python Style Guide. In: Google Python Style Guide [Internet]. [cited 22 Jul 2022]. Available: https://google.github.io/styleguide/pyguide.html \u21a9</p> </li> <li> <p>Poetry is extremely slow when resolving the dependencies \u00b7 Issue #2094 \u00b7 python-poetry/poetry. In: GitHub [Internet]. [cited 23 Jul 2022]. Available: https://github.com/python-poetry/poetry/issues/2094 \u21a9</p> </li> <li> <p>FAQ. In: Poetry - Python dependency management and packaging made easy [Internet]. [cited 29 Jan 2024]. Available: https://python-poetry.org/docs/faq/#why-is-the-dependency-resolution-process-slow \u21a9</p> </li> </ol>"}, {"location": "meta/changelog/", "title": "Changelog", "text": ""}, {"location": "meta/roadmap/", "title": "Roadmap", "text": "<p>When I switched to data science, I built my digital garden, datumorphism. I deliberately designed this digital garden as my second brain. As a result, most of the articles are fragments of knowledge and require context to understand them.</p> <p>Making bricks is easy but assembling them into a house is not easy. So I have decided to use this repository to practice my house-building techniques.</p> <p>I do not have a finished blueprint yet. But I have a framework in my mind: I want to consolidate some of my thoughts and learnings in an organized way. However, I do not want to compile a reference book, as datumorphism already serves this purpose. I am thinking of an</p>"}, {"location": "meta/roadmap/#open-source", "title": "Open Source", "text": "<p>This is an open-source project on GitHub:  emptymalei/deep-learning.</p>"}, {"location": "meta/roadmap/#how-do-i-write-it", "title": "How do I Write It", "text": "<p>I am trying out a more \"agile\" method. Instead of finishing the whole project at once, I will release the book by chapter. A few thoughts on this plan:</p> <ul> <li>Each new section should be a PR.</li> <li>Release on every new section.</li> </ul>"}, {"location": "meta/roadmap/#how-do-i-track-the-progress", "title": "How do I track the Progress", "text": "<p>I use GitHub Projects. Here is my board.</p>"}, {"location": "notebooks/creating_time_series_datasets/", "title": "Creating Time Series Datasets", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom loguru import logger\nfrom torch.utils.data import Dataset\n</pre> from typing import Tuple  import numpy as np import pandas as pd from loguru import logger from torch.utils.data import Dataset  In\u00a0[\u00a0]: Copied! <pre>class DataFrameDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param history_length: length of input X in time dimension\n        in the final Dataset class.\n    :param horizon: number of steps to be forecasted.\n    :param gap: gap between input history and prediction\n    \"\"\"\n\n    def __init__(\n        self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.history_length = history_length\n        self.horzion = horizon\n        self.gap = gap\n        self.dataframe_rows = len(self.dataframe)\n        self.length = (\n            self.dataframe_rows - self.history_length - self.horzion - self.gap + 1\n        )\n\n    def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        x, y = (\n            self.dataframe[idx : self.history_length + idx].values,\n            self.dataframe[\n                self.history_length\n                + idx\n                + gap : self.history_length\n                + self.horzion\n                + idx\n                + gap\n            ].values,\n        )\n        return x, y\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [\n                self.moving_slicing(i, self.gap)\n                for i in range(idx.start, idx.stop, step)\n            ]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx, self.gap)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</pre> class DataFrameDataset(Dataset):     \"\"\"A dataset from a pandas dataframe.      For a given pandas dataframe, this generates a pytorch     compatible dataset by sliding in time dimension.      ```python     ds = DataFrameDataset(         dataframe=df, history_length=10, horizon=2     )     ```      :param dataframe: input dataframe with a DatetimeIndex.     :param history_length: length of input X in time dimension         in the final Dataset class.     :param horizon: number of steps to be forecasted.     :param gap: gap between input history and prediction     \"\"\"      def __init__(         self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0     ):         super().__init__()         self.dataframe = dataframe         self.history_length = history_length         self.horzion = horizon         self.gap = gap         self.dataframe_rows = len(self.dataframe)         self.length = (             self.dataframe_rows - self.history_length - self.horzion - self.gap + 1         )      def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:         x, y = (             self.dataframe[idx : self.history_length + idx].values,             self.dataframe[                 self.history_length                 + idx                 + gap : self.history_length                 + self.horzion                 + idx                 + gap             ].values,         )         return x, y      def _validate_dataframe(self) -&gt; None:         \"\"\"Validate the input dataframe.          - We require the dataframe index to be DatetimeIndex.         - This dataset is null aversion.         - Dataframe index should be sorted.         \"\"\"          if not isinstance(             self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex         ):             raise TypeError(                 \"Type of the dataframe index is not DatetimeIndex\"                 f\": {type(self.dataframe.index)}\"             )          has_na = self.dataframe.isnull().values.any()          if has_na:             logger.warning(\"Dataframe has null\")          has_index_sorted = self.dataframe.index.equals(             self.dataframe.index.sort_values()         )          if not has_index_sorted:             logger.warning(\"Dataframe index is not sorted\")      def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:         if isinstance(idx, slice):             if (idx.start &lt; 0) or (idx.stop &gt;= self.length):                 raise IndexError(f\"Slice out of range: {idx}\")             step = idx.step if idx.step is not None else 1             return [                 self.moving_slicing(i, self.gap)                 for i in range(idx.start, idx.stop, step)             ]         else:             if idx &gt;= self.length:                 raise IndexError(\"End of dataset\")             return self.moving_slicing(idx, self.gap)      def __len__(self) -&gt; int:         return self.length <p>We create a sample dataframe with one single variable \"y\"</p> In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(np.arange(15), columns=[\"y\"])\ndf\n</pre> df = pd.DataFrame(np.arange(15), columns=[\"y\"]) df In\u00a0[\u00a0]: Copied! <pre>ds_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=1)\n</pre> ds_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=1) In\u00a0[\u00a0]: Copied! <pre>list(ds_1)\n</pre> list(ds_1) In\u00a0[\u00a0]: Copied! <pre>ds_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=2)\n</pre> ds_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=2) In\u00a0[\u00a0]: Copied! <pre>list(ds_2)\n</pre> list(ds_2) In\u00a0[\u00a0]: Copied! <pre>ds_1_gap_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=1, gap=1)\n</pre> ds_1_gap_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=1, gap=1) In\u00a0[\u00a0]: Copied! <pre>list(ds_1_gap_1)\n</pre> list(ds_1_gap_1) In\u00a0[\u00a0]: Copied! <pre>ds_1_gap_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=1, gap=2)\n</pre> ds_1_gap_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=1, gap=2) In\u00a0[\u00a0]: Copied! <pre>list(ds_1_gap_2)\n</pre> list(ds_1_gap_2) In\u00a0[\u00a0]: Copied! <pre>ds_2_gap_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=2, gap=1)\n</pre> ds_2_gap_1 = DataFrameDataset(dataframe=df, history_length=10, horizon=2, gap=1) In\u00a0[\u00a0]: Copied! <pre>list(ds_2_gap_1)\n</pre> list(ds_2_gap_1) In\u00a0[\u00a0]: Copied! <pre>ds_2_gap_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=2, gap=2)\n</pre> ds_2_gap_2 = DataFrameDataset(dataframe=df, history_length=10, horizon=2, gap=2) In\u00a0[\u00a0]: Copied! <pre>list(ds_2_gap_2)\n</pre> list(ds_2_gap_2)"}, {"location": "notebooks/creating_time_series_datasets/#creating-time-series-datasets", "title": "Creating Time Series Datasets\u00b6", "text": "<p>In this notebook, we explain how to create a time series dataset for PyTorch using the moving slicing technique.</p> <p>The class <code>DataFrameDataset</code> is also included in our <code>ts_dl_utils</code> package.</p>"}, {"location": "notebooks/creating_time_series_datasets/#examples", "title": "Examples\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon1", "title": "history_length=10, horizon=1\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon2", "title": "history_length=10, horizon=2\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon1-gap1", "title": "history_length=10, horizon=1, gap=1\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon1-gap2", "title": "history_length=10, horizon=1, gap=2\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon2-gap1", "title": "history_length=10, horizon=2, gap=1\u00b6", "text": ""}, {"location": "notebooks/creating_time_series_datasets/#history_length10-horizon2-gap2", "title": "history_length=10, horizon=2, gap=2\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/", "title": "Diffusion Model", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\nfrom functools import cached_property\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport torch\nimport torch.nn as nn\nfrom loguru import logger\n</pre> import dataclasses from functools import cached_property  import numpy as np import pandas as pd import plotly.express as px import torch import torch.nn as nn from loguru import logger In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass DiffusionPocessParams:\n    \"\"\"Parameter that defines a diffusion process.\n\n    :param steps: Number of steps in the diffusion process.\n    :param beta: Beta parameter for the diffusion process.\n    \"\"\"\n\n    steps: int\n    beta: float\n\n    @cached_property\n    def alpha(self) -&gt; float:\n        r\"\"\"$\\alpha = 1 - \\beta$\"\"\"\n        return 1.0 - self.beta\n\n    @cached_property\n    def beta_by_step(self) -&gt; np.ndarray:\n        \"\"\"the beta parameter for each step\n        in the diffusion process.\n        \"\"\"\n        return np.array([self.beta] * self.steps)\n\n    @cached_property\n    def alpha_by_step(self) -&gt; np.ndarray:\n        \"\"\"the alpha parameter for each step\n        in the diffusion process.\"\"\"\n        return np.array([self.alpha] * self.steps)\n\n\nclass DiffusionProcess:\n    \"\"\"\n    Diffusion process.\n\n    :param params: DiffusionParams that defines\n        how the diffusion process works\n    :param noise: noise tensor,\n        shape is (batch_size, params.steps)\n    \"\"\"\n\n    def __init__(\n        self,\n        params: DiffusionPocessParams,\n        noise: torch.Tensor,\n        dtype: torch.dtype = torch.float32,\n    ):\n        self.params = params\n        self.noise = noise\n        self.dtype = dtype\n\n    @cached_property\n    def alpha_by_step(self) -&gt; torch.Tensor:\n        \"\"\"The alpha parameter for each step\n        in the diffusion process.\n        \"\"\"\n        return torch.tensor(self.params.alpha_by_step, dtype=self.dtype)\n\n    def _forward_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know\n        the noise at step $t$,\n\n        $$\n        x(t) = \\sqrt{\\alpha(t)}x(t-1)\n        + \\sqrt{1 - \\alpha(t)}\\epsilon(t)\n        $$\n\n        :param state: The state at step $t-1$.\n        :param step: The current step $t$.\n        :return: The state at step $t$.\n        \"\"\"\n        return (\n            torch.sqrt(self.alpha_by_step[step]) * state\n            + torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]\n        )\n\n    def _inverse_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know\n        the noise at step $t$,\n\n        $$\n        x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}\n        (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))\n        $$\n        \"\"\"\n        return (\n            state - torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]\n        ) / torch.sqrt(self.alpha_by_step[step])\n\n\ndef gaussian_noise(n_var: int, length: int) -&gt; torch.Tensor:\n    \"\"\"Generate a Gaussian noise tensor.\n\n    :param n_var: Number of variables.\n    :param length: Length of the tensor.\n    \"\"\"\n    return torch.normal(mean=0, std=1, size=(n_var, length))\n</pre> @dataclasses.dataclass class DiffusionPocessParams:     \"\"\"Parameter that defines a diffusion process.      :param steps: Number of steps in the diffusion process.     :param beta: Beta parameter for the diffusion process.     \"\"\"      steps: int     beta: float      @cached_property     def alpha(self) -&gt; float:         r\"\"\"$\\alpha = 1 - \\beta$\"\"\"         return 1.0 - self.beta      @cached_property     def beta_by_step(self) -&gt; np.ndarray:         \"\"\"the beta parameter for each step         in the diffusion process.         \"\"\"         return np.array([self.beta] * self.steps)      @cached_property     def alpha_by_step(self) -&gt; np.ndarray:         \"\"\"the alpha parameter for each step         in the diffusion process.\"\"\"         return np.array([self.alpha] * self.steps)   class DiffusionProcess:     \"\"\"     Diffusion process.      :param params: DiffusionParams that defines         how the diffusion process works     :param noise: noise tensor,         shape is (batch_size, params.steps)     \"\"\"      def __init__(         self,         params: DiffusionPocessParams,         noise: torch.Tensor,         dtype: torch.dtype = torch.float32,     ):         self.params = params         self.noise = noise         self.dtype = dtype      @cached_property     def alpha_by_step(self) -&gt; torch.Tensor:         \"\"\"The alpha parameter for each step         in the diffusion process.         \"\"\"         return torch.tensor(self.params.alpha_by_step, dtype=self.dtype)      def _forward_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:         r\"\"\"Assuming that we know         the noise at step $t$,          $$         x(t) = \\sqrt{\\alpha(t)}x(t-1)         + \\sqrt{1 - \\alpha(t)}\\epsilon(t)         $$          :param state: The state at step $t-1$.         :param step: The current step $t$.         :return: The state at step $t$.         \"\"\"         return (             torch.sqrt(self.alpha_by_step[step]) * state             + torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]         )      def _inverse_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:         r\"\"\"Assuming that we know         the noise at step $t$,          $$         x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}         (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))         $$         \"\"\"         return (             state - torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]         ) / torch.sqrt(self.alpha_by_step[step])   def gaussian_noise(n_var: int, length: int) -&gt; torch.Tensor:     \"\"\"Generate a Gaussian noise tensor.      :param n_var: Number of variables.     :param length: Length of the tensor.     \"\"\"     return torch.normal(mean=0, std=1, size=(n_var, length)) In\u00a0[\u00a0]: Copied! <pre>diffusion_process_params = DiffusionPocessParams(\n    steps=100,\n    beta=0.005,\n    # beta=0,\n)\ndiffusion_batch_size = 1000\n# diffusion_batch_size = 2\n\nnoise = gaussian_noise(diffusion_batch_size, diffusion_process_params.steps)\n\ndiffusion_process = DiffusionProcess(diffusion_process_params, noise=noise)\n</pre> diffusion_process_params = DiffusionPocessParams(     steps=100,     beta=0.005,     # beta=0, ) diffusion_batch_size = 1000 # diffusion_batch_size = 2  noise = gaussian_noise(diffusion_batch_size, diffusion_process_params.steps)  diffusion_process = DiffusionProcess(diffusion_process_params, noise=noise) In\u00a0[\u00a0]: Copied! <pre># diffusion_initial_x = torch.sin(\n#     torch.linspace(0, 1, diffusion_batch_size)\n#     .reshape(diffusion_batch_size)\n# )\n\ndiffusion_initial_x = torch.rand(diffusion_batch_size)\n# diffusion_initial_x = (\n#     torch.distributions.Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n#     .sample((diffusion_batch_size, 1))\n#     .reshape(diffusion_batch_size)\n# )\n\ndiffusion_initial_x\n</pre> # diffusion_initial_x = torch.sin( #     torch.linspace(0, 1, diffusion_batch_size) #     .reshape(diffusion_batch_size) # )  diffusion_initial_x = torch.rand(diffusion_batch_size) # diffusion_initial_x = ( #     torch.distributions.Beta(torch.tensor([0.5]), torch.tensor([0.5])) #     .sample((diffusion_batch_size, 1)) #     .reshape(diffusion_batch_size) # )  diffusion_initial_x In\u00a0[\u00a0]: Copied! <pre>diffusion_steps_step_by_step = [diffusion_initial_x.detach().numpy()]\n\nfor i in range(0, diffusion_process_params.steps):\n    logger.info(f\"step {i}\")\n    i_state = (\n        diffusion_process._forward_process_by_step(\n            torch.from_numpy(diffusion_steps_step_by_step[-1]), step=i\n        )\n        .detach()\n        .numpy()\n    )\n    logger.info(f\"i_state {i_state[:2]}\")\n    diffusion_steps_step_by_step.append(i_state)\n</pre> diffusion_steps_step_by_step = [diffusion_initial_x.detach().numpy()]  for i in range(0, diffusion_process_params.steps):     logger.info(f\"step {i}\")     i_state = (         diffusion_process._forward_process_by_step(             torch.from_numpy(diffusion_steps_step_by_step[-1]), step=i         )         .detach()         .numpy()     )     logger.info(f\"i_state {i_state[:2]}\")     diffusion_steps_step_by_step.append(i_state)  In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_initial_x)\n</pre> px.histogram(diffusion_initial_x) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_step_by_step[0])\n</pre> px.histogram(diffusion_steps_step_by_step[0]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_step_by_step[-1])\n</pre> px.histogram(diffusion_steps_step_by_step[-1]) In\u00a0[\u00a0]: Copied! <pre>diffusion_steps_reverse = [diffusion_steps_step_by_step[-1]]\n\nfor i in range(diffusion_process_params.steps - 1, -1, -1):\n    logger.info(f\"step {i}\")\n    i_state = (\n        diffusion_process._inverse_process_by_step(\n            torch.from_numpy(diffusion_steps_reverse[-1]), step=i\n        )\n        .detach()\n        .numpy()\n    )\n    logger.info(f\"i_state {i_state[:2]}\")\n    diffusion_steps_reverse.append(i_state)\n</pre> diffusion_steps_reverse = [diffusion_steps_step_by_step[-1]]  for i in range(diffusion_process_params.steps - 1, -1, -1):     logger.info(f\"step {i}\")     i_state = (         diffusion_process._inverse_process_by_step(             torch.from_numpy(diffusion_steps_reverse[-1]), step=i         )         .detach()         .numpy()     )     logger.info(f\"i_state {i_state[:2]}\")     diffusion_steps_reverse.append(i_state)  In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_reverse[0])\n</pre> px.histogram(diffusion_steps_reverse[0]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_reverse[-1])\n</pre> px.histogram(diffusion_steps_reverse[-1]) In\u00a0[\u00a0]: Copied! <pre>df_diffusion_example = pd.DataFrame(\n    {i: v for i, v in enumerate(diffusion_steps_step_by_step)}\n).T\ndf_diffusion_example[\"step\"] = df_diffusion_example.index\n\ndf_diffusion_example_melted = df_diffusion_example.melt(\n    id_vars=[\"step\"], var_name=\"variable\", value_name=\"value\"\n)\ndf_diffusion_example_melted.tail()\n</pre> df_diffusion_example = pd.DataFrame(     {i: v for i, v in enumerate(diffusion_steps_step_by_step)} ).T df_diffusion_example[\"step\"] = df_diffusion_example.index  df_diffusion_example_melted = df_diffusion_example.melt(     id_vars=[\"step\"], var_name=\"variable\", value_name=\"value\" ) df_diffusion_example_melted.tail() In\u00a0[\u00a0]: Copied! <pre>px.histogram(\n    df_diffusion_example_melted,\n    x=\"value\",\n    histnorm=\"probability density\",\n    animation_frame=\"step\",\n)\n</pre> px.histogram(     df_diffusion_example_melted,     x=\"value\",     histnorm=\"probability density\",     animation_frame=\"step\", ) In\u00a0[\u00a0]: Copied! <pre>px.violin(\n    df_diffusion_example_melted.loc[\n        df_diffusion_example_melted[\"step\"].isin(\n            [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n        )\n    ],\n    x=\"step\",\n    y=\"value\",\n)\n</pre> px.violin(     df_diffusion_example_melted.loc[         df_diffusion_example_melted[\"step\"].isin(             [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]         )     ],     x=\"step\",     y=\"value\", ) In\u00a0[\u00a0]: Copied! <pre>px.line(\n    df_diffusion_example_melted,\n    x=\"step\",\n    y=\"value\",\n    color=\"variable\",\n)\n</pre> px.line(     df_diffusion_example_melted,     x=\"step\",     y=\"value\",     color=\"variable\", ) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(\n    df_diffusion_example_melted.loc[df_diffusion_example_melted[\"step\"] == 0],\n    x=\"value\",\n    stat=\"probability\",\n    color=\"k\",\n    label=\"Initial Distribution\",\n    ax=ax,\n)\n\nax.set_title(\"Initial Distribution\")\nax.set_xlabel(\"Position\")\n</pre> _, ax = plt.subplots(figsize=(10, 6)) sns.histplot(     df_diffusion_example_melted.loc[df_diffusion_example_melted[\"step\"] == 0],     x=\"value\",     stat=\"probability\",     color=\"k\",     label=\"Initial Distribution\",     ax=ax, )  ax.set_title(\"Initial Distribution\") ax.set_xlabel(\"Position\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6))\n\nsns.histplot(\n    df_diffusion_example_melted.loc[\n        df_diffusion_example_melted[\"step\"] == max(df_diffusion_example_melted[\"step\"])\n    ],\n    x=\"value\",\n    stat=\"probability\",\n    color=\"k\",\n    ax=ax,\n)\n\nax.set_title(\"Final Distribution\")\nax.set_xlabel(\"Position\")\n</pre> _, ax = plt.subplots(figsize=(10, 6))  sns.histplot(     df_diffusion_example_melted.loc[         df_diffusion_example_melted[\"step\"] == max(df_diffusion_example_melted[\"step\"])     ],     x=\"value\",     stat=\"probability\",     color=\"k\",     ax=ax, )  ax.set_title(\"Final Distribution\") ax.set_xlabel(\"Position\") In\u00a0[\u00a0]: Copied! <pre>df_diffusion_example_melted\n</pre> df_diffusion_example_melted In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(15, 6))\n\nsns.lineplot(\n    df_diffusion_example_melted.loc[\n        df_diffusion_example_melted[\"variable\"].isin(\n            [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n        )\n    ].rename(columns={\"variable\": \"particle id\"}),\n    x=\"step\",\n    y=\"value\",\n    style=\"particle id\",\n)\n\nax.set_xlabel(\"Time Step\")\nax.set_ylabel(\"Particle Position\")\n</pre> _, ax = plt.subplots(figsize=(15, 6))  sns.lineplot(     df_diffusion_example_melted.loc[         df_diffusion_example_melted[\"variable\"].isin(             [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]         )     ].rename(columns={\"variable\": \"particle id\"}),     x=\"step\",     y=\"value\",     style=\"particle id\", )  ax.set_xlabel(\"Time Step\") ax.set_ylabel(\"Particle Position\") In\u00a0[\u00a0]: Copied! <pre>ridge_steps = 10\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\ncolors = plt.cm.viridis(np.linspace(0, 1, ridge_steps))\nbin_edges = np.histogram_bin_edges(df_diffusion_example_melted[\"value\"], bins=\"auto\")\n\nfor i, step in enumerate([0, 10, 20, 30, 40, 50, 60, 70, 80, 90]):\n    values = df_diffusion_example_melted[df_diffusion_example_melted[\"step\"] == step][\n        \"value\"\n    ]\n    counts, _ = np.histogram(values, bins=bin_edges)\n    counts = counts / counts.max()\n\n    offset = i * 1.2\n    ax.fill_between(\n        bin_edges[:-1], offset, counts + offset, step=\"mid\", color=colors[i], alpha=0.7\n    )\n    ax.text(bin_edges[-1] + 0.2, offset, step, va=\"center\")\n\n\nax.axes.get_yaxis().set_visible(False)\n\nplt.axis(\"off\")\nax.set_xlabel(\"Position\")\nplt.tight_layout()\n</pre> ridge_steps = 10   fig, ax = plt.subplots(figsize=(8, 6)) colors = plt.cm.viridis(np.linspace(0, 1, ridge_steps)) bin_edges = np.histogram_bin_edges(df_diffusion_example_melted[\"value\"], bins=\"auto\")  for i, step in enumerate([0, 10, 20, 30, 40, 50, 60, 70, 80, 90]):     values = df_diffusion_example_melted[df_diffusion_example_melted[\"step\"] == step][         \"value\"     ]     counts, _ = np.histogram(values, bins=bin_edges)     counts = counts / counts.max()      offset = i * 1.2     ax.fill_between(         bin_edges[:-1], offset, counts + offset, step=\"mid\", color=colors[i], alpha=0.7     )     ax.text(bin_edges[-1] + 0.2, offset, step, va=\"center\")   ax.axes.get_yaxis().set_visible(False)  plt.axis(\"off\") ax.set_xlabel(\"Position\") plt.tight_layout()  <p>We create a naive model based on the idea of diffusion.</p> <ol> <li>Connect the real data to the latent space through diffusion process.</li> <li>We forecast in the latent space.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import lightning.pytorch as pl\nfrom lightning import LightningModule\n</pre> import lightning.pytorch as pl from lightning import LightningModule In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass LatentRNNParams:\n    \"\"\"Parameters for Diffusion process.\n\n    :param latent_size: latent space dimension\n    :param history_length: input sequence length\n    :param n_features: number of features\n    \"\"\"\n\n    history_length: int\n    latent_size: int = 100\n    num_layers: int = 2\n    n_features: int = 1\n    initial_state: torch.Tensor = None\n\n    @cached_property\n    def data_size(self) -&gt; int:\n        \"\"\"The dimension of the input data\n        when flattened.\n        \"\"\"\n        return self.sequence_length * self.n_features\n\n    def asdict(self) -&gt; dict:\n        return dataclasses.asdict(self)\n\n\nclass LatentRNN(nn.Module):\n    \"\"\"Forecasting the next step in latent space.\"\"\"\n\n    def __init__(self, params: LatentRNNParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        self.rnn = nn.GRU(\n            input_size=self.params.history_length,\n            hidden_size=self.params.latent_size,\n            num_layers=self.params.num_layers,\n            batch_first=True,\n        )\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n\n        :param x: input data, shape (batch_size, history_length * n_features)\n        \"\"\"\n\n        outputs, _ = self.rnn(x, self.params.initial_state)\n\n        return outputs\n\n\nclass DiffusionEncoder(nn.Module):\n    \"\"\"Encode the time series into the latent space.\"\"\"\n\n    def __init__(\n        self,\n        params: DiffusionPocessParams,\n        noise: torch.Tensor,\n    ):\n        super().__init__()\n        self.params = params\n        self.noise = noise\n\n    @staticmethod\n    def _forward_process_by_step(\n        state: torch.Tensor, alpha_by_step: torch.Tensor, noise: torch.Tensor, step: int\n    ) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know the noise at step $t$,\n\n        $$\n        x(t) = \\sqrt{\\alpha(t)}x(t-1) + \\sqrt{1 - \\alpha(t)}\\epsilon(t)\n        $$\n        \"\"\"\n        batch_size = state.shape[0]\n        return torch.sqrt(alpha_by_step[step]) * state + (\n            torch.sqrt(1 - alpha_by_step[step]) * noise[:batch_size, step]\n        ).reshape(batch_size, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encoding the latent space into a distribution.\n\n        :param x: input data, shape (batch_size, history_length, n_features)\n        \"\"\"\n\n        alpha_by_step = torch.tensor(self.params.alpha_by_step).to(x)\n        self.noise = self.noise.to(x)\n        # logger.debug(\n        #     f\"alpha_by_step: {alpha_by_step.shape}\"\n        #     f\"noise: {self.noise.shape}\"\n        #     f\"x: {x.shape}\"\n        # )\n\n        diffusion_steps_step_by_step = [x]\n\n        for i in range(0, self.params.steps):\n            i_state = self._forward_process_by_step(\n                diffusion_steps_step_by_step[-1],\n                alpha_by_step=alpha_by_step,\n                noise=self.noise,\n                step=i,\n            )\n            diffusion_steps_step_by_step.append(i_state)\n\n        return diffusion_steps_step_by_step[-1]\n\n\nclass DiffusionDecoder(nn.Module):\n    \"\"\"Decode the latent space into a distribution.\"\"\"\n\n    def __init__(\n        self,\n        params: DiffusionPocessParams,\n        noise: torch.Tensor,\n    ):\n        super().__init__()\n        self.params = params\n        self.noise = noise\n\n    @staticmethod\n    def _inverse_process_by_step(\n        state: torch.Tensor, alpha_by_step: torch.Tensor, noise: torch.Tensor, step: int\n    ) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know the noise at step $t$,\n\n        $$\n        x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}\n        (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))\n        $$\n        \"\"\"\n        batch_size = state.shape[0]\n        return (\n            state\n            - (torch.sqrt(1 - alpha_by_step[step]) * noise[:batch_size, step]).reshape(\n                batch_size, 1\n            )\n        ) / torch.sqrt(alpha_by_step[step])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encoding the latent space into a distribution.\n\n        :param x: input data, shape (batch_size, history_length, n_features)\n        \"\"\"\n        alpha_by_step = torch.tensor(self.params.alpha_by_step).to(x)\n        self.noise = self.noise.to(x)\n\n        diffusion_steps_reverse = [x]\n\n        for i in range(self.params.steps - 1, -1, -1):\n            i_state = self._inverse_process_by_step(\n                state=diffusion_steps_reverse[-1],\n                alpha_by_step=alpha_by_step,\n                noise=self.noise,\n                step=i,\n            )\n            diffusion_steps_reverse.append(i_state)\n\n        return diffusion_steps_reverse[-1]\n\n\nclass NaiveDiffusionModel(nn.Module):\n    \"\"\"A naive diffusion model that explicitly calculates\n    the diffusion process.\n    \"\"\"\n\n    def __init__(\n        self,\n        rnn: LatentRNN,\n        diffusion_decoder: DiffusionDecoder,\n        diffusion_encoder: DiffusionEncoder,\n        horizon: int = 1,\n    ):\n        super().__init__()\n        self.rnn = rnn\n        self.diffusion_decoder = diffusion_decoder\n        self.diffusion_encoder = diffusion_encoder\n        self.horizon = horizon\n        self.scale = nn.Linear(\n            in_features=self.rnn.params.latent_size,\n            out_features=self.horizon,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\n        # logger.debug(f\"x.squeeze(-1): {x.squeeze(-1).shape=}\")\n        x_latent = self.diffusion_encoder(x.squeeze(-1))\n        # logger.debug(f\"x_latent: {x_latent.shape=}\")\n        y_latent = self.rnn(x_latent)\n        # logger.debug(f\"y_latent: {y_latent.shape=}\")\n        y_hat = self.diffusion_decoder(y_latent)\n        # logger.debug(f\"y_hat: {y_hat.shape=}\")\n        y_hat = self.scale(y_hat)\n        # logger.debug(f\"scaled y_hat: {y_hat.shape=}\")\n\n        return y_hat\n\n\nclass NaiveDiffusionForecaster(LightningModule):\n    \"\"\"A assembled lightning module for the naive diffusion model.\"\"\"\n\n    def __init__(\n        self,\n        model: NaiveDiffusionModel,\n        loss: nn.Module = nn.MSELoss(),\n    ):\n        super().__init__()\n        self.model = model\n        self.loss = loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.type(self.dtype)\n        y = y.type(self.dtype)\n        batch_size = x.shape[0]\n\n        y_hat = self.model(x)[:batch_size, :].reshape_as(y)\n\n        loss = self.loss(y_hat, y).mean()\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.type(self.dtype)\n        y = y.type(self.dtype)\n        batch_size = x.shape[0]\n\n        y_hat = self.model(x)[:batch_size, :].reshape_as(y)\n\n        loss = self.loss(y_hat, y).mean()\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.type(self.dtype)\n        y = y.type(self.dtype)\n        batch_size = x.shape[0]\n\n        y_hat = self.model(x)[:batch_size, :].reshape_as(y)\n        return x, y_hat\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x.to(self.model.rnn.rnn.weight_ih_l0)\n        return self.model(x)\n</pre> @dataclasses.dataclass class LatentRNNParams:     \"\"\"Parameters for Diffusion process.      :param latent_size: latent space dimension     :param history_length: input sequence length     :param n_features: number of features     \"\"\"      history_length: int     latent_size: int = 100     num_layers: int = 2     n_features: int = 1     initial_state: torch.Tensor = None      @cached_property     def data_size(self) -&gt; int:         \"\"\"The dimension of the input data         when flattened.         \"\"\"         return self.sequence_length * self.n_features      def asdict(self) -&gt; dict:         return dataclasses.asdict(self)   class LatentRNN(nn.Module):     \"\"\"Forecasting the next step in latent space.\"\"\"      def __init__(self, params: LatentRNNParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          self.rnn = nn.GRU(             input_size=self.params.history_length,             hidden_size=self.params.latent_size,             num_layers=self.params.num_layers,             batch_first=True,         )      def forward(         self, x: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"          :param x: input data, shape (batch_size, history_length * n_features)         \"\"\"          outputs, _ = self.rnn(x, self.params.initial_state)          return outputs   class DiffusionEncoder(nn.Module):     \"\"\"Encode the time series into the latent space.\"\"\"      def __init__(         self,         params: DiffusionPocessParams,         noise: torch.Tensor,     ):         super().__init__()         self.params = params         self.noise = noise      @staticmethod     def _forward_process_by_step(         state: torch.Tensor, alpha_by_step: torch.Tensor, noise: torch.Tensor, step: int     ) -&gt; torch.Tensor:         r\"\"\"Assuming that we know the noise at step $t$,          $$         x(t) = \\sqrt{\\alpha(t)}x(t-1) + \\sqrt{1 - \\alpha(t)}\\epsilon(t)         $$         \"\"\"         batch_size = state.shape[0]         return torch.sqrt(alpha_by_step[step]) * state + (             torch.sqrt(1 - alpha_by_step[step]) * noise[:batch_size, step]         ).reshape(batch_size, 1)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"Encoding the latent space into a distribution.          :param x: input data, shape (batch_size, history_length, n_features)         \"\"\"          alpha_by_step = torch.tensor(self.params.alpha_by_step).to(x)         self.noise = self.noise.to(x)         # logger.debug(         #     f\"alpha_by_step: {alpha_by_step.shape}\"         #     f\"noise: {self.noise.shape}\"         #     f\"x: {x.shape}\"         # )          diffusion_steps_step_by_step = [x]          for i in range(0, self.params.steps):             i_state = self._forward_process_by_step(                 diffusion_steps_step_by_step[-1],                 alpha_by_step=alpha_by_step,                 noise=self.noise,                 step=i,             )             diffusion_steps_step_by_step.append(i_state)          return diffusion_steps_step_by_step[-1]   class DiffusionDecoder(nn.Module):     \"\"\"Decode the latent space into a distribution.\"\"\"      def __init__(         self,         params: DiffusionPocessParams,         noise: torch.Tensor,     ):         super().__init__()         self.params = params         self.noise = noise      @staticmethod     def _inverse_process_by_step(         state: torch.Tensor, alpha_by_step: torch.Tensor, noise: torch.Tensor, step: int     ) -&gt; torch.Tensor:         r\"\"\"Assuming that we know the noise at step $t$,          $$         x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}         (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))         $$         \"\"\"         batch_size = state.shape[0]         return (             state             - (torch.sqrt(1 - alpha_by_step[step]) * noise[:batch_size, step]).reshape(                 batch_size, 1             )         ) / torch.sqrt(alpha_by_step[step])      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"Encoding the latent space into a distribution.          :param x: input data, shape (batch_size, history_length, n_features)         \"\"\"         alpha_by_step = torch.tensor(self.params.alpha_by_step).to(x)         self.noise = self.noise.to(x)          diffusion_steps_reverse = [x]          for i in range(self.params.steps - 1, -1, -1):             i_state = self._inverse_process_by_step(                 state=diffusion_steps_reverse[-1],                 alpha_by_step=alpha_by_step,                 noise=self.noise,                 step=i,             )             diffusion_steps_reverse.append(i_state)          return diffusion_steps_reverse[-1]   class NaiveDiffusionModel(nn.Module):     \"\"\"A naive diffusion model that explicitly calculates     the diffusion process.     \"\"\"      def __init__(         self,         rnn: LatentRNN,         diffusion_decoder: DiffusionDecoder,         diffusion_encoder: DiffusionEncoder,         horizon: int = 1,     ):         super().__init__()         self.rnn = rnn         self.diffusion_decoder = diffusion_decoder         self.diffusion_encoder = diffusion_encoder         self.horizon = horizon         self.scale = nn.Linear(             in_features=self.rnn.params.latent_size,             out_features=self.horizon,         )      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:          # logger.debug(f\"x.squeeze(-1): {x.squeeze(-1).shape=}\")         x_latent = self.diffusion_encoder(x.squeeze(-1))         # logger.debug(f\"x_latent: {x_latent.shape=}\")         y_latent = self.rnn(x_latent)         # logger.debug(f\"y_latent: {y_latent.shape=}\")         y_hat = self.diffusion_decoder(y_latent)         # logger.debug(f\"y_hat: {y_hat.shape=}\")         y_hat = self.scale(y_hat)         # logger.debug(f\"scaled y_hat: {y_hat.shape=}\")          return y_hat   class NaiveDiffusionForecaster(LightningModule):     \"\"\"A assembled lightning module for the naive diffusion model.\"\"\"      def __init__(         self,         model: NaiveDiffusionModel,         loss: nn.Module = nn.MSELoss(),     ):         super().__init__()         self.model = model         self.loss = loss      def configure_optimizers(self):         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)         return optimizer      def training_step(self, batch, batch_idx):         x, y = batch         x = x.type(self.dtype)         y = y.type(self.dtype)         batch_size = x.shape[0]          y_hat = self.model(x)[:batch_size, :].reshape_as(y)          loss = self.loss(y_hat, y).mean()         self.log_dict({\"train_loss\": loss}, prog_bar=True)          return loss      def validation_step(self, batch, batch_idx):         x, y = batch         x = x.type(self.dtype)         y = y.type(self.dtype)         batch_size = x.shape[0]          y_hat = self.model(x)[:batch_size, :].reshape_as(y)          loss = self.loss(y_hat, y).mean()         self.log_dict({\"val_loss\": loss}, prog_bar=True)         return loss      def predict_step(self, batch, batch_idx):         x, y = batch         x = x.type(self.dtype)         y = y.type(self.dtype)         batch_size = x.shape[0]          y_hat = self.model(x)[:batch_size, :].reshape_as(y)         return x, y_hat      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = x.to(self.model.rnn.rnn.weight_ih_l0)         return self.model(x) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(\n    {\"t\": np.linspace(0, 100, 501), \"y\": np.sin(np.linspace(0, 100, 501))}\n)\n\n_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"y\", ax=ax)\n</pre> df = pd.DataFrame(     {\"t\": np.linspace(0, 100, 501), \"y\": np.sin(np.linspace(0, 100, 501))} )  _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"y\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>from ts_bolt.datamodules.pandas import DataFrameDataModule\n</pre> from ts_bolt.datamodules.pandas import DataFrameDataModule In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\ntraining_batch_size = 64\n\ntraining_noise = gaussian_noise(training_batch_size, diffusion_process_params.steps)\n</pre> history_length_1_step = 100 horizon_1_step = 1 training_batch_size = 64  training_noise = gaussian_noise(training_batch_size, diffusion_process_params.steps) In\u00a0[\u00a0]: Copied! <pre>diffusion_process_params.alpha_by_step.shape, training_noise.shape\n</pre> diffusion_process_params.alpha_by_step.shape, training_noise.shape In\u00a0[\u00a0]: Copied! <pre>test_state = torch.rand(training_batch_size, diffusion_process_params.steps)\ntest_state.shape\n</pre> test_state = torch.rand(training_batch_size, diffusion_process_params.steps) test_state.shape In\u00a0[\u00a0]: Copied! <pre>torch.sqrt(torch.from_numpy(diffusion_process_params.alpha_by_step)[0])\n</pre> torch.sqrt(torch.from_numpy(diffusion_process_params.alpha_by_step)[0]) In\u00a0[\u00a0]: Copied! <pre>(\n    test_state\n    - torch.sqrt(torch.from_numpy(diffusion_process_params.alpha_by_step)[0])\n    * training_noise[:, 0].reshape(training_batch_size, 1)\n).shape\n</pre> (     test_state     - torch.sqrt(torch.from_numpy(diffusion_process_params.alpha_by_step)[0])     * training_noise[:, 0].reshape(training_batch_size, 1) ).shape In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = DataFrameDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    dataframe=df[[\"y\"]].astype(np.float32),\n    batch_size=training_batch_size,\n)\n</pre> pdm_1_step = DataFrameDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     dataframe=df[[\"y\"]].astype(np.float32),     batch_size=training_batch_size, ) In\u00a0[\u00a0]: Copied! <pre>diffusion_decoder = DiffusionDecoder(diffusion_process_params, training_noise)\ndiffusion_encoder = DiffusionEncoder(diffusion_process_params, training_noise)\n</pre> diffusion_decoder = DiffusionDecoder(diffusion_process_params, training_noise) diffusion_encoder = DiffusionEncoder(diffusion_process_params, training_noise) In\u00a0[\u00a0]: Copied! <pre>latent_rnn_params = LatentRNNParams(\n    history_length=history_length_1_step,\n    latent_size=diffusion_process_params.steps,\n)\n\nlatent_rnn = LatentRNN(latent_rnn_params)\n</pre> latent_rnn_params = LatentRNNParams(     history_length=history_length_1_step,     latent_size=diffusion_process_params.steps, )  latent_rnn = LatentRNN(latent_rnn_params) In\u00a0[\u00a0]: Copied! <pre>naive_diffusion_model = NaiveDiffusionModel(\n    rnn=latent_rnn,\n    diffusion_decoder=diffusion_decoder,\n    diffusion_encoder=diffusion_encoder,\n)\nnaive_diffusion_forecaster = NaiveDiffusionForecaster(\n    model=naive_diffusion_model.float(),\n)\n</pre> naive_diffusion_model = NaiveDiffusionModel(     rnn=latent_rnn,     diffusion_decoder=diffusion_decoder,     diffusion_encoder=diffusion_encoder, ) naive_diffusion_forecaster = NaiveDiffusionForecaster(     model=naive_diffusion_model.float(), ) In\u00a0[\u00a0]: Copied! <pre>naive_diffusion_forecaster\n</pre> naive_diffusion_forecaster In\u00a0[\u00a0]: Copied! <pre>logger_1_step = pl.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"naive_diffusion_ts_1_step\"\n)\nprecision = \"64\"\ntrainer_1_step = pl.Trainer(\n    # precision=\"32\",\n    precision=precision,\n    # max_epochs=5000,\n    max_epochs=10000,\n    min_epochs=5,\n    # callbacks=[\n    #     pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-8, patience=4)\n    # ],\n    logger=logger_1_step,\n    # accelerator=\"mps\",\n    accelerator=\"cuda\",\n)\n</pre> logger_1_step = pl.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"naive_diffusion_ts_1_step\" ) precision = \"64\" trainer_1_step = pl.Trainer(     # precision=\"32\",     precision=precision,     # max_epochs=5000,     max_epochs=10000,     min_epochs=5,     # callbacks=[     #     pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-8, patience=4)     # ],     logger=logger_1_step,     # accelerator=\"mps\",     accelerator=\"cuda\", ) In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=naive_diffusion_forecaster, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=naive_diffusion_forecaster, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>from typing import Dict, List, Sequence, Tuple\n\nimport matplotlib as mpl\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchmetrics import MetricCollection\nfrom torchmetrics.regression import (\n    MeanAbsoluteError,\n    MeanAbsolutePercentageError,\n    MeanSquaredError,\n    SymmetricMeanAbsolutePercentageError,\n)\nfrom ts_bolt.evaluation.evaluator import Evaluator\nfrom ts_bolt.naive_forecasters.last_observation import LastObservationForecaster\n\n\nclass Evaluator:\n    \"\"\"Evaluate the predictions\n\n    :param step: which prediction step to be evaluated.\n    :param gap: gap between input history and target/prediction.\n    \"\"\"\n\n    def __init__(self, step: int = 0, gap: int = 0):\n        self.step = step\n        self.gap = gap\n\n    @staticmethod\n    def get_one_history(\n        predictions: Sequence[Sequence], idx: int, batch_idx: int = 0\n    ) -&gt; torch.Tensor:\n        return predictions[batch_idx][0][idx, ...]\n\n    @staticmethod\n    def get_one_pred(predictions: List, idx: int, batch_idx: int = 0) -&gt; torch.Tensor:\n        return predictions[batch_idx][1][idx, ...]\n\n    @staticmethod\n    def get_y(predictions: List, step: int) -&gt; List[torch.Tensor]:\n        return [i[1][..., step] for i in predictions]\n\n    def y(self, predictions: List, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y(predictions, self.step)[batch_idx].detach()\n\n    @staticmethod\n    def get_y_true(dataloader: DataLoader, step: int) -&gt; list[torch.Tensor]:\n        return [i[1][..., step] for i in dataloader]\n\n    def y_true(self, dataloader: DataLoader, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y_true(dataloader, step=self.step)[batch_idx].detach()\n\n    def get_one_sample(\n        self, predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        return (\n            self.get_one_history(predictions, idx, batch_idx),\n            self.get_one_pred(predictions, idx, batch_idx),\n        )\n\n    def plot_one_sample(\n        self, ax: mpl.axes.Axes, predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; None:\n        history, pred = self.get_one_sample(predictions, idx, batch_idx)\n\n        x_raw = np.arange(len(history) + len(pred) + self.gap)\n        x_history = x_raw[: len(history)]\n        x_pred = x_raw[len(history) + self.gap :]\n        x = np.concatenate([x_history, x_pred])\n\n        y = np.concatenate([history, pred])\n\n        ax.plot(x, y, marker=\".\", label=f\"input ({idx})\")\n\n        ax.axvspan(x_pred[0], x_pred[-1], color=\"orange\", alpha=0.1)\n\n    @property\n    def metric_collection(self) -&gt; MetricCollection:\n        return MetricCollection(\n            MeanAbsoluteError(),\n            MeanAbsolutePercentageError(),\n            MeanSquaredError(),\n            SymmetricMeanAbsolutePercentageError(),\n        )\n\n    @staticmethod\n    def metric_dataframe(metrics: Dict) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            [{k: float(v) for k, v in metrics.items()}], index=[\"values\"]\n        ).T\n\n    def metrics(\n        self, predictions: List, dataloader: DataLoader, batch_idx: int = 0\n    ) -&gt; pd.DataFrame:\n        truths = self.y_true(dataloader)\n        preds = self.y(predictions, batch_idx=batch_idx)\n\n        return self.metric_dataframe(self.metric_collection(preds, truths))\n</pre> from typing import Dict, List, Sequence, Tuple  import matplotlib as mpl import numpy as np import pandas as pd import torch from torch.utils.data import DataLoader from torchmetrics import MetricCollection from torchmetrics.regression import (     MeanAbsoluteError,     MeanAbsolutePercentageError,     MeanSquaredError,     SymmetricMeanAbsolutePercentageError, ) from ts_bolt.evaluation.evaluator import Evaluator from ts_bolt.naive_forecasters.last_observation import LastObservationForecaster   class Evaluator:     \"\"\"Evaluate the predictions      :param step: which prediction step to be evaluated.     :param gap: gap between input history and target/prediction.     \"\"\"      def __init__(self, step: int = 0, gap: int = 0):         self.step = step         self.gap = gap      @staticmethod     def get_one_history(         predictions: Sequence[Sequence], idx: int, batch_idx: int = 0     ) -&gt; torch.Tensor:         return predictions[batch_idx][0][idx, ...]      @staticmethod     def get_one_pred(predictions: List, idx: int, batch_idx: int = 0) -&gt; torch.Tensor:         return predictions[batch_idx][1][idx, ...]      @staticmethod     def get_y(predictions: List, step: int) -&gt; List[torch.Tensor]:         return [i[1][..., step] for i in predictions]      def y(self, predictions: List, batch_idx: int = 0) -&gt; torch.Tensor:         return self.get_y(predictions, self.step)[batch_idx].detach()      @staticmethod     def get_y_true(dataloader: DataLoader, step: int) -&gt; list[torch.Tensor]:         return [i[1][..., step] for i in dataloader]      def y_true(self, dataloader: DataLoader, batch_idx: int = 0) -&gt; torch.Tensor:         return self.get_y_true(dataloader, step=self.step)[batch_idx].detach()      def get_one_sample(         self, predictions: List, idx: int, batch_idx: int = 0     ) -&gt; Tuple[torch.Tensor, torch.Tensor]:         return (             self.get_one_history(predictions, idx, batch_idx),             self.get_one_pred(predictions, idx, batch_idx),         )      def plot_one_sample(         self, ax: mpl.axes.Axes, predictions: List, idx: int, batch_idx: int = 0     ) -&gt; None:         history, pred = self.get_one_sample(predictions, idx, batch_idx)          x_raw = np.arange(len(history) + len(pred) + self.gap)         x_history = x_raw[: len(history)]         x_pred = x_raw[len(history) + self.gap :]         x = np.concatenate([x_history, x_pred])          y = np.concatenate([history, pred])          ax.plot(x, y, marker=\".\", label=f\"input ({idx})\")          ax.axvspan(x_pred[0], x_pred[-1], color=\"orange\", alpha=0.1)      @property     def metric_collection(self) -&gt; MetricCollection:         return MetricCollection(             MeanAbsoluteError(),             MeanAbsolutePercentageError(),             MeanSquaredError(),             SymmetricMeanAbsolutePercentageError(),         )      @staticmethod     def metric_dataframe(metrics: Dict) -&gt; pd.DataFrame:         return pd.DataFrame(             [{k: float(v) for k, v in metrics.items()}], index=[\"values\"]         ).T      def metrics(         self, predictions: List, dataloader: DataLoader, batch_idx: int = 0     ) -&gt; pd.DataFrame:         truths = self.y_true(dataloader)         preds = self.y(predictions, batch_idx=batch_idx)          return self.metric_dataframe(self.metric_collection(preds, truths)) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=naive_diffusion_forecaster, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=naive_diffusion_forecaster, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\n# ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  # ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = pl.Trainer(precision=precision)\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = pl.Trainer(precision=precision)  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(\n    [[i.unsqueeze(-1) for i in lobs_1_step_predictions[0]]],\n    pdm_1_step.predict_dataloader(),\n)\n</pre> evaluator_1_step.metrics(     [[i.unsqueeze(-1) for i in lobs_1_step_predictions[0]]],     pdm_1_step.predict_dataloader(), )"}, {"location": "notebooks/diffusion_model/#diffusion-model", "title": "Diffusion Model\u00b6", "text": "<p>References:</p> <ol> <li><code>pts/model/time_grad/time_grad_network.py</code></li> </ol>"}, {"location": "notebooks/diffusion_model/#forward-process-step-by-step", "title": "Forward process step by step\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#reverse-step-by-step", "title": "Reverse step by step\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#diffusion-distributions", "title": "Diffusion Distributions\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#create-visuals", "title": "Create Visuals\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#model", "title": "Model\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#traning", "title": "Traning\u00b6", "text": ""}, {"location": "notebooks/diffusion_model/#evaluation", "title": "Evaluation\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/", "title": "Diffusion Model", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\nfrom functools import cached_property\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport torch\nimport torch.nn as nn\nfrom loguru import logger\n</pre> import dataclasses from functools import cached_property  import numpy as np import pandas as pd import plotly.express as px import torch import torch.nn as nn from loguru import logger In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass DiffusionParams:\n    \"\"\" \"\"\"\n\n    steps: int\n    beta: float\n\n    @cached_property\n    def alpha(self) -&gt; float:\n        \"\"\" \"\"\"\n        return 1.0 - self.beta\n\n    @cached_property\n    def alpha_bar_by_step(self) -&gt; np.ndarray:\n        \"\"\" \"\"\"\n        return np.cumprod(self.alpha_by_step)\n\n    @cached_property\n    def beta_by_step(self) -&gt; np.ndarray:\n        \"\"\" \"\"\"\n        return np.array([self.beta] * self.steps)\n\n    @cached_property\n    def alpha_by_step(self) -&gt; np.ndarray:\n        \"\"\" \"\"\"\n        return np.array([self.alpha] * self.steps)\n\n    # @cached_property\n    # def sigma_q_squared(self) -&gt; float:\n    #     r\"\"\"\n\n    #     $$\n    #     \\sigma_q^2(t) = \\frac{\n    #         (1 - \\alpha(t))(1 - \\bar\\alpha(t-1))\n    #     }{1 - \\bar\\alpha(t)}\n    #     $$\n    #     \"\"\"\n    #     sigma_q_squared = (\n    #         1 - self.alpha_by_step[1:]\n    #     ) * (1 - self.alpha_bar_by_step[:-1]) / (1 - self.alpha_bar_by_step[1:])\n    #     return np.insert(sigma_q_squared, 0, 0)\n\n\nclass Diffusion(nn.Module):\n    def __init__(\n        self,\n        batch_size: int,\n        params: DiffusionParams,\n        dtype: torch.dtype = torch.float32,\n    ):\n        self.batch_size = batch_size\n        self.params = params\n        self.dtype = dtype\n\n    @cached_property\n    def alpha_bar_by_step(self) -&gt; torch.Tensor:\n        return torch.tensor(self.params.alpha_bar_by_step, dtype=self.dtype).detach()\n\n    @cached_property\n    def alpha_by_step(self) -&gt; torch.Tensor:\n        return torch.tensor(self.params.alpha_by_step, dtype=self.dtype).detach()\n\n    # @cached_property\n    # def sigma_q_squared(self) -&gt; torch.Tensor:\n    #     return torch.tensor(self.params.sigma_q_squared, dtype=self.dtype)\n\n    def likelihood(self):\n        pass\n\n    def _gaussian(self, n_var: int, length: int) -&gt; torch.Tensor:\n        return torch.normal(mean=0, std=1, size=(n_var, length))\n\n    @cached_property\n    def noise(self) -&gt; torch.Tensor:\n        return self._gaussian(self.batch_size, self.params.steps).detach()\n\n    def _forward_process(self, initial: torch.Tensor) -&gt; torch.Tensor:\n        return (\n            torch.outer(\n                initial,\n                torch.sqrt(self.alpha_bar_by_step),\n            )\n            + torch.sqrt(1 - self.alpha_bar_by_step) * self.noise\n        )\n\n    def _forward_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know the noise at step $t$,\n\n        $$\n        x(t) = \\sqrt{\\alpha(t)}x(t-1) + \\sqrt{1 - \\alpha(t)}\\epsilon(t)\n        $$\n        \"\"\"\n        return (\n            torch.sqrt(self.alpha_by_step[step]) * state\n            + torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]\n        )\n\n    def _inverse_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:\n        r\"\"\"Assuming that we know the noise at step $t$,\n\n        $$\n        x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}\n        (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))\n        $$\n        \"\"\"\n        return (\n            state - torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]\n        ) / torch.sqrt(self.alpha_by_step[step])\n\n    def forward(self):\n        pass\n</pre> @dataclasses.dataclass class DiffusionParams:     \"\"\" \"\"\"      steps: int     beta: float      @cached_property     def alpha(self) -&gt; float:         \"\"\" \"\"\"         return 1.0 - self.beta      @cached_property     def alpha_bar_by_step(self) -&gt; np.ndarray:         \"\"\" \"\"\"         return np.cumprod(self.alpha_by_step)      @cached_property     def beta_by_step(self) -&gt; np.ndarray:         \"\"\" \"\"\"         return np.array([self.beta] * self.steps)      @cached_property     def alpha_by_step(self) -&gt; np.ndarray:         \"\"\" \"\"\"         return np.array([self.alpha] * self.steps)      # @cached_property     # def sigma_q_squared(self) -&gt; float:     #     r\"\"\"      #     $$     #     \\sigma_q^2(t) = \\frac{     #         (1 - \\alpha(t))(1 - \\bar\\alpha(t-1))     #     }{1 - \\bar\\alpha(t)}     #     $$     #     \"\"\"     #     sigma_q_squared = (     #         1 - self.alpha_by_step[1:]     #     ) * (1 - self.alpha_bar_by_step[:-1]) / (1 - self.alpha_bar_by_step[1:])     #     return np.insert(sigma_q_squared, 0, 0)   class Diffusion(nn.Module):     def __init__(         self,         batch_size: int,         params: DiffusionParams,         dtype: torch.dtype = torch.float32,     ):         self.batch_size = batch_size         self.params = params         self.dtype = dtype      @cached_property     def alpha_bar_by_step(self) -&gt; torch.Tensor:         return torch.tensor(self.params.alpha_bar_by_step, dtype=self.dtype).detach()      @cached_property     def alpha_by_step(self) -&gt; torch.Tensor:         return torch.tensor(self.params.alpha_by_step, dtype=self.dtype).detach()      # @cached_property     # def sigma_q_squared(self) -&gt; torch.Tensor:     #     return torch.tensor(self.params.sigma_q_squared, dtype=self.dtype)      def likelihood(self):         pass      def _gaussian(self, n_var: int, length: int) -&gt; torch.Tensor:         return torch.normal(mean=0, std=1, size=(n_var, length))      @cached_property     def noise(self) -&gt; torch.Tensor:         return self._gaussian(self.batch_size, self.params.steps).detach()      def _forward_process(self, initial: torch.Tensor) -&gt; torch.Tensor:         return (             torch.outer(                 initial,                 torch.sqrt(self.alpha_bar_by_step),             )             + torch.sqrt(1 - self.alpha_bar_by_step) * self.noise         )      def _forward_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:         r\"\"\"Assuming that we know the noise at step $t$,          $$         x(t) = \\sqrt{\\alpha(t)}x(t-1) + \\sqrt{1 - \\alpha(t)}\\epsilon(t)         $$         \"\"\"         return (             torch.sqrt(self.alpha_by_step[step]) * state             + torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]         )      def _inverse_process_by_step(self, state: torch.Tensor, step: int) -&gt; torch.Tensor:         r\"\"\"Assuming that we know the noise at step $t$,          $$         x(t-1) = \\frac{1}{\\sqrt{\\alpha(t)}}         (x(t) - \\sqrt{1 - \\alpha(t)}\\epsilon(t))         $$         \"\"\"         return (             state - torch.sqrt(1 - self.alpha_by_step[step]) * self.noise[:, step]         ) / torch.sqrt(self.alpha_by_step[step])      def forward(self):         pass In\u00a0[\u00a0]: Copied! <pre>diffusion_params = DiffusionParams(\n    steps=100,\n    beta=0.005,\n    # beta=0,\n)\ndiffusion_batch_size = 1000\n# diffusion_batch_size = 2\ndiffusion_process = Diffusion(diffusion_batch_size, diffusion_params)\n</pre> diffusion_params = DiffusionParams(     steps=100,     beta=0.005,     # beta=0, ) diffusion_batch_size = 1000 # diffusion_batch_size = 2 diffusion_process = Diffusion(diffusion_batch_size, diffusion_params) In\u00a0[\u00a0]: Copied! <pre># diffusion_initial_x = torch.sin(\n#     torch.linspace(0, 1, diffusion_batch_size)\n#     .reshape(diffusion_batch_size)\n# )\n\ndiffusion_initial_x = torch.rand(diffusion_batch_size)\n# diffusion_initial_x = (\n#     torch.distributions.Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n#     .sample((diffusion_batch_size, 1))\n#     .reshape(diffusion_batch_size)\n# )\n\ndiffusion_initial_x\n</pre> # diffusion_initial_x = torch.sin( #     torch.linspace(0, 1, diffusion_batch_size) #     .reshape(diffusion_batch_size) # )  diffusion_initial_x = torch.rand(diffusion_batch_size) # diffusion_initial_x = ( #     torch.distributions.Beta(torch.tensor([0.5]), torch.tensor([0.5])) #     .sample((diffusion_batch_size, 1)) #     .reshape(diffusion_batch_size) # )  diffusion_initial_x In\u00a0[\u00a0]: Copied! <pre>diffusion_steps_step_by_step = [diffusion_initial_x.detach().numpy()]\n\nfor i in range(0, diffusion_params.steps):\n    logger.info(f\"step {i}\")\n    i_state = (\n        diffusion_process._forward_process_by_step(\n            torch.from_numpy(diffusion_steps_step_by_step[-1]), step=i\n        )\n        .detach()\n        .numpy()\n    )\n    logger.info(f\"i_state {i_state[:2]}\")\n    diffusion_steps_step_by_step.append(i_state)\n</pre> diffusion_steps_step_by_step = [diffusion_initial_x.detach().numpy()]  for i in range(0, diffusion_params.steps):     logger.info(f\"step {i}\")     i_state = (         diffusion_process._forward_process_by_step(             torch.from_numpy(diffusion_steps_step_by_step[-1]), step=i         )         .detach()         .numpy()     )     logger.info(f\"i_state {i_state[:2]}\")     diffusion_steps_step_by_step.append(i_state)  In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_initial_x)\n</pre> px.histogram(diffusion_initial_x) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_step_by_step[0])\n</pre> px.histogram(diffusion_steps_step_by_step[0]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_step_by_step[-1])\n</pre> px.histogram(diffusion_steps_step_by_step[-1]) In\u00a0[\u00a0]: Copied! <pre>diffusion_steps = diffusion_process._forward_process(diffusion_initial_x)\n\ndiffusion_initial_x.shape, diffusion_steps.shape\n</pre> diffusion_steps = diffusion_process._forward_process(diffusion_initial_x)  diffusion_initial_x.shape, diffusion_steps.shape In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps[:, -1].detach().numpy().squeeze())\n</pre> px.histogram(diffusion_steps[:, -1].detach().numpy().squeeze()) In\u00a0[\u00a0]: Copied! <pre>diffusion_steps_reverse = [diffusion_steps_step_by_step[-1]]\n\nfor i in range(diffusion_params.steps - 1, -1, -1):\n    logger.info(f\"step {i}\")\n    i_state = (\n        diffusion_process._inverse_process_by_step(\n            torch.from_numpy(diffusion_steps_reverse[-1]), step=i\n        )\n        .detach()\n        .numpy()\n    )\n    logger.info(f\"i_state {i_state[:2]}\")\n    diffusion_steps_reverse.append(i_state)\n</pre> diffusion_steps_reverse = [diffusion_steps_step_by_step[-1]]  for i in range(diffusion_params.steps - 1, -1, -1):     logger.info(f\"step {i}\")     i_state = (         diffusion_process._inverse_process_by_step(             torch.from_numpy(diffusion_steps_reverse[-1]), step=i         )         .detach()         .numpy()     )     logger.info(f\"i_state {i_state[:2]}\")     diffusion_steps_reverse.append(i_state)  In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_reverse[-1])\n</pre> px.histogram(diffusion_steps_reverse[-1]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(diffusion_steps_reverse[0])\n</pre> px.histogram(diffusion_steps_reverse[0]) In\u00a0[\u00a0]: Copied! <pre>px.line(diffusion_params.alpha_bar_by_step)\n</pre> px.line(diffusion_params.alpha_bar_by_step) In\u00a0[\u00a0]: Copied! <pre>df_diffusion_example = pd.DataFrame(\n    np.concatenate(\n        [\n            diffusion_initial_x.reshape(1, diffusion_batch_size).detach().numpy(),\n            diffusion_steps.T.detach().numpy(),\n        ]\n    ),\n    columns=[f\"x_{i}\" for i in range(len(diffusion_steps))],\n)\ndf_diffusion_example[\"step\"] = df_diffusion_example.index\n\ndf_diffusion_example_melted = df_diffusion_example.melt(\n    id_vars=[\"step\"], var_name=\"variable\", value_name=\"value\"\n)\ndf_diffusion_example_melted.tail()\n</pre> df_diffusion_example = pd.DataFrame(     np.concatenate(         [             diffusion_initial_x.reshape(1, diffusion_batch_size).detach().numpy(),             diffusion_steps.T.detach().numpy(),         ]     ),     columns=[f\"x_{i}\" for i in range(len(diffusion_steps))], ) df_diffusion_example[\"step\"] = df_diffusion_example.index  df_diffusion_example_melted = df_diffusion_example.melt(     id_vars=[\"step\"], var_name=\"variable\", value_name=\"value\" ) df_diffusion_example_melted.tail() In\u00a0[\u00a0]: Copied! <pre>px.histogram(\n    df_diffusion_example_melted,\n    x=\"value\",\n    histnorm=\"probability density\",\n    animation_frame=\"step\",\n)\n</pre> px.histogram(     df_diffusion_example_melted,     x=\"value\",     histnorm=\"probability density\",     animation_frame=\"step\", ) In\u00a0[\u00a0]: Copied! <pre>px.violin(\n    df_diffusion_example_melted.loc[\n        df_diffusion_example_melted[\"step\"].isin(\n            [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n        )\n    ],\n    x=\"step\",\n    y=\"value\",\n)\n</pre> px.violin(     df_diffusion_example_melted.loc[         df_diffusion_example_melted[\"step\"].isin(             [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]         )     ],     x=\"step\",     y=\"value\", ) In\u00a0[\u00a0]: Copied! <pre>px.line(\n    df_diffusion_example_melted,\n    x=\"step\",\n    y=\"value\",\n    color=\"variable\",\n)\n</pre> px.line(     df_diffusion_example_melted,     x=\"step\",     y=\"value\",     color=\"variable\", ) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(\n    df_diffusion_example_melted.loc[df_diffusion_example_melted[\"step\"] == 0],\n    x=\"value\",\n    stat=\"probability\",\n    color=\"k\",\n    label=\"Initial Distribution\",\n    ax=ax,\n)\n\nax.set_title(\"Initial Distribution\")\nax.set_xlabel(\"Position\")\n</pre> _, ax = plt.subplots(figsize=(10, 6)) sns.histplot(     df_diffusion_example_melted.loc[df_diffusion_example_melted[\"step\"] == 0],     x=\"value\",     stat=\"probability\",     color=\"k\",     label=\"Initial Distribution\",     ax=ax, )  ax.set_title(\"Initial Distribution\") ax.set_xlabel(\"Position\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6))\n\nsns.histplot(\n    df_diffusion_example_melted.loc[\n        df_diffusion_example_melted[\"step\"] == max(df_diffusion_example_melted[\"step\"])\n    ],\n    x=\"value\",\n    stat=\"probability\",\n    color=\"k\",\n    ax=ax,\n)\n\nax.set_title(\"Final Distribution\")\nax.set_xlabel(\"Position\")\n</pre> _, ax = plt.subplots(figsize=(10, 6))  sns.histplot(     df_diffusion_example_melted.loc[         df_diffusion_example_melted[\"step\"] == max(df_diffusion_example_melted[\"step\"])     ],     x=\"value\",     stat=\"probability\",     color=\"k\",     ax=ax, )  ax.set_title(\"Final Distribution\") ax.set_xlabel(\"Position\") In\u00a0[\u00a0]: Copied! <pre>from lightning import LightningModule\n</pre> from lightning import LightningModule In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass DiffusionEncoderParams:\n    \"\"\"Parameters for VAEEncoder and VAEDecoder\n\n    :param latent_size: latent space dimension\n    :param history_length: input sequence length\n    :param n_features: number of features\n    \"\"\"\n\n    latent_size: int = 40\n    num_layers: int = 2\n    history_length: int\n    n_features: int = 1\n    initial_state: torch.Tensor = None\n\n    @cached_property\n    def data_size(self) -&gt; int:\n        \"\"\"The dimension of the input data\n        when flattened.\n        \"\"\"\n        return self.sequence_length * self.n_features\n\n    def asdict(self) -&gt; dict:\n        return dataclasses.asdict(self)\n\n\nclass Encoder(nn.Module):\n    \"\"\" \"\"\"\n\n    def __init__(self, params: DiffusionEncoderParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        self.rnn = nn.GRU(\n            input_size=self.params.history_length,\n            hidden_size=self.params.latent_size,\n            num_layers=self.params.num_layers,\n        )\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n\n        :param x: input data, shape (batch_size, history_length, n_features)\n        \"\"\"\n        batch_size, _, _ = x.size()\n        x = x.transpose(1, 2)\n\n        outputs, state = self.rnn(x, self.params.initial_state)\n\n        return outputs, state\n\n\nclass Distribution(nn.Module):\n    \"\"\" \"\"\"\n\n    def __init__(self, params: DiffusionEncoderParams, samples: int = 100):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n        self.samples = samples\n\n        self.linear = nn.Linear(self.params.latent_size, samples)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear(x)\n\n\nclass DiffusionModel(nn.Module):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        encoder_params: DiffusionEncoderParams,\n        diffusion_params: DiffusionParams,\n        batch_size: int,\n    ):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        outputs, state = self.encoder(x)\n        distribution = self.distribution(outputs)\n\n        return distribution\n\n\nclass Model(LightningModule):\n    def __init__(\n        self,\n        encoder_params: DiffusionEncoderParams,\n        diffusion_params: DiffusionParams,\n        batch_size: int,\n    ):\n        self.encoder = Encoder(encoder_params)\n        self.diffusion = Diffusion(batch_size, diffusion_params)\n        self.distribution = Distribution(encoder_params, samples=100)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        outputs, state = self.encoder(x)\n        distribution = self.distribution(outputs)\n\n        return distribution\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n\n        distribution = self(x)\n\n        loss = self.loss(distribution, y)\n\n        return loss\n\n    def loss(self, y: torch.Tensor, y_hat: torch.Tensor) -&gt; torch.Tensor:\n\n        return\n</pre> @dataclasses.dataclass class DiffusionEncoderParams:     \"\"\"Parameters for VAEEncoder and VAEDecoder      :param latent_size: latent space dimension     :param history_length: input sequence length     :param n_features: number of features     \"\"\"      latent_size: int = 40     num_layers: int = 2     history_length: int     n_features: int = 1     initial_state: torch.Tensor = None      @cached_property     def data_size(self) -&gt; int:         \"\"\"The dimension of the input data         when flattened.         \"\"\"         return self.sequence_length * self.n_features      def asdict(self) -&gt; dict:         return dataclasses.asdict(self)   class Encoder(nn.Module):     \"\"\" \"\"\"      def __init__(self, params: DiffusionEncoderParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          self.rnn = nn.GRU(             input_size=self.params.history_length,             hidden_size=self.params.latent_size,             num_layers=self.params.num_layers,         )      def forward(         self, x: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         \"\"\"          :param x: input data, shape (batch_size, history_length, n_features)         \"\"\"         batch_size, _, _ = x.size()         x = x.transpose(1, 2)          outputs, state = self.rnn(x, self.params.initial_state)          return outputs, state   class Distribution(nn.Module):     \"\"\" \"\"\"      def __init__(self, params: DiffusionEncoderParams, samples: int = 100):         super().__init__()          self.params = params         self.hparams = params.asdict()         self.samples = samples          self.linear = nn.Linear(self.params.latent_size, samples)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear(x)   class DiffusionModel(nn.Module):     \"\"\" \"\"\"      def __init__(         self,         encoder_params: DiffusionEncoderParams,         diffusion_params: DiffusionParams,         batch_size: int,     ):         super().__init__()      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         outputs, state = self.encoder(x)         distribution = self.distribution(outputs)          return distribution   class Model(LightningModule):     def __init__(         self,         encoder_params: DiffusionEncoderParams,         diffusion_params: DiffusionParams,         batch_size: int,     ):         self.encoder = Encoder(encoder_params)         self.diffusion = Diffusion(batch_size, diffusion_params)         self.distribution = Distribution(encoder_params, samples=100)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         outputs, state = self.encoder(x)         distribution = self.distribution(outputs)          return distribution      def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         x = batch[\"x\"]         y = batch[\"y\"]          distribution = self(x)          loss = self.loss(distribution, y)          return loss      def loss(self, y: torch.Tensor, y_hat: torch.Tensor) -&gt; torch.Tensor:          return In\u00a0[\u00a0]: Copied! <pre>diffusion_params.steps\n</pre> diffusion_params.steps In\u00a0[\u00a0]: Copied! <pre>enc_params = DiffusionEncoderParams(\n    latent_size=10,\n    num_layers=1,\n    history_length=5,\n    n_features=1,\n    initial_state=None,\n)\nencoder = Encoder(\n    params=enc_params,\n)\n\nencoder(\n    x=torch.rand(8, 5, 1),\n)[0].shape, encoder(\n    x=torch.rand(8, 5, 1),\n)[-1].shape\n</pre> enc_params = DiffusionEncoderParams(     latent_size=10,     num_layers=1,     history_length=5,     n_features=1,     initial_state=None, ) encoder = Encoder(     params=enc_params, )  encoder(     x=torch.rand(8, 5, 1), )[0].shape, encoder(     x=torch.rand(8, 5, 1), )[-1].shape In\u00a0[\u00a0]: Copied! <pre>class DiffusionEmbedding(nn.Module):\n    def __init__(self, dim, proj_dim, max_steps=500):\n        super().__init__()\n        self.register_buffer(\n            \"embedding\", self._build_embedding(dim, max_steps), persistent=False\n        )\n        self.projection1 = nn.Linear(dim * 2, proj_dim)\n        self.projection2 = nn.Linear(proj_dim, proj_dim)\n\n    def forward(self, diffusion_step):\n        x = self.embedding[diffusion_step]\n        x = self.projection1(x)\n        x = F.silu(x)\n        x = self.projection2(x)\n        x = F.silu(x)\n        return x\n\n    def _build_embedding(self, dim, max_steps):\n        steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n        dims = torch.arange(dim).unsqueeze(0)  # [1,dim]\n        table = steps * 10.0 ** (dims * 4.0 / dim)  # [T,dim]\n        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n        return table\n\n\nclass EpsilonTheta(nn.Module):\n    def __init__(\n        self,\n        target_dim,\n        cond_length,\n        time_emb_dim=16,\n        residual_layers=8,\n        residual_channels=8,\n        dilation_cycle_length=2,\n        residual_hidden=64,\n    ):\n        super().__init__()\n        self.input_projection = nn.Conv1d(\n            1, residual_channels, 1, padding=2, padding_mode=\"circular\"\n        )\n        self.diffusion_embedding = DiffusionEmbedding(\n            time_emb_dim, proj_dim=residual_hidden\n        )\n        self.cond_upsampler = CondUpsampler(\n            target_dim=target_dim, cond_length=cond_length\n        )\n        self.residual_layers = nn.ModuleList(\n            [\n                ResidualBlock(\n                    residual_channels=residual_channels,\n                    dilation=2 ** (i % dilation_cycle_length),\n                    hidden_size=residual_hidden,\n                )\n                for i in range(residual_layers)\n            ]\n        )\n        self.skip_projection = nn.Conv1d(residual_channels, residual_channels, 3)\n        self.output_projection = nn.Conv1d(residual_channels, 1, 3)\n\n        nn.init.kaiming_normal_(self.input_projection.weight)\n        nn.init.kaiming_normal_(self.skip_projection.weight)\n        nn.init.zeros_(self.output_projection.weight)\n\n    def forward(self, inputs, time, cond):\n        x = self.input_projection(inputs)\n        x = F.leaky_relu(x, 0.4)\n\n        diffusion_step = self.diffusion_embedding(time)\n        cond_up = self.cond_upsampler(cond)\n        skip = []\n        for layer in self.residual_layers:\n            x, skip_connection = layer(x, cond_up, diffusion_step)\n            skip.append(skip_connection)\n\n        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n        x = self.skip_projection(x)\n        x = F.leaky_relu(x, 0.4)\n        x = self.output_projection(x)\n        return x\n</pre> class DiffusionEmbedding(nn.Module):     def __init__(self, dim, proj_dim, max_steps=500):         super().__init__()         self.register_buffer(             \"embedding\", self._build_embedding(dim, max_steps), persistent=False         )         self.projection1 = nn.Linear(dim * 2, proj_dim)         self.projection2 = nn.Linear(proj_dim, proj_dim)      def forward(self, diffusion_step):         x = self.embedding[diffusion_step]         x = self.projection1(x)         x = F.silu(x)         x = self.projection2(x)         x = F.silu(x)         return x      def _build_embedding(self, dim, max_steps):         steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]         dims = torch.arange(dim).unsqueeze(0)  # [1,dim]         table = steps * 10.0 ** (dims * 4.0 / dim)  # [T,dim]         table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)         return table   class EpsilonTheta(nn.Module):     def __init__(         self,         target_dim,         cond_length,         time_emb_dim=16,         residual_layers=8,         residual_channels=8,         dilation_cycle_length=2,         residual_hidden=64,     ):         super().__init__()         self.input_projection = nn.Conv1d(             1, residual_channels, 1, padding=2, padding_mode=\"circular\"         )         self.diffusion_embedding = DiffusionEmbedding(             time_emb_dim, proj_dim=residual_hidden         )         self.cond_upsampler = CondUpsampler(             target_dim=target_dim, cond_length=cond_length         )         self.residual_layers = nn.ModuleList(             [                 ResidualBlock(                     residual_channels=residual_channels,                     dilation=2 ** (i % dilation_cycle_length),                     hidden_size=residual_hidden,                 )                 for i in range(residual_layers)             ]         )         self.skip_projection = nn.Conv1d(residual_channels, residual_channels, 3)         self.output_projection = nn.Conv1d(residual_channels, 1, 3)          nn.init.kaiming_normal_(self.input_projection.weight)         nn.init.kaiming_normal_(self.skip_projection.weight)         nn.init.zeros_(self.output_projection.weight)      def forward(self, inputs, time, cond):         x = self.input_projection(inputs)         x = F.leaky_relu(x, 0.4)          diffusion_step = self.diffusion_embedding(time)         cond_up = self.cond_upsampler(cond)         skip = []         for layer in self.residual_layers:             x, skip_connection = layer(x, cond_up, diffusion_step)             skip.append(skip_connection)          x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))         x = self.skip_projection(x)         x = F.leaky_relu(x, 0.4)         x = self.output_projection(x)         return x In\u00a0[\u00a0]: Copied! <pre>def extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    mm = len(x_shape) - 1\n    return out.reshape(b, *((1,) * mm))\n\n\ndef q_sample(x_start, t, noise=None):\n    noise = default(noise, lambda: torch.randn_like(x_start))\n\n    return (\n        extract(sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        + extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n    )\n\n\ndef p_losses(x_start, cond, t, noise=None):\n    loss_type = \"l2\"\n    noise = default(noise, lambda: torch.randn_like(x_start))\n\n    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n    x_recon = denoise_fn(x_noisy, t, cond=cond)\n\n    if loss_type == \"l1\":\n        loss = torch.nn.functional.l1_loss(x_recon, noise)\n    elif loss_type == \"l2\":\n        loss = torch.nn.functional.mse_loss(x_recon, noise)\n    elif loss_type == \"huber\":\n        loss = torch.nn.functional.smooth_l1_loss(x_recon, noise)\n    else:\n        raise NotImplementedError()\n\n    return loss\n\n\ndef log_prob(self, x, cond, *args, **kwargs):\n    if self.scale is not None:\n        x /= self.scale\n\n    B, T, _ = x.shape\n\n    time = torch.randint(0, self.num_timesteps, (B * T,), device=x.device).long()\n    loss = p_losses(\n        x.reshape(B * T, 1, -1), cond.reshape(B * T, 1, -1), time, *args, **kwargs\n    )\n\n    return loss\n</pre> def extract(a, t, x_shape):     b, *_ = t.shape     out = a.gather(-1, t)     mm = len(x_shape) - 1     return out.reshape(b, *((1,) * mm))   def q_sample(x_start, t, noise=None):     noise = default(noise, lambda: torch.randn_like(x_start))      return (         extract(sqrt_alphas_cumprod, t, x_start.shape) * x_start         + extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise     )   def p_losses(x_start, cond, t, noise=None):     loss_type = \"l2\"     noise = default(noise, lambda: torch.randn_like(x_start))      x_noisy = q_sample(x_start=x_start, t=t, noise=noise)     x_recon = denoise_fn(x_noisy, t, cond=cond)      if loss_type == \"l1\":         loss = torch.nn.functional.l1_loss(x_recon, noise)     elif loss_type == \"l2\":         loss = torch.nn.functional.mse_loss(x_recon, noise)     elif loss_type == \"huber\":         loss = torch.nn.functional.smooth_l1_loss(x_recon, noise)     else:         raise NotImplementedError()      return loss   def log_prob(self, x, cond, *args, **kwargs):     if self.scale is not None:         x /= self.scale      B, T, _ = x.shape      time = torch.randint(0, self.num_timesteps, (B * T,), device=x.device).long()     loss = p_losses(         x.reshape(B * T, 1, -1), cond.reshape(B * T, 1, -1), time, *args, **kwargs     )      return loss In\u00a0[\u00a0]: Copied! <pre>tmp_x_start = torch.ones((2, 3))\ntmp_x_start\n</pre> tmp_x_start = torch.ones((2, 3)) tmp_x_start In\u00a0[\u00a0]: Copied! <pre>tmp_a = torch.Tensor([1, 2, 3, 4, 5])\ntmp_a\n</pre> tmp_a = torch.Tensor([1, 2, 3, 4, 5]) tmp_a In\u00a0[\u00a0]: Copied! <pre>extract(tmp_a, tmp_t, x_shape=tmp_x_start.shape)\n</pre> extract(tmp_a, tmp_t, x_shape=tmp_x_start.shape)"}, {"location": "notebooks/diffusion_model_timegrad/#diffusion-model", "title": "Diffusion Model\u00b6", "text": "<p>References:</p> <ol> <li><code>pts/model/time_grad/time_grad_network.py</code></li> </ol>"}, {"location": "notebooks/diffusion_model_timegrad/#forward-process-step-by-step", "title": "Forward process step by step\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/#calculate-state-at-time-t-in-one-batch", "title": "Calculate State at time t in one batch\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/#reverse-step-by-step", "title": "Reverse step by step\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/#diffusion-distributions", "title": "Diffusion Distributions\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/#create-visuals", "title": "Create Visuals\u00b6", "text": ""}, {"location": "notebooks/diffusion_model_timegrad/#model", "title": "Model\u00b6", "text": ""}, {"location": "notebooks/diffusion_process/", "title": "Diffusion Process", "text": ""}, {"location": "notebooks/diffusion_process/#diffusion-process", "title": "Diffusion Process\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/", "title": "Feedforward Neural Networks for Univariate Time Series Forecasting", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\nimport math\n</pre> import dataclasses import math In\u00a0[\u00a0]: Copied! <pre>import os\nfrom functools import cached_property\nfrom typing import Dict, List, Tuple\n\nimport lightning as L\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom loguru import logger\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule\nfrom ts_dl_utils.evaluation.evaluator import Evaluator\nfrom ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster\n</pre> import os from functools import cached_property from typing import Dict, List, Tuple  import lightning as L import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from loguru import logger from torch import nn from torch.utils.data import DataLoader, Dataset from ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule from ts_dl_utils.evaluation.evaluator import Evaluator from ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster  In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\n</pre> pen = Pendulum(length=100) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001))\n</pre> df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001)) In\u00a0[\u00a0]: Copied! <pre>df[\"theta\"] = df[\"theta\"] + 2\n</pre> df[\"theta\"] = df[\"theta\"] + 2 <p>Since the damping constant is very small, the data generated is mostly a sin wave.</p> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass TSFFNParams:\n    \"\"\"A dataclass to be served as our parameters for the model.\n\n    :param hidden_widths: list of dimensions for the hidden layers\n    \"\"\"\n\n    hidden_widths: List[int]\n\n\nclass TSFeedForward(nn.Module):\n    \"\"\"Feedforward networks for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param ffn_params: the parameters for the FFN network.\n    \"\"\"\n\n    def __init__(self, history_length: int, horizon: int, ffn_params: TSFFNParams):\n        super().__init__()\n        self.ffn_params = ffn_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.regulate_input = nn.Linear(\n            self.history_length, self.ffn_params.hidden_widths[0]\n        )\n\n        self.hidden_layers = nn.Sequential(\n            *[\n                self._linear_block(dim_in, dim_out)\n                for dim_in, dim_out in zip(\n                    self.ffn_params.hidden_widths[:-1],\n                    self.ffn_params.hidden_widths[1:],\n                )\n            ]\n        )\n\n        self.regulate_output = nn.Linear(\n            self.ffn_params.hidden_widths[-1], self.horizon\n        )\n\n    @property\n    def ffn_config(self):\n        return dataclasses.asdict(self.ffn_params)\n\n    def _linear_block(self, dim_in, dim_out):\n        return nn.Sequential(*[nn.Linear(dim_in, dim_out), nn.ReLU()])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.regulate_input(x)\n        x = self.hidden_layers(x)\n\n        return self.regulate_output(x)\n</pre> @dataclasses.dataclass class TSFFNParams:     \"\"\"A dataclass to be served as our parameters for the model.      :param hidden_widths: list of dimensions for the hidden layers     \"\"\"      hidden_widths: List[int]   class TSFeedForward(nn.Module):     \"\"\"Feedforward networks for univaraite time series modeling.      :param history_length: the length of the input history.     :param horizon: the number of steps to be forecasted.     :param ffn_params: the parameters for the FFN network.     \"\"\"      def __init__(self, history_length: int, horizon: int, ffn_params: TSFFNParams):         super().__init__()         self.ffn_params = ffn_params         self.history_length = history_length         self.horizon = horizon          self.regulate_input = nn.Linear(             self.history_length, self.ffn_params.hidden_widths[0]         )          self.hidden_layers = nn.Sequential(             *[                 self._linear_block(dim_in, dim_out)                 for dim_in, dim_out in zip(                     self.ffn_params.hidden_widths[:-1],                     self.ffn_params.hidden_widths[1:],                 )             ]         )          self.regulate_output = nn.Linear(             self.ffn_params.hidden_widths[-1], self.horizon         )      @property     def ffn_config(self):         return dataclasses.asdict(self.ffn_params)      def _linear_block(self, dim_in, dim_out):         return nn.Sequential(*[nn.Linear(dim_in, dim_out), nn.ReLU()])      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.regulate_input(x)         x = self.hidden_layers(x)          return self.regulate_output(x) <p>We use lightning to train our model.</p> In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\n\ngap = 10\n</pre> history_length_1_step = 100 horizon_1_step = 1  gap = 10 <p>We will build a few utilities</p> <ol> <li>To be able to feed the data into our model, we build a class (<code>DataFrameDataset</code>) that converts the pandas dataframe into a Dataset for pytorch.</li> <li>To make the lightning training code simpler, we will build a LightningDataModule (<code>PendulumDataModule</code>) and a LightningModule (<code>FFNForecaster</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>class FFNForecaster(L.LightningModule):\n    def __init__(self, ffn: nn.Module):\n        super().__init__()\n        self.ffn = ffn\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.ffn(x)\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.ffn(x)\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.ffn(x)\n        return x, y_hat\n\n    def forward(self, x):\n        x = x.squeeze().type(self.dtype)\n        return x, self.ffn(x)\n</pre> class FFNForecaster(L.LightningModule):     def __init__(self, ffn: nn.Module):         super().__init__()         self.ffn = ffn      def configure_optimizers(self):         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)         return optimizer      def training_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.ffn(x)          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"train_loss\": loss}, prog_bar=True)         return loss      def validation_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.ffn(x)          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"val_loss\": loss}, prog_bar=True)         return loss      def predict_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.ffn(x)         return x, y_hat      def forward(self, x):         x = x.squeeze().type(self.dtype)         return x, self.ffn(x) In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = PendulumDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    gap=gap,\n    dataframe=df[[\"theta\"]],\n)\n</pre> pdm_1_step = PendulumDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     gap=gap,     dataframe=df[[\"theta\"]], ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\npdm_1_step_sample = list(pdm_1_step.train_dataloader())[0]\n\npdm_1_step_sample_history = pdm_1_step_sample[0][0, ...].squeeze(-1).numpy()\npdm_1_step_sample_target = pdm_1_step_sample[1][0, ...].squeeze(-1).numpy()\n\npdm_1_step_sample_steps = np.arange(\n    0, len(pdm_1_step_sample_history) + gap + len(pdm_1_step_sample_target)\n)\n\nax.plot(\n    pdm_1_step_sample_steps[: len(pdm_1_step_sample_history)],\n    pdm_1_step_sample_history,\n    marker=\".\",\n    label=\"Input\",\n)\n\nax.plot(\n    pdm_1_step_sample_steps[len(pdm_1_step_sample_history) + gap :],\n    pdm_1_step_sample_target,\n    \"r--\",\n    marker=\"x\",\n    label=\"Target\",\n)\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  pdm_1_step_sample = list(pdm_1_step.train_dataloader())[0]  pdm_1_step_sample_history = pdm_1_step_sample[0][0, ...].squeeze(-1).numpy() pdm_1_step_sample_target = pdm_1_step_sample[1][0, ...].squeeze(-1).numpy()  pdm_1_step_sample_steps = np.arange(     0, len(pdm_1_step_sample_history) + gap + len(pdm_1_step_sample_target) )  ax.plot(     pdm_1_step_sample_steps[: len(pdm_1_step_sample_history)],     pdm_1_step_sample_history,     marker=\".\",     label=\"Input\", )  ax.plot(     pdm_1_step_sample_steps[len(pdm_1_step_sample_history) + gap :],     pdm_1_step_sample_target,     \"r--\",     marker=\"x\",     label=\"Target\", ) plt.legend() In\u00a0[\u00a0]: Copied! <pre>ts_ffn_params_1_step = TSFFNParams(hidden_widths=[512, 256, 64, 256, 512])\n\nts_ffn_1_step = TSFeedForward(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    ffn_params=ts_ffn_params_1_step,\n)\n\nts_ffn_1_step\n</pre> ts_ffn_params_1_step = TSFFNParams(hidden_widths=[512, 256, 64, 256, 512])  ts_ffn_1_step = TSFeedForward(     history_length=history_length_1_step,     horizon=horizon_1_step,     ffn_params=ts_ffn_params_1_step, )  ts_ffn_1_step In\u00a0[\u00a0]: Copied! <pre>ffn_forecaster_1_step = FFNForecaster(ffn=ts_ffn_1_step)\n</pre> ffn_forecaster_1_step = FFNForecaster(ffn=ts_ffn_1_step) In\u00a0[\u00a0]: Copied! <pre>logger_1_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"ffn_ts_1_step\"\n)\n\ntrainer_1_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)\n    ],\n    logger=logger_1_step,\n)\n</pre> logger_1_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"ffn_ts_1_step\" )  trainer_1_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)     ],     logger=logger_1_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=ffn_forecaster_1_step, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=ffn_forecaster_1_step, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=ffn_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=ffn_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = L.Trainer(precision=\"64\")  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>history_length_m_step = 100\nhorizon_m_step = 3\n</pre> history_length_m_step = 100 horizon_m_step = 3 In\u00a0[\u00a0]: Copied! <pre>pdm_m_step = PendulumDataModule(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_m_step = PendulumDataModule(     history_length=history_length_m_step,     horizon=horizon_m_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_ffn_params_m_step = TSFFNParams(hidden_widths=[512, 256, 64, 256, 512])\n\nts_ffn_m_step = TSFeedForward(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    ffn_params=ts_ffn_params_m_step,\n)\n\nts_ffn_m_step\n</pre> ts_ffn_params_m_step = TSFFNParams(hidden_widths=[512, 256, 64, 256, 512])  ts_ffn_m_step = TSFeedForward(     history_length=history_length_m_step,     horizon=horizon_m_step,     ffn_params=ts_ffn_params_m_step, )  ts_ffn_m_step In\u00a0[\u00a0]: Copied! <pre>ffn_forecaster_m_step = FFNForecaster(ffn=ts_ffn_m_step)\n</pre> ffn_forecaster_m_step = FFNForecaster(ffn=ts_ffn_m_step) In\u00a0[\u00a0]: Copied! <pre>logger_m_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"ffn_ts_m_step\"\n)\n\ntrainer_m_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)\n    ],\n    logger=logger_m_step,\n)\n</pre> logger_m_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"ffn_ts_m_step\" )  trainer_m_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)     ],     logger=logger_m_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_m_step.fit(model=ffn_forecaster_m_step, datamodule=pdm_m_step)\n</pre> trainer_m_step.fit(model=ffn_forecaster_m_step, datamodule=pdm_m_step) In\u00a0[\u00a0]: Copied! <pre>predictions_m_step = trainer_m_step.predict(\n    model=ffn_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> predictions_m_step = trainer_m_step.predict(     model=ffn_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_m_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step)\nlobs_m_step_predictions = trainer_naive_m_step.predict(\n    model=lobs_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> trainer_naive_m_step = L.Trainer(precision=\"64\")  lobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step) lobs_m_step_predictions = trainer_naive_m_step.predict(     model=lobs_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step = Evaluator(step=2, gap=gap)\n</pre> evaluator_m_step = Evaluator(step=2, gap=gap) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\n\nfor i in np.arange(0, 1000, 120):\n    evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i)\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))   for i in np.arange(0, 1000, 120):     evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())"}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#feedforward-neural-networks-for-univariate-time-series-forecasting", "title": "Feedforward Neural Networks for Univariate Time Series Forecasting\u00b6", "text": "<p>In this notebook, we build a feedforward neural network using pytorch to forecast $\\sin$ function as a time series.</p>"}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#data", "title": "Data\u00b6", "text": "<p>We create a dataset that models a damped pendulum. The pendulum is modelled as a damped harmonic oscillator, i.e.,</p> $$ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), $$<p>where $\\theta(t)$ is the angle of the pendulum at time $t$. The period $p$ is calculated using</p> $$ p = 2 \\pi \\sqrt(L / g), $$<p>with $L$ being the length of the pendulum and $g$ being the surface gravity.</p>"}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#model", "title": "Model\u00b6", "text": "<p>In this section, we create the FFN model.</p>"}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#forecasting-horizon1", "title": "Forecasting (horizon=1)\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#training-utilities", "title": "Training Utilities\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#data-model-and-training", "title": "Data, Model and Training\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#datamodule", "title": "DataModule\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#lightningmodule", "title": "LightningModule\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#trainer", "title": "Trainer\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#fitting", "title": "Fitting\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#retrieving-predictions", "title": "Retrieving Predictions\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#naive-forecasts", "title": "Naive Forecasts\u00b6", "text": "<p>To understand how good our forecasts are, we take the last observations in time and use them as forecasts.</p> <p><code>ts_LastObservationForecaster</code> is a forecaster we have build for this purpose.</p>"}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#forecasting-horizon3", "title": "Forecasting (horizon=3)\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#train-a-model", "title": "Train a Model\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/feedforward_neural_netwroks_timeseries/#evaluation", "title": "Evaluation\u00b6", "text": ""}, {"location": "notebooks/hierarchical_forecasting_mint/", "title": "Forecast Reconciliation", "text": "In\u00a0[\u00a0]: Copied! <pre>import sympy as sp\nfrom darts import TimeSeries\nfrom darts.utils.model_selection import train_test_split\nfrom darts.utils.statistics import plot_pacf\n\nsns.reset_orig()\n\nplt.rcParams[\"figure.figsize\"] = (10, 6.18)\nprint(plt.rcParams.get(\"figure.figsize\"))\n</pre> import sympy as sp from darts import TimeSeries from darts.utils.model_selection import train_test_split from darts.utils.statistics import plot_pacf  sns.reset_orig()  plt.rcParams[\"figure.figsize\"] = (10, 6.18) print(plt.rcParams.get(\"figure.figsize\")) In\u00a0[\u00a0]: Copied! <pre>m_l = 3\n\n\nm_w_diag_elements = tuple(sp.Symbol(f\"W_{i}\") for i in range(1, m_l + 1))\n\nm_s_ident_diag = np.diag([1] * (m_l - 1)).tolist()\n\nm_w_diag_elements, m_s_ident_diag\n</pre> m_l = 3   m_w_diag_elements = tuple(sp.Symbol(f\"W_{i}\") for i in range(1, m_l + 1))  m_s_ident_diag = np.diag([1] * (m_l - 1)).tolist()  m_w_diag_elements, m_s_ident_diag In\u00a0[\u00a0]: Copied! <pre>class MinTMatrices:\n    def __init__(self, levels: int):\n        self.levels = levels\n\n    @property\n    def s(self):\n        s_ident_diag = np.diag([1] * (self.levels - 1)).tolist()\n        return sp.Matrix(\n            [\n                [1] * (self.levels - 1),\n            ]\n            + s_ident_diag\n        )\n\n    @property\n    def w_diag_elements(self):\n        return tuple(sp.Symbol(f\"W_{i}\") for i in range(1, self.levels + 1))\n\n    @property\n    def w(self):\n        return sp.Matrix(np.diag(self.w_diag_elements).tolist())\n\n    @property\n    def p_left(self):\n        return sp.Inverse(sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w), self.s))\n\n    @property\n    def p_right(self):\n        return sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w))\n\n    @property\n    def p(self):\n        return sp.MatMul(self.p_left, self.p_right)\n\n    @property\n    def s_p(self):\n        return sp.MatMul(self.s, self.p)\n\n    @property\n    def s_p_numerical(self):\n        return sp.lambdify(self.w_diag_elements, self.s_p)\n\n    def visualize_s_p(self, w_elements, ax):\n        sns.heatmap(self.s_p_numerical(*w_elements), annot=True, cbar=False, ax=ax)\n        ax.grid(False)\n        ax.set(xticklabels=[], yticklabels=[])\n        ax.tick_params(bottom=False, left=False)\n        ax.set_title(f\"$W_{{diag}} = {w_elements}$\")\n        return ax\n</pre> class MinTMatrices:     def __init__(self, levels: int):         self.levels = levels      @property     def s(self):         s_ident_diag = np.diag([1] * (self.levels - 1)).tolist()         return sp.Matrix(             [                 [1] * (self.levels - 1),             ]             + s_ident_diag         )      @property     def w_diag_elements(self):         return tuple(sp.Symbol(f\"W_{i}\") for i in range(1, self.levels + 1))      @property     def w(self):         return sp.Matrix(np.diag(self.w_diag_elements).tolist())      @property     def p_left(self):         return sp.Inverse(sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w), self.s))      @property     def p_right(self):         return sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w))      @property     def p(self):         return sp.MatMul(self.p_left, self.p_right)      @property     def s_p(self):         return sp.MatMul(self.s, self.p)      @property     def s_p_numerical(self):         return sp.lambdify(self.w_diag_elements, self.s_p)      def visualize_s_p(self, w_elements, ax):         sns.heatmap(self.s_p_numerical(*w_elements), annot=True, cbar=False, ax=ax)         ax.grid(False)         ax.set(xticklabels=[], yticklabels=[])         ax.tick_params(bottom=False, left=False)         ax.set_title(f\"$W_{{diag}} = {w_elements}$\")         return ax In\u00a0[\u00a0]: Copied! <pre>mtm_3 = MinTMatrices(levels=3)\n</pre> mtm_3 = MinTMatrices(levels=3) In\u00a0[\u00a0]: Copied! <pre>print(\n    f\"s: {sp.latex(mtm_3.s)}\\n\"\n    f\"p: {sp.latex(mtm_3.p.as_explicit())}\\n\"\n    f\"s_p: {sp.latex(mtm_3.s_p.as_explicit())}\\n\"\n)\n</pre> print(     f\"s: {sp.latex(mtm_3.s)}\\n\"     f\"p: {sp.latex(mtm_3.p.as_explicit())}\\n\"     f\"s_p: {sp.latex(mtm_3.s_p.as_explicit())}\\n\" ) In\u00a0[\u00a0]: Copied! <pre>mtm_3.s\n</pre> mtm_3.s In\u00a0[\u00a0]: Copied! <pre>mtm_3.p\n</pre> mtm_3.p In\u00a0[\u00a0]: Copied! <pre>mtm_3.s_p.as_explicit()\n</pre> mtm_3.s_p.as_explicit() In\u00a0[\u00a0]: Copied! <pre>mtm_3.w_diag_elements\n</pre> mtm_3.w_diag_elements In\u00a0[\u00a0]: Copied! <pre>mtm_3.s_p_numerical(1, 2, 3)\n</pre> mtm_3.s_p_numerical(1, 2, 3) In\u00a0[\u00a0]: Copied! <pre>w_elements = [(1, 1, 1), (2, 1, 1)]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))\n\nfor idx, w in enumerate(w_elements):\n    mtm_3.visualize_s_p(w, axes[idx])\nfig.show()\n</pre> w_elements = [(1, 1, 1), (2, 1, 1)]  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))  for idx, w in enumerate(w_elements):     mtm_3.visualize_s_p(w, axes[idx]) fig.show() In\u00a0[\u00a0]: Copied! <pre>mtm_4 = MinTMatrices(levels=4)\n\nprint(\n    f\"s: {sp.latex(mtm_4.s)}\\n\"\n    f\"p: {sp.latex(mtm_4.p.as_explicit())}\\n\"\n    f\"s_p: {sp.latex(mtm_4.s_p.as_explicit())}\\n\"\n)\n\nw_elements = [(1, 1, 1, 1), (3, 1, 1, 1)]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))\n\nfor idx, w in enumerate(w_elements):\n    mtm_4.visualize_s_p(w, axes[idx])\nfig.show()\n</pre> mtm_4 = MinTMatrices(levels=4)  print(     f\"s: {sp.latex(mtm_4.s)}\\n\"     f\"p: {sp.latex(mtm_4.p.as_explicit())}\\n\"     f\"s_p: {sp.latex(mtm_4.s_p.as_explicit())}\\n\" )  w_elements = [(1, 1, 1, 1), (3, 1, 1, 1)]  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))  for idx, w in enumerate(w_elements):     mtm_4.visualize_s_p(w, axes[idx]) fig.show() In\u00a0[\u00a0]: Copied! <pre>mtm_5 = MinTMatrices(levels=5)\n\nprint(\n    f\"s: {sp.latex(mtm_5.s)}\\n\"\n    f\"p: {sp.latex(mtm_5.p.as_explicit())}\\n\"\n    f\"s_p: {sp.latex(mtm_5.s_p.as_explicit())}\\n\"\n)\n\nw_elements = [(1, 1, 1, 1, 1), (4, 1, 1, 1, 1)]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))\n\nfor idx, w in enumerate(w_elements):\n    mtm_5.visualize_s_p(w, axes[idx])\nfig.show()\n</pre> mtm_5 = MinTMatrices(levels=5)  print(     f\"s: {sp.latex(mtm_5.s)}\\n\"     f\"p: {sp.latex(mtm_5.p.as_explicit())}\\n\"     f\"s_p: {sp.latex(mtm_5.s_p.as_explicit())}\\n\" )  w_elements = [(1, 1, 1, 1, 1), (4, 1, 1, 1, 1)]  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4 * 2, 4))  for idx, w in enumerate(w_elements):     mtm_5.visualize_s_p(w, axes[idx]) fig.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(\n    \"https://github.com/datumorphism/dataset-m5-simplified/raw/main/dataset/m5_store_sales.csv\",\n    index_col=\"date\",\n)\n</pre> df = pd.read_csv(     \"https://github.com/datumorphism/dataset-m5-simplified/raw/main/dataset/m5_store_sales.csv\",     index_col=\"date\", ) In\u00a0[\u00a0]: Copied! <pre>df[\"Total\"] = df[[\"CA\", \"TX\", \"WI\"]].sum(axis=\"columns\")\n</pre> df[\"Total\"] = df[[\"CA\", \"TX\", \"WI\"]].sum(axis=\"columns\") In\u00a0[\u00a0]: Copied! <pre>df.index = pd.to_datetime(df.index)\n</pre> df.index = pd.to_datetime(df.index) In\u00a0[\u00a0]: Copied! <pre>re_simple_col = re.compile(r\"'(\\w{2}_\\d{1})'\")\n\ndf.rename(\n    columns={\n        c: re_simple_col.findall(c)[0] for c in df.columns if re_simple_col.findall(c)\n    },\n    inplace=True,\n)\n</pre> re_simple_col = re.compile(r\"'(\\w{2}_\\d{1})'\")  df.rename(     columns={         c: re_simple_col.findall(c)[0] for c in df.columns if re_simple_col.findall(c)     },     inplace=True, ) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>value_columns = df.columns.tolist()\nvalue_columns\n</pre> value_columns = df.columns.tolist() value_columns In\u00a0[\u00a0]: Copied! <pre>hierarchy = {\n    \"CA_1\": [\"CA\"],\n    \"CA_2\": [\"CA\"],\n    \"CA_3\": [\"CA\"],\n    \"CA_4\": [\"CA\"],\n    \"TX_1\": [\"TX\"],\n    \"TX_2\": [\"TX\"],\n    \"TX_3\": [\"TX\"],\n    \"WI_1\": [\"WI\"],\n    \"WI_2\": [\"WI\"],\n    \"WI_3\": [\"WI\"],\n    \"CA\": [\"Total\"],\n    \"TX\": [\"Total\"],\n    \"WI\": [\"Total\"],\n}\n</pre> hierarchy = {     \"CA_1\": [\"CA\"],     \"CA_2\": [\"CA\"],     \"CA_3\": [\"CA\"],     \"CA_4\": [\"CA\"],     \"TX_1\": [\"TX\"],     \"TX_2\": [\"TX\"],     \"TX_3\": [\"TX\"],     \"WI_1\": [\"WI\"],     \"WI_2\": [\"WI\"],     \"WI_3\": [\"WI\"],     \"CA\": [\"Total\"],     \"TX\": [\"Total\"],     \"WI\": [\"Total\"], } In\u00a0[\u00a0]: Copied! <pre>ts = TimeSeries.from_dataframe(\n    df, value_cols=value_columns, freq=\"d\", hierarchy=hierarchy\n)\n</pre> ts = TimeSeries.from_dataframe(     df, value_cols=value_columns, freq=\"d\", hierarchy=hierarchy ) In\u00a0[\u00a0]: Copied! <pre>ts\n</pre> ts In\u00a0[\u00a0]: Copied! <pre>ts_sample = ts.drop_after(ts.time_index[20])\n</pre> ts_sample = ts.drop_after(ts.time_index[20]) In\u00a0[\u00a0]: Copied! <pre>ts_sample[[\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]].plot()\n</pre> ts_sample[[\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]].plot() In\u00a0[\u00a0]: Copied! <pre>ts_sample[\"CA\"].plot(label=\"CA\")\n(ts_sample[\"CA_1\"] + ts_sample[\"CA_2\"] + ts_sample[\"CA_3\"] + ts_sample[\"CA_4\"]).plot(\n    label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\"\n)\n</pre> ts_sample[\"CA\"].plot(label=\"CA\") (ts_sample[\"CA_1\"] + ts_sample[\"CA_2\"] + ts_sample[\"CA_3\"] + ts_sample[\"CA_4\"]).plot(     label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\" ) In\u00a0[\u00a0]: Copied! <pre>ts.time_index\n</pre> ts.time_index In\u00a0[\u00a0]: Copied! <pre>ts_train, ts_test = ts.split_after(ts.time_index[1863])\n\nts_train[\"Total\"].plot(label=\"Train\")\nts_test[\"Total\"].plot(label=\"Test\")\n</pre> ts_train, ts_test = ts.split_after(ts.time_index[1863])  ts_train[\"Total\"].plot(label=\"Train\") ts_test[\"Total\"].plot(label=\"Test\") <p>We check the partial autocorrelation function to choose some parameters for our models.</p> In\u00a0[\u00a0]: Copied! <pre>plot_pacf(ts_train[\"Total\"])\n</pre> plot_pacf(ts_train[\"Total\"]) In\u00a0[\u00a0]: Copied! <pre>from darts.models import LightGBMModel\n</pre> from darts.models import LightGBMModel In\u00a0[\u00a0]: Copied! <pre>model_params = {\"lags\": 14, \"linear_tree\": True, \"output_chunk_length\": 10}\nmodel = LightGBMModel(**model_params)\n</pre> model_params = {\"lags\": 14, \"linear_tree\": True, \"output_chunk_length\": 10} model = LightGBMModel(**model_params) In\u00a0[\u00a0]: Copied! <pre>model.fit(ts_train)\n</pre> model.fit(ts_train) In\u00a0[\u00a0]: Copied! <pre>model.save(\"lightgbm.pkl\")\n</pre> model.save(\"lightgbm.pkl\") In\u00a0[\u00a0]: Copied! <pre>ts_pred = model.predict(n=len(ts_test))\n</pre> ts_pred = model.predict(n=len(ts_test)) <p>We check the performance visually for CA. The patterns looks similar but the scales are a bit off.</p> In\u00a0[\u00a0]: Copied! <pre>ca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]\nts_test[ca_columns].plot()\nts_pred[ca_columns].plot(linestyle=\"--\")\n</pre> ca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"] ts_test[ca_columns].plot() ts_pred[ca_columns].plot(linestyle=\"--\") In\u00a0[\u00a0]: Copied! <pre>vis_columns = [\"CA_4\"]\nts_test[vis_columns].plot()\nts_pred[vis_columns].plot(linestyle=\"--\")\n</pre> vis_columns = [\"CA_4\"] ts_test[vis_columns].plot() ts_pred[vis_columns].plot(linestyle=\"--\") <p>The forecasts are not coherent.</p> In\u00a0[\u00a0]: Copied! <pre>ts_pred[\"Total\"].plot(label=\"CA\")\n(ts_pred[\"CA\"] + ts_pred[\"TX\"] + ts_pred[\"WI\"]).plot(\n    label=\"CA + TX + WI\", linestyle=\"--\", color=\"r\"\n)\n</pre> ts_pred[\"Total\"].plot(label=\"CA\") (ts_pred[\"CA\"] + ts_pred[\"TX\"] + ts_pred[\"WI\"]).plot(     label=\"CA + TX + WI\", linestyle=\"--\", color=\"r\" ) In\u00a0[\u00a0]: Copied! <pre>ts_pred[\"CA\"].plot(label=\"CA\")\n(ts_pred[\"CA_1\"] + ts_pred[\"CA_2\"] + ts_pred[\"CA_3\"] + ts_pred[\"CA_4\"]).plot(\n    label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\"\n)\n</pre> ts_pred[\"CA\"].plot(label=\"CA\") (ts_pred[\"CA_1\"] + ts_pred[\"CA_2\"] + ts_pred[\"CA_3\"] + ts_pred[\"CA_4\"]).plot(     label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\" ) In\u00a0[\u00a0]: Copied! <pre>from darts.dataprocessing.transformers import MinTReconciliator\n</pre> from darts.dataprocessing.transformers import MinTReconciliator In\u00a0[\u00a0]: Copied! <pre>reconciliator = MinTReconciliator(method=\"wls_val\")\n</pre> reconciliator = MinTReconciliator(method=\"wls_val\") In\u00a0[\u00a0]: Copied! <pre>reconciliator.fit(ts_train)\nts_pred_recon = reconciliator.transform(ts_pred)\n</pre> reconciliator.fit(ts_train) ts_pred_recon = reconciliator.transform(ts_pred) In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon[\"Total\"].plot(label=\"CA\")\n(ts_pred_recon[\"CA\"] + ts_pred_recon[\"TX\"] + ts_pred_recon[\"WI\"]).plot(\n    label=\"CA + TX + WI\", linestyle=\"--\", color=\"r\"\n)\n</pre> ts_pred_recon[\"Total\"].plot(label=\"CA\") (ts_pred_recon[\"CA\"] + ts_pred_recon[\"TX\"] + ts_pred_recon[\"WI\"]).plot(     label=\"CA + TX + WI\", linestyle=\"--\", color=\"r\" ) In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon[\"CA\"].plot(label=\"CA\")\n(\n    ts_pred_recon[\"CA_1\"]\n    + ts_pred_recon[\"CA_2\"]\n    + ts_pred_recon[\"CA_3\"]\n    + ts_pred_recon[\"CA_4\"]\n).plot(label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\")\n</pre> ts_pred_recon[\"CA\"].plot(label=\"CA\") (     ts_pred_recon[\"CA_1\"]     + ts_pred_recon[\"CA_2\"]     + ts_pred_recon[\"CA_3\"]     + ts_pred_recon[\"CA_4\"] ).plot(label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\nca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]\nts_test[ca_columns].plot(ax=ax)\nts_pred_recon[ca_columns].plot(linestyle=\"--\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18)) ca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"] ts_test[ca_columns].plot(ax=ax) ts_pred_recon[ca_columns].plot(linestyle=\"--\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon_shift = ts_pred_recon - ts_pred\n</pre> ts_pred_recon_shift = ts_pred_recon - ts_pred In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\nts_pred_recon_shift[[\"Total\", \"CA\", \"WI\", \"TX\"]].plot(ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  ts_pred_recon_shift[[\"Total\", \"CA\", \"WI\", \"TX\"]].plot(ax=ax) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\nts_pred_recon_shift[ca_columns + [\"Total\"]].plot(ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18)) ts_pred_recon_shift[ca_columns + [\"Total\"]].plot(ax=ax) <p>To see how the predictions are shifted during reconciliation, we plot out the changes from reconciliation as box plots.</p> In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon_shift[ca_columns + [\"Total\"]].pd_dataframe().plot.box()\n</pre> ts_pred_recon_shift[ca_columns + [\"Total\"]].pd_dataframe().plot.box() In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon_shift[\"CA\"].pd_dataframe().plot.box()\n</pre> ts_pred_recon_shift[\"CA\"].pd_dataframe().plot.box() In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon_shift[[\"Total\", \"CA\", \"TX\", \"WI\"]].pd_dataframe().plot.box(\n    title=\"Box Plot for Reconciled - Original Prediction\"\n)\n</pre> ts_pred_recon_shift[[\"Total\", \"CA\", \"TX\", \"WI\"]].pd_dataframe().plot.box(     title=\"Box Plot for Reconciled - Original Prediction\" ) In\u00a0[\u00a0]: Copied! <pre>ts_pred_recon_shift[[\"Total\", \"CA\", \"TX\", \"WI\"]].pd_dataframe()\n</pre> ts_pred_recon_shift[[\"Total\", \"CA\", \"TX\", \"WI\"]].pd_dataframe() In\u00a0[\u00a0]: Copied! <pre>max(ts_pred.values().max(), ts_pred_recon.values().max())\n</pre> max(ts_pred.values().max(), ts_pred_recon.values().max()) In\u00a0[\u00a0]: Copied! <pre>chart_component = \"Total\"\nchart_max = max(\n    ts_pred[chart_component].values().max(),\n    ts_pred_recon[chart_component].values().max(),\n)\nchart_min = min(\n    ts_pred[chart_component].values().min(),\n    ts_pred_recon[chart_component].values().min(),\n)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n\nax.scatter(ts_pred[chart_component].values(), ts_pred_recon[chart_component].values())\n\nax.plot(np.linspace(chart_min, chart_max), np.linspace(chart_min, chart_max))\n</pre> chart_component = \"Total\" chart_max = max(     ts_pred[chart_component].values().max(),     ts_pred_recon[chart_component].values().max(), ) chart_min = min(     ts_pred[chart_component].values().min(),     ts_pred_recon[chart_component].values().min(), )  fig, ax = plt.subplots(figsize=(10, 10))   ax.scatter(ts_pred[chart_component].values(), ts_pred_recon[chart_component].values())  ax.plot(np.linspace(chart_min, chart_max), np.linspace(chart_min, chart_max)) In\u00a0[\u00a0]: Copied! <pre>chart_component = \"CA\"\nchart_max = max(\n    ts_pred[chart_component].values().max(),\n    ts_pred_recon[chart_component].values().max(),\n)\nchart_min = min(\n    ts_pred[chart_component].values().min(),\n    ts_pred_recon[chart_component].values().min(),\n)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nax.scatter(ts_pred[chart_component].values(), ts_pred_recon[chart_component].values())\n\nax.plot(np.linspace(chart_min, chart_max), np.linspace(chart_min, chart_max))\n</pre> chart_component = \"CA\" chart_max = max(     ts_pred[chart_component].values().max(),     ts_pred_recon[chart_component].values().max(), ) chart_min = min(     ts_pred[chart_component].values().min(),     ts_pred_recon[chart_component].values().min(), )  fig, ax = plt.subplots(figsize=(10, 10))  ax.scatter(ts_pred[chart_component].values(), ts_pred_recon[chart_component].values())  ax.plot(np.linspace(chart_min, chart_max), np.linspace(chart_min, chart_max)) In\u00a0[\u00a0]: Copied! <pre>reconciliator_pred_bias = MinTReconciliator(method=\"wls_val\")\n</pre> reconciliator_pred_bias = MinTReconciliator(method=\"wls_val\") In\u00a0[\u00a0]: Copied! <pre>df_pred_biased = ts_pred.pd_dataframe().copy()\ndf_pred_biased[\"CA_1\"] = df_pred_biased[\"CA_1\"] * 0.5\nts_pred_biased = TimeSeries.from_dataframe(df_pred_biased, hierarchy=ts_pred.hierarchy)\n\nts_pred[\"CA_1\"].plot(label=\"Original Prediction for CA_1\")\nts_pred_biased[\"CA_1\"].plot(label=\"Manually Shifted Prediction for CA_1\")\n</pre> df_pred_biased = ts_pred.pd_dataframe().copy() df_pred_biased[\"CA_1\"] = df_pred_biased[\"CA_1\"] * 0.5 ts_pred_biased = TimeSeries.from_dataframe(df_pred_biased, hierarchy=ts_pred.hierarchy)  ts_pred[\"CA_1\"].plot(label=\"Original Prediction for CA_1\") ts_pred_biased[\"CA_1\"].plot(label=\"Manually Shifted Prediction for CA_1\") In\u00a0[\u00a0]: Copied! <pre>reconciliator_pred_bias.fit(ts_pred_biased)\nts_pred_biased_recon = reconciliator_pred_bias.transform(ts_pred_biased)\n</pre> reconciliator_pred_bias.fit(ts_pred_biased) ts_pred_biased_recon = reconciliator_pred_bias.transform(ts_pred_biased) In\u00a0[\u00a0]: Copied! <pre>ts_pred[\"CA_1\"].plot(label=\"Original Prediction for CA_1\")\nts_pred_biased[\"CA_1\"].plot(label=\"Manually Shifted Prediction for CA_1\")\nts_pred_biased_recon[\"CA_1\"].plot(label=\"Reconciled Shifted Prediction for CA_1\")\n</pre> ts_pred[\"CA_1\"].plot(label=\"Original Prediction for CA_1\") ts_pred_biased[\"CA_1\"].plot(label=\"Manually Shifted Prediction for CA_1\") ts_pred_biased_recon[\"CA_1\"].plot(label=\"Reconciled Shifted Prediction for CA_1\") In\u00a0[\u00a0]: Copied! <pre>ts_pred_biased_recon[\"CA\"].plot(label=\"CA\")\n(\n    ts_pred_biased_recon[\"CA_1\"]\n    + ts_pred_biased_recon[\"CA_2\"]\n    + ts_pred_biased_recon[\"CA_3\"]\n    + ts_pred_biased_recon[\"CA_4\"]\n).plot(label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\")\n</pre> ts_pred_biased_recon[\"CA\"].plot(label=\"CA\") (     ts_pred_biased_recon[\"CA_1\"]     + ts_pred_biased_recon[\"CA_2\"]     + ts_pred_biased_recon[\"CA_3\"]     + ts_pred_biased_recon[\"CA_4\"] ).plot(label=\"CA_1 + CA_2 + CA_3 + CA_4\", linestyle=\"--\", color=\"r\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\nca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]\nts_test[ca_columns].plot(ax=ax)\nts_pred_biased_recon[ca_columns].plot(linestyle=\"--\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18)) ca_columns = [\"CA\", \"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"] ts_test[ca_columns].plot(ax=ax) ts_pred_biased_recon[ca_columns].plot(linestyle=\"--\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>reconciliator_mint_cov = MinTReconciliator(method=\"mint_cov\")\n\nreconciliator_mint_cov.fit(ts_pred - ts_test)\n\nts_test[ca_columns].plot()\nreconciliator_mint_cov.transform(ts_pred)[ca_columns].plot(linestyle=\"--\")\n</pre> reconciliator_mint_cov = MinTReconciliator(method=\"mint_cov\")  reconciliator_mint_cov.fit(ts_pred - ts_test)  ts_test[ca_columns].plot() reconciliator_mint_cov.transform(ts_pred)[ca_columns].plot(linestyle=\"--\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/hierarchical_forecasting_mint/#forecast-reconciliation", "title": "Forecast Reconciliation\u00b6", "text": "<p>This is a notebook for the section Hierarchical Time Series Reconciliation.</p> <p>import re</p> <p>import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns</p>"}, {"location": "notebooks/hierarchical_forecasting_mint/#some-mint-matrics", "title": "Some MinT Matrics\u00b6", "text": "<p>This section shows a few examples of the MinT method. We use these examples to interpret how MinT works.</p>"}, {"location": "notebooks/hierarchical_forecasting_mint/#load-data", "title": "Load data\u00b6", "text": "<p>We load a small sample of the M5 dataset.</p>"}, {"location": "notebooks/hierarchical_forecasting_mint/#visualize-and-validate-the-data", "title": "Visualize and Validate the Data\u00b6", "text": ""}, {"location": "notebooks/hierarchical_forecasting_mint/#forecasts", "title": "Forecasts\u00b6", "text": "<p>We split the dataset into two time series, <code>ts_train</code> and <code>ts_test</code>. We will hold out <code>ts_test</code> from training.</p>"}, {"location": "notebooks/hierarchical_forecasting_mint/#reconciliation", "title": "Reconciliation\u00b6", "text": ""}, {"location": "notebooks/hierarchical_forecasting_mint/#what-changed", "title": "What Changed\u00b6", "text": ""}, {"location": "notebooks/hierarchical_forecasting_mint/#can-reconciliations-adjust-bias", "title": "Can Reconciliations Adjust Bias?\u00b6", "text": "<p>We create some artificial bias by shifting one of the series down and then perform reconciliations.</p> <p>This assumes that the reconciliation already learned about the general patterns on different levels, since we only manually shift the predictions only. The training is not touched.</p>"}, {"location": "notebooks/lstm_properties/", "title": "LSTM", "text": "In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport seaborn as sns\n\nsns.set_theme()\nimport pandas as pd\n</pre> import numpy as np import seaborn as sns  sns.set_theme() import pandas as pd  In\u00a0[\u00a0]: Copied! <pre>def cell_state(f, m, c_prev: int = 1):\n    return np.tanh(c_prev * f + m)\n</pre> def cell_state(f, m, c_prev: int = 1):     return np.tanh(c_prev * f + m) In\u00a0[\u00a0]: Copied! <pre>f = np.linspace(-1, 1, 101)\nm = np.linspace(-1, 1, 101)\n</pre> f = np.linspace(-1, 1, 101) m = np.linspace(-1, 1, 101) In\u00a0[\u00a0]: Copied! <pre>c = []\nfor f_i in f:\n    c_m = []\n    for m_i in m:\n        c_m.append(cell_state(f_i, m_i))\n    c.append(c_m)\n</pre> c = [] for f_i in f:     c_m = []     for m_i in m:         c_m.append(cell_state(f_i, m_i))     c.append(c_m) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(c, columns=[f\"{i:.2f}\" for i in m], index=[f\"{i:.2f}\" for i in f])\n</pre> df = pd.DataFrame(c, columns=[f\"{i:.2f}\" for i in m], index=[f\"{i:.2f}\" for i in f]) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(8.5, 8))\nax = sns.heatmap(df, ax=ax)\nax.set_title(\"LSTM Cell States\")\nax.set_xlabel(r\"i * h\")\n</pre> _, ax = plt.subplots(figsize=(8.5, 8)) ax = sns.heatmap(df, ax=ax) ax.set_title(\"LSTM Cell States\") ax.set_xlabel(r\"i * h\") In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame()\nc_init = 5\n\nfor f in [0.1, 0.9]:\n    for i_g in [-0.5, 0.5, 0.9]:\n        cell_states = [\n            {\n                \"iter\": 0,\n                \"c\": c_init,\n                \"f\": f,\n                \"i*g\": i_g,\n                \"c_prev\": np.nan,\n                \"tanh_c\": np.tanh(c_init),\n            }\n        ]\n\n        for _ in range(10):\n            f_ = cell_states[-1][\"f\"]\n            c_ = cell_states[-1][\"c\"]\n            i_g_ = cell_states[-1][\"i*g\"]\n            c_new = cell_state(f=f_, m=i_g_, c_prev=c_)\n            cell_states.append(\n                {\n                    \"iter\": cell_states[-1][\"iter\"] + 1,\n                    \"c\": c_new,\n                    \"f\": f_,\n                    \"i*g\": i_g_,\n                    \"c_prev\": c_,\n                    \"tanh_c\": np.tanh(c_new),\n                }\n            )\n\n        df = pd.concat([df, pd.DataFrame(cell_states)])\n</pre> df = pd.DataFrame() c_init = 5  for f in [0.1, 0.9]:     for i_g in [-0.5, 0.5, 0.9]:         cell_states = [             {                 \"iter\": 0,                 \"c\": c_init,                 \"f\": f,                 \"i*g\": i_g,                 \"c_prev\": np.nan,                 \"tanh_c\": np.tanh(c_init),             }         ]          for _ in range(10):             f_ = cell_states[-1][\"f\"]             c_ = cell_states[-1][\"c\"]             i_g_ = cell_states[-1][\"i*g\"]             c_new = cell_state(f=f_, m=i_g_, c_prev=c_)             cell_states.append(                 {                     \"iter\": cell_states[-1][\"iter\"] + 1,                     \"c\": c_new,                     \"f\": f_,                     \"i*g\": i_g_,                     \"c_prev\": c_,                     \"tanh_c\": np.tanh(c_new),                 }             )          df = pd.concat([df, pd.DataFrame(cell_states)]) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.lineplot(\n    df,\n    x=\"iter\",\n    y=\"c\",\n    hue=\"f\",\n    style=\"i*g\",\n    size=\"f\",\n    palette=sns.palettes.color_palette(\"colorblind\")[:2],\n    markers=True,\n    ax=ax,\n)\n\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"c\")\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.lineplot(     df,     x=\"iter\",     y=\"c\",     hue=\"f\",     style=\"i*g\",     size=\"f\",     palette=sns.palettes.color_palette(\"colorblind\")[:2],     markers=True,     ax=ax, )  ax.set_xlabel(\"Iteration\") ax.set_ylabel(\"c\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.lineplot(\n    df,\n    x=\"iter\",\n    y=\"tanh_c\",\n    hue=\"f\",\n    style=\"i*g\",\n    size=\"f\",\n    palette=sns.palettes.color_palette(\"colorblind\")[:2],\n    markers=True,\n    ax=ax,\n)\n\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"tanh(c)\")\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.lineplot(     df,     x=\"iter\",     y=\"tanh_c\",     hue=\"f\",     style=\"i*g\",     size=\"f\",     palette=sns.palettes.color_palette(\"colorblind\")[:2],     markers=True,     ax=ax, )  ax.set_xlabel(\"Iteration\") ax.set_ylabel(\"tanh(c)\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.scatterplot(\n    df,\n    x=\"c_prev\",\n    y=\"c\",\n    hue=\"f\",\n    style=\"i*g\",\n    size=\"f\",\n    palette=sns.palettes.color_palette(\"colorblind\")[:2],\n    markers=True,\n    ax=ax,\n)\nax.set_xlabel(\"c_prev\")\nax.set_ylabel(\"c\")\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.scatterplot(     df,     x=\"c_prev\",     y=\"c\",     hue=\"f\",     style=\"i*g\",     size=\"f\",     palette=sns.palettes.color_palette(\"colorblind\")[:2],     markers=True,     ax=ax, ) ax.set_xlabel(\"c_prev\") ax.set_ylabel(\"c\")"}, {"location": "notebooks/lstm_properties/#lstm", "title": "LSTM\u00b6", "text": "<p>In this notebook, we demonstrate a few special properties of LSTM.</p>"}, {"location": "notebooks/lstm_properties/#cell-state-phase-space", "title": "Cell State Phase Space\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/", "title": "NeuralODE for Univariate Time Series Forecasting", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>import math\nfrom functools import cached_property\nfrom typing import Dict, List, Tuple\n\nimport lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom loguru import logger\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchdyn.core import NeuralODE\nfrom ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule\nfrom ts_dl_utils.evaluation.evaluator import Evaluator\nfrom ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster\n</pre> import math from functools import cached_property from typing import Dict, List, Tuple  import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from loguru import logger from torch import nn from torch.utils.data import DataLoader, Dataset from torchdyn.core import NeuralODE from ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule from ts_dl_utils.evaluation.evaluator import Evaluator from ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster  In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\n</pre> pen = Pendulum(length=100) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001))\n</pre> df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001)) <p>Since the damping constant is very small, the data generated is mostly a sin wave.</p> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass TSNODEParams:\n    \"\"\"A dataclass to be served as our parameters for the model.\n\n    :param hidden_widths: list of dimensions for the hidden layers\n    \"\"\"\n\n    hidden_widths: List[int]\n    time_span: torch.Tensor\n\n\nclass TSNODE(nn.Module):\n    \"\"\"NeuralODE for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param ffn_params: the parameters for the NODE network.\n    \"\"\"\n\n    def __init__(self, history_length: int, horizon: int, model_params: TSNODEParams):\n        super().__init__()\n        self.model_params = model_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.time_span = model_params.time_span\n\n        self.regulate_input = nn.Linear(\n            self.history_length, self.model_params.hidden_widths[0]\n        )\n\n        self.hidden_layers = nn.Sequential(\n            *[\n                self._linear_block(dim_in, dim_out)\n                for dim_in, dim_out in zip(\n                    self.model_params.hidden_widths[:-1],\n                    self.model_params.hidden_widths[1:],\n                )\n            ]\n        )\n\n        self.regulate_output = nn.Linear(\n            self.model_params.hidden_widths[-1], self.history_length\n        )\n\n        self.network = nn.Sequential(\n            *[self.regulate_input, self.hidden_layers, self.regulate_output]\n        )\n\n    @property\n    def node_config(self):\n        return dataclasses.asdict(self.ffn_params)\n\n    def _linear_block(self, dim_in, dim_out):\n        return nn.Sequential(*[nn.Linear(dim_in, dim_out), nn.ReLU()])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.network(x)\n</pre> @dataclasses.dataclass class TSNODEParams:     \"\"\"A dataclass to be served as our parameters for the model.      :param hidden_widths: list of dimensions for the hidden layers     \"\"\"      hidden_widths: List[int]     time_span: torch.Tensor   class TSNODE(nn.Module):     \"\"\"NeuralODE for univaraite time series modeling.      :param history_length: the length of the input history.     :param horizon: the number of steps to be forecasted.     :param ffn_params: the parameters for the NODE network.     \"\"\"      def __init__(self, history_length: int, horizon: int, model_params: TSNODEParams):         super().__init__()         self.model_params = model_params         self.history_length = history_length         self.horizon = horizon          self.time_span = model_params.time_span          self.regulate_input = nn.Linear(             self.history_length, self.model_params.hidden_widths[0]         )          self.hidden_layers = nn.Sequential(             *[                 self._linear_block(dim_in, dim_out)                 for dim_in, dim_out in zip(                     self.model_params.hidden_widths[:-1],                     self.model_params.hidden_widths[1:],                 )             ]         )          self.regulate_output = nn.Linear(             self.model_params.hidden_widths[-1], self.history_length         )          self.network = nn.Sequential(             *[self.regulate_input, self.hidden_layers, self.regulate_output]         )      @property     def node_config(self):         return dataclasses.asdict(self.ffn_params)      def _linear_block(self, dim_in, dim_out):         return nn.Sequential(*[nn.Linear(dim_in, dim_out), nn.ReLU()])      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.network(x) <p>We use lightning to train our model.</p> In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\n\ngap = 10\n</pre> history_length_1_step = 100 horizon_1_step = 1  gap = 10 <p>We will build a few utilities</p> <ol> <li>To be able to feed the data into our model, we build a class (<code>DataFrameDataset</code>) that converts the pandas dataframe into a Dataset for pytorch.</li> <li>To make the lightning training code simpler, we will build a LightningDataModule (<code>PendulumDataModule</code>) and a LightningModule (<code>FFNForecaster</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>class NODEForecaster(L.LightningModule):\n    def __init__(self, model: nn.Module):\n        super().__init__()\n        self.model = model\n\n        self.neural_ode = NeuralODE(\n            self.model.network,\n            sensitivity=\"adjoint\",\n            solver=\"dopri5\",\n            atol_adjoint=1e-4,\n            rtol_adjoint=1e-4,\n        )\n        self.time_span = self.model.time_span\n        self.horizon = self.model.horizon\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze(-1).type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        t_, y_hat = self.neural_ode(x, self.time_span)\n        y_hat = y_hat[-1, ..., -self.horizon :]\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze(-1).type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        t_, y_hat = self.neural_ode(x, self.time_span)\n        y_hat = y_hat[-1, ..., -self.horizon :]\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze(-1).type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        t_, y_hat = self.neural_ode(x, self.time_span)\n        y_hat = y_hat[-1, ..., -self.horizon :]\n\n        return x, y_hat\n\n    def forward(self, x):\n        x = x.squeeze(-1).type(self.dtype)\n        t_, y_hat = self.neural_ode(x, self.time_span)\n        y_hat = y_hat[-1, ..., -self.horizon :]\n        return x, y_hat\n</pre> class NODEForecaster(L.LightningModule):     def __init__(self, model: nn.Module):         super().__init__()         self.model = model          self.neural_ode = NeuralODE(             self.model.network,             sensitivity=\"adjoint\",             solver=\"dopri5\",             atol_adjoint=1e-4,             rtol_adjoint=1e-4,         )         self.time_span = self.model.time_span         self.horizon = self.model.horizon      def configure_optimizers(self):         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)         return optimizer      def training_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze(-1).type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          t_, y_hat = self.neural_ode(x, self.time_span)         y_hat = y_hat[-1, ..., -self.horizon :]          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"train_loss\": loss}, prog_bar=True)         return loss      def validation_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze(-1).type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          t_, y_hat = self.neural_ode(x, self.time_span)         y_hat = y_hat[-1, ..., -self.horizon :]          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"val_loss\": loss}, prog_bar=True)         return loss      def predict_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze(-1).type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          t_, y_hat = self.neural_ode(x, self.time_span)         y_hat = y_hat[-1, ..., -self.horizon :]          return x, y_hat      def forward(self, x):         x = x.squeeze(-1).type(self.dtype)         t_, y_hat = self.neural_ode(x, self.time_span)         y_hat = y_hat[-1, ..., -self.horizon :]         return x, y_hat In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = PendulumDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_1_step = PendulumDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_model_params_1_step = TSNODEParams(\n    hidden_widths=[256], time_span=torch.linspace(0, 1, 101)\n)\n\nts_node_1_step = TSNODE(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    model_params=ts_model_params_1_step,\n)\n\nts_node_1_step\n</pre> ts_model_params_1_step = TSNODEParams(     hidden_widths=[256], time_span=torch.linspace(0, 1, 101) )  ts_node_1_step = TSNODE(     history_length=history_length_1_step,     horizon=horizon_1_step,     model_params=ts_model_params_1_step, )  ts_node_1_step In\u00a0[\u00a0]: Copied! <pre>node_forecaster_1_step = NODEForecaster(model=ts_node_1_step)\n</pre> node_forecaster_1_step = NODEForecaster(model=ts_node_1_step) In\u00a0[\u00a0]: Copied! <pre>logger_1_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"neuralode_ts_1_step\"\n)\n\ntrainer_1_step = L.Trainer(\n    precision=\"32\",\n    max_epochs=10,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)\n    ],\n    logger=logger_1_step,\n)\n</pre> logger_1_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"neuralode_ts_1_step\" )  trainer_1_step = L.Trainer(     precision=\"32\",     max_epochs=10,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)     ],     logger=logger_1_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=node_forecaster_1_step, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=node_forecaster_1_step, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=node_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=node_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = L.Trainer(precision=\"64\")  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() <p>To quantify the results, we compute a few metrics.</p> In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>history_length_m_step = 100\nhorizon_m_step = 3\n</pre> history_length_m_step = 100 horizon_m_step = 3 In\u00a0[\u00a0]: Copied! <pre>pdm_m_step = PendulumDataModule(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_m_step = PendulumDataModule(     history_length=history_length_m_step,     horizon=horizon_m_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_model_params_m_step = TSNODEParams(\n    hidden_widths=[256], time_span=torch.linspace(0, 1, 101)\n)\n\nts_node_m_step = TSNODE(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    model_params=ts_model_params_m_step,\n)\n\nts_node_m_step\n</pre> ts_model_params_m_step = TSNODEParams(     hidden_widths=[256], time_span=torch.linspace(0, 1, 101) )  ts_node_m_step = TSNODE(     history_length=history_length_m_step,     horizon=horizon_m_step,     model_params=ts_model_params_m_step, )  ts_node_m_step In\u00a0[\u00a0]: Copied! <pre>node_forecaster_m_step = NODEForecaster(model=ts_node_m_step)\n</pre> node_forecaster_m_step = NODEForecaster(model=ts_node_m_step) In\u00a0[\u00a0]: Copied! <pre>logger_m_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"neuralode_ts_m_step\"\n)\n\ntrainer_m_step = L.Trainer(\n    precision=\"32\",\n    max_epochs=10,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)\n    ],\n    logger=logger_m_step,\n)\n</pre> logger_m_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"neuralode_ts_m_step\" )  trainer_m_step = L.Trainer(     precision=\"32\",     max_epochs=10,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-4, patience=2)     ],     logger=logger_m_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_m_step.fit(model=node_forecaster_m_step, datamodule=pdm_m_step)\n</pre> trainer_m_step.fit(model=node_forecaster_m_step, datamodule=pdm_m_step) In\u00a0[\u00a0]: Copied! <pre>predictions_m_step = trainer_m_step.predict(\n    model=node_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> predictions_m_step = trainer_m_step.predict(     model=node_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_m_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step)\nlobs_m_step_predictions = trainer_naive_m_step.predict(\n    model=lobs_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> trainer_naive_m_step = L.Trainer(precision=\"64\")  lobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step) lobs_m_step_predictions = trainer_naive_m_step.predict(     model=lobs_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step = Evaluator(step=2, gap=gap)\n</pre> evaluator_m_step = Evaluator(step=2, gap=gap) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\n\nfor i in np.arange(0, 1000, 120):\n    evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i)\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))   for i in np.arange(0, 1000, 120):     evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())"}, {"location": "notebooks/neuralode_timeseries/#neuralode-for-univariate-time-series-forecasting", "title": "NeuralODE for Univariate Time Series Forecasting\u00b6", "text": "<p>In this notebook, we build a NeuralODE using pytorch to forecast $\\sin$ function as a time series.</p>"}, {"location": "notebooks/neuralode_timeseries/#data", "title": "Data\u00b6", "text": "<p>We create a dataset that models a damped pendulum. The pendulum is modelled as a damped harmonic oscillator, i.e.,</p> $$ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), $$<p>where $\\theta(t)$ is the angle of the pendulum at time $t$. The period $p$ is calculated using</p> $$ p = 2 \\pi \\sqrt(L / g), $$<p>with $L$ being the length of the pendulum and $g$ being the surface gravity.</p>"}, {"location": "notebooks/neuralode_timeseries/#model", "title": "Model\u00b6", "text": "<p>In this section, we create the NeuralODE model.</p>"}, {"location": "notebooks/neuralode_timeseries/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#training-utilities", "title": "Training Utilities\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#data-model-and-training", "title": "Data, Model and Training\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#datamodule", "title": "DataModule\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#lightningmodule", "title": "LightningModule\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#trainer", "title": "Trainer\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#fitting", "title": "Fitting\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#retrieving-predictions", "title": "Retrieving Predictions\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#naive-forecasters", "title": "Naive Forecasters\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#forecasting-horizon3", "title": "Forecasting (horizon=3)\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#train-a-model", "title": "Train a Model\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/neuralode_timeseries/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/pendulum_dataset/", "title": "A Dataset Generated by Damped Pendulum", "text": "In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\nfrom typing import List, Tuple\n</pre> from functools import cached_property from typing import List, Tuple In\u00a0[\u00a0]: Copied! <pre>import lightning as L\nimport matplotlib as mpl\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchmetrics import MetricCollection\nfrom torchmetrics.regression import (\n    MeanAbsoluteError,\n    MeanAbsolutePercentageError,\n    MeanSquaredError,\n    SymmetricMeanAbsolutePercentageError,\n)\nfrom ts_dl_utils.datasets.dataset import DataFrameDataset\nfrom ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule\nfrom ts_dl_utils.evaluation.evaluator import Evaluator\nfrom ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster\n</pre> import lightning as L import matplotlib as mpl import matplotlib.animation as animation import matplotlib.pyplot as plt import pandas as pd from torchmetrics import MetricCollection from torchmetrics.regression import (     MeanAbsoluteError,     MeanAbsolutePercentageError,     MeanSquaredError,     SymmetricMeanAbsolutePercentageError, ) from ts_dl_utils.datasets.dataset import DataFrameDataset from ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule from ts_dl_utils.evaluation.evaluator import Evaluator from ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=200)\n</pre> pen = Pendulum(length=200) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(\n    pen(num_periods=5, num_samples_per_period=100, initial_angle=1, beta=0.01)\n)\n</pre> df = pd.DataFrame(     pen(num_periods=5, num_samples_per_period=100, initial_angle=1, beta=0.01) ) <p>Since the damping constant is very small, the data generated is mostly a sin wave.</p> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>history_length = 100\nhorizon = 5\n</pre> history_length = 100 horizon = 5 In\u00a0[\u00a0]: Copied! <pre>ds = DataFrameDataset(dataframe=df, history_length=history_length, horizon=horizon)\n</pre> ds = DataFrameDataset(dataframe=df, history_length=history_length, horizon=horizon) In\u00a0[\u00a0]: Copied! <pre>print(\n    f\"\"\"\n    There were {len(df)} rows in the dataframe\\n\n    We got {len(ds)} data points in the dataset (history length: {history_length}, horizon: {horizon})\n    \"\"\"\n)\n</pre> print(     f\"\"\"     There were {len(df)} rows in the dataframe\\n     We got {len(ds)} data points in the dataset (history length: {history_length}, horizon: {horizon})     \"\"\" ) <p>We can create a LightningDataModule for Lightning. When training/evaluating using Lightning, we only need to pass this object <code>pdm</code> to the training.</p> In\u00a0[\u00a0]: Copied! <pre>pdm = PendulumDataModule(\n    history_length=history_length, horizon=horizon, dataframe=df[[\"theta\"]]\n)\n</pre> pdm = PendulumDataModule(     history_length=history_length, horizon=horizon, dataframe=df[[\"theta\"]] ) In\u00a0[\u00a0]: Copied! <pre>prediction_truths = [i[1].squeeze() for i in pdm.predict_dataloader()]\n</pre> prediction_truths = [i[1].squeeze() for i in pdm.predict_dataloader()] In\u00a0[\u00a0]: Copied! <pre>trainer_naive = L.Trainer(precision=\"64\")\n\nlobs_forecaster = LastObservationForecaster(horizon=horizon)\nlobs_predictions = trainer_naive.predict(model=lobs_forecaster, datamodule=pdm)\n</pre> trainer_naive = L.Trainer(precision=\"64\")  lobs_forecaster = LastObservationForecaster(horizon=horizon) lobs_predictions = trainer_naive.predict(model=lobs_forecaster, datamodule=pdm) In\u00a0[\u00a0]: Copied! <pre>evaluator = Evaluator(step=0)\n</pre> evaluator = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator.y_true(dataloader=pdm.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator.y(lobs_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator.y_true(dataloader=pdm.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator.y(lobs_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>evaluator.metrics(lobs_predictions, pdm.predict_dataloader())\n</pre> evaluator.metrics(lobs_predictions, pdm.predict_dataloader()) <p>Naive forecaster works well since we do not have dramatic changes between two time steps.</p> In\u00a0[\u00a0]: Copied! <pre>ds_de = DataFrameDataset(dataframe=df[\"theta\"][:200], history_length=1, horizon=1)\n</pre> ds_de = DataFrameDataset(dataframe=df[\"theta\"][:200], history_length=1, horizon=1) In\u00a0[\u00a0]: Copied! <pre>class DelayedEmbeddingAnimation:\n    \"\"\"Builds an animation for univariate time series\n    using delayed embedding.\n\n    ```python\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    dea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)\n    ani = dea.build(interval=10, save_count=dea.time_steps)\n    ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")\n    ```\n\n    :param dataset: a PyTorch dataset, input and target should have only length 1\n    :param fig: figure object from matplotlib\n    :param ax: axis object from matplotlib\n    \"\"\"\n\n    def __init__(\n        self, dataset: DataFrameDataset, fig: mpl.figure.Figure, ax: mpl.axes.Axes\n    ):\n        self.dataset = dataset\n        self.ax = ax\n        self.fig = fig\n\n    @cached_property\n    def data(self) -&gt; List[Tuple[float, float]]:\n        return [(i[0][0], i[1][0]) for i in self.dataset]\n\n    @cached_property\n    def x(self):\n        return [i[0] for i in self.data]\n\n    @cached_property\n    def y(self):\n        return [i[1] for i in self.data]\n\n    def data_gen(self):\n        for i in self.data:\n            yield i\n\n    def animation_init(self) -&gt; mpl.axes.Axes:\n        ax.plot(\n            self.x,\n            self.y,\n        )\n        ax.set_xlim([-1.1, 1.1])\n        ax.set_ylim([-1.1, 1.1])\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"t+1\")\n\n        return self.ax\n\n    def animation_run(self, data: Tuple[float, float]) -&gt; mpl.axes.Axes:\n        x, y = data\n        self.ax.scatter(x, y)\n        return self.ax\n\n    @cached_property\n    def time_steps(self):\n        return len(self.data)\n\n    def build(self, interval: int = 10, save_count: int = 10):\n        return animation.FuncAnimation(\n            self.fig,\n            self.animation_run,\n            self.data_gen,\n            interval=interval,\n            init_func=self.animation_init,\n            save_count=save_count,\n        )\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\ndea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)\n\nani = dea.build(interval=10, save_count=dea.time_steps)\n\ngif_writer = animation.PillowWriter(fps=5, metadata=dict(artist=\"Lei Ma\"), bitrate=100)\n\nani.save(\"results/pendulum_dataset/delayed_embedding_animation.gif\", writer=gif_writer)\n# ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")\n</pre> class DelayedEmbeddingAnimation:     \"\"\"Builds an animation for univariate time series     using delayed embedding.      ```python     fig, ax = plt.subplots(figsize=(10, 10))      dea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)     ani = dea.build(interval=10, save_count=dea.time_steps)     ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")     ```      :param dataset: a PyTorch dataset, input and target should have only length 1     :param fig: figure object from matplotlib     :param ax: axis object from matplotlib     \"\"\"      def __init__(         self, dataset: DataFrameDataset, fig: mpl.figure.Figure, ax: mpl.axes.Axes     ):         self.dataset = dataset         self.ax = ax         self.fig = fig      @cached_property     def data(self) -&gt; List[Tuple[float, float]]:         return [(i[0][0], i[1][0]) for i in self.dataset]      @cached_property     def x(self):         return [i[0] for i in self.data]      @cached_property     def y(self):         return [i[1] for i in self.data]      def data_gen(self):         for i in self.data:             yield i      def animation_init(self) -&gt; mpl.axes.Axes:         ax.plot(             self.x,             self.y,         )         ax.set_xlim([-1.1, 1.1])         ax.set_ylim([-1.1, 1.1])         ax.set_xlabel(\"t\")         ax.set_ylabel(\"t+1\")          return self.ax      def animation_run(self, data: Tuple[float, float]) -&gt; mpl.axes.Axes:         x, y = data         self.ax.scatter(x, y)         return self.ax      @cached_property     def time_steps(self):         return len(self.data)      def build(self, interval: int = 10, save_count: int = 10):         return animation.FuncAnimation(             self.fig,             self.animation_run,             self.data_gen,             interval=interval,             init_func=self.animation_init,             save_count=save_count,         )   fig, ax = plt.subplots(figsize=(10, 10))  dea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)  ani = dea.build(interval=10, save_count=dea.time_steps)  gif_writer = animation.PillowWriter(fps=5, metadata=dict(artist=\"Lei Ma\"), bitrate=100)  ani.save(\"results/pendulum_dataset/delayed_embedding_animation.gif\", writer=gif_writer) # ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")"}, {"location": "notebooks/pendulum_dataset/#a-dataset-generated-by-damped-pendulum", "title": "A Dataset Generated by Damped Pendulum\u00b6", "text": "<p>In this notebook, we demo a dataset we created to simulate the oscillations of a pendulumn.</p>"}, {"location": "notebooks/pendulum_dataset/#data", "title": "Data\u00b6", "text": "<p>We create a dataset that models a damped pendulum. The pendulum is modelled as a damped harmonic oscillator, i.e.,</p> $$ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), $$<p>where $\\theta(t)$ is the angle of the pendulum at time $t$. The period $p$ is calculated using</p> $$ p = 2 \\pi \\sqrt(L / g), $$<p>with $L$ being the length of the pendulum and $g$ being the surface gravity.</p>"}, {"location": "notebooks/pendulum_dataset/#pytorch-and-lighting-datamodule", "title": "PyTorch and Lighting DataModule\u00b6", "text": ""}, {"location": "notebooks/pendulum_dataset/#naive-forecasts", "title": "Naive Forecasts\u00b6", "text": ""}, {"location": "notebooks/pendulum_dataset/#delayed-embedding", "title": "Delayed Embedding\u00b6", "text": ""}, {"location": "notebooks/rnn_phase_space/", "title": "RNN Dynamics During Inference", "text": "In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\n</pre> from functools import cached_property In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import numpy as np import numpy.typing as npt import pandas as pd import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>class RNNState:\n    \"\"\"\n    Describes a RNN state and computes the history of the states\n    based on inputs.\n\n    :param w_h: $W_h$, the weight for $h$\n    :param w_i: $W_i$, the weight for $x$\n    :param b: $b$, the bias before applying activation\n    :param h_init: the initial hidden state\n    :param activation: the activation function to be applied.\n    \"\"\"\n\n    def __init__(\n        self,\n        w_h: npt.ArrayLike,\n        w_i: npt.ArrayLike,\n        b: npt.ArrayLike,\n        h_init: npt.ArrayLike,\n        activation: callable = np.tanh,\n    ):\n        self.w_h = np.array(w_h)\n        self.w_i = np.array(w_i)\n        self.b = np.array(b)\n        self.h_init = np.array(h_init)\n        self.activation = activation\n\n    def compute_new_state(\n        self, h_current: npt.ArrayLike, z_i: npt.ArrayLike\n    ) -&gt; tuple[npt.ArrayLike, npt.ArrayLike]:\n        \"\"\"\n        compute_new_state computes a new state\n\n        :param h_current: the current hidden state\n        :param z_i: external input\n        :return: the new hidden state, and the difference between new and old\n        \"\"\"\n        h_new = self.activation(\n            np.dot(self.w_h, h_current) + np.dot(self.w_i, z_i) + self.b\n        )\n        h_delta = h_new - h_current\n\n        return h_new, h_delta\n\n    @cached_property\n    def _metadata(self) -&gt; dict:\n        return {\n            \"experiment\": (\n                f\"w_h={np.array2string(self.w_h)};w_i={np.array2string(self.w_i)};\"\n                f\"b={np.array2string(self.b)};h_init={np.array2string(self.h_init)};{self.activation.__name__}\"\n            ),\n            \"w_h\": np.array2string(self.w_h),\n            \"w_i\": np.array2string(self.w_i),\n            \"b\": np.array2string(self.b),\n            \"activation\": self.activation.__name__,\n            \"initial_state\": np.array2string(self.h_init),\n        }\n\n    def states(self, z: npt.ArrayLike) -&gt; dict[str : npt.ArrayLike]:\n        \"\"\"\n        states calculates the history of RNN states.\n\n        We designed function to be easily readable by\n        computing the values step by step.\n\n        :param z: input values for the RNN\n        :return: history of the RNN hidden states\n        \"\"\"\n        h_t = [self.h_init]\n        t_steps = [0]\n        h_t_delta = [np.zeros_like(self.h_init)]\n        for t, z_i in enumerate(z):\n            h_new, h_delta = self.compute_new_state(h_current=h_t[-1], z_i=z_i)\n            h_t.append(h_new)\n            h_t_delta.append(h_delta)\n            t_steps.append(t + 1)\n\n        total_time_steps = len(t_steps)\n\n        return {\n            **{\n                \"t\": np.array(t_steps),\n                \"h\": np.array(h_t),\n                \"dh\": np.array(h_t_delta),\n                \"z\": np.pad(z, (1, 0), constant_values=0),\n            },\n            **{k: [v] * total_time_steps for k, v in self._metadata.items()},\n        }\n</pre> class RNNState:     \"\"\"     Describes a RNN state and computes the history of the states     based on inputs.      :param w_h: $W_h$, the weight for $h$     :param w_i: $W_i$, the weight for $x$     :param b: $b$, the bias before applying activation     :param h_init: the initial hidden state     :param activation: the activation function to be applied.     \"\"\"      def __init__(         self,         w_h: npt.ArrayLike,         w_i: npt.ArrayLike,         b: npt.ArrayLike,         h_init: npt.ArrayLike,         activation: callable = np.tanh,     ):         self.w_h = np.array(w_h)         self.w_i = np.array(w_i)         self.b = np.array(b)         self.h_init = np.array(h_init)         self.activation = activation      def compute_new_state(         self, h_current: npt.ArrayLike, z_i: npt.ArrayLike     ) -&gt; tuple[npt.ArrayLike, npt.ArrayLike]:         \"\"\"         compute_new_state computes a new state          :param h_current: the current hidden state         :param z_i: external input         :return: the new hidden state, and the difference between new and old         \"\"\"         h_new = self.activation(             np.dot(self.w_h, h_current) + np.dot(self.w_i, z_i) + self.b         )         h_delta = h_new - h_current          return h_new, h_delta      @cached_property     def _metadata(self) -&gt; dict:         return {             \"experiment\": (                 f\"w_h={np.array2string(self.w_h)};w_i={np.array2string(self.w_i)};\"                 f\"b={np.array2string(self.b)};h_init={np.array2string(self.h_init)};{self.activation.__name__}\"             ),             \"w_h\": np.array2string(self.w_h),             \"w_i\": np.array2string(self.w_i),             \"b\": np.array2string(self.b),             \"activation\": self.activation.__name__,             \"initial_state\": np.array2string(self.h_init),         }      def states(self, z: npt.ArrayLike) -&gt; dict[str : npt.ArrayLike]:         \"\"\"         states calculates the history of RNN states.          We designed function to be easily readable by         computing the values step by step.          :param z: input values for the RNN         :return: history of the RNN hidden states         \"\"\"         h_t = [self.h_init]         t_steps = [0]         h_t_delta = [np.zeros_like(self.h_init)]         for t, z_i in enumerate(z):             h_new, h_delta = self.compute_new_state(h_current=h_t[-1], z_i=z_i)             h_t.append(h_new)             h_t_delta.append(h_delta)             t_steps.append(t + 1)          total_time_steps = len(t_steps)          return {             **{                 \"t\": np.array(t_steps),                 \"h\": np.array(h_t),                 \"dh\": np.array(h_t_delta),                 \"z\": np.pad(z, (1, 0), constant_values=0),             },             **{k: [v] * total_time_steps for k, v in self._metadata.items()},         } In\u00a0[\u00a0]: Copied! <pre>def rnn_inference(rnn_params: list[dict], z: npt.ArrayLike) -&gt; pd.DataFrame:\n    \"\"\"\n    Run through a list of parameters and return the states\n\n    :param rnn_params: list of RNN parameters\n    :param z: input time series values\n    \"\"\"\n    df_experiments = pd.DataFrame()\n    for p in rnn_params:\n        df_experiments = pd.concat(\n            [df_experiments, pd.DataFrame(RNNState(**p).states(z=z))]\n        )\n\n    return df_experiments\n\n\ndef rnn_inference_1d_visual(dataframe_experiment: pd.DataFrame, title: str) -&gt; None:\n    \"\"\"\n    Visualize RNN inference experiments\n\n    :param dataframe_experiment: dataframe from the inference experiment\n    :param title: title of the figure\n    \"\"\"\n\n    z = dataframe_experiment.loc[\n        dataframe_experiment.experiment == dataframe_experiment.experiment.iloc[0]\n    ].z\n\n    _, ax = plt.subplots(figsize=(10, 6.18))\n\n    sns.lineplot(\n        dataframe_experiment,\n        x=\"t\",\n        y=\"h\",\n        hue=\"w_h\",\n        size=\"initial_state\",\n        linestyle=\"dashed\",\n        ax=ax,\n    )\n\n    ax_right = ax.twinx()\n\n    sns.lineplot(\n        x=np.arange(1, len(z) + 1),\n        y=z,\n        linestyle=\"dashed\",\n        color=\"gray\",\n        label=r\"Input: $z$\",\n        ax=ax_right,\n    )\n\n    ax_right.set_ylabel(r\"$z$\")\n    ax.legend(loc=1)\n    ax.set_title(title)\n    ax.legend(loc=4)\n</pre> def rnn_inference(rnn_params: list[dict], z: npt.ArrayLike) -&gt; pd.DataFrame:     \"\"\"     Run through a list of parameters and return the states      :param rnn_params: list of RNN parameters     :param z: input time series values     \"\"\"     df_experiments = pd.DataFrame()     for p in rnn_params:         df_experiments = pd.concat(             [df_experiments, pd.DataFrame(RNNState(**p).states(z=z))]         )      return df_experiments   def rnn_inference_1d_visual(dataframe_experiment: pd.DataFrame, title: str) -&gt; None:     \"\"\"     Visualize RNN inference experiments      :param dataframe_experiment: dataframe from the inference experiment     :param title: title of the figure     \"\"\"      z = dataframe_experiment.loc[         dataframe_experiment.experiment == dataframe_experiment.experiment.iloc[0]     ].z      _, ax = plt.subplots(figsize=(10, 6.18))      sns.lineplot(         dataframe_experiment,         x=\"t\",         y=\"h\",         hue=\"w_h\",         size=\"initial_state\",         linestyle=\"dashed\",         ax=ax,     )      ax_right = ax.twinx()      sns.lineplot(         x=np.arange(1, len(z) + 1),         y=z,         linestyle=\"dashed\",         color=\"gray\",         label=r\"Input: $z$\",         ax=ax_right,     )      ax_right.set_ylabel(r\"$z$\")     ax.legend(loc=1)     ax.set_title(title)     ax.legend(loc=4) In\u00a0[\u00a0]: Copied! <pre>z_1 = np.linspace(0, 10, 101)\n# z_1 =\nz_1 = np.random.rand(20)\n# z_1 = np.sin(np.linspace(0, 10, 51))\n</pre> z_1 = np.linspace(0, 10, 101) # z_1 = z_1 = np.random.rand(20) # z_1 = np.sin(np.linspace(0, 10, 51)) In\u00a0[\u00a0]: Copied! <pre>experiment_params = [\n    {\"w_h\": 0.5, \"w_i\": 1, \"b\": 0, \"h_init\": 0.1},\n    {\"w_h\": 1.5, \"w_i\": 1, \"b\": 0, \"h_init\": 0.1},\n    {\"w_h\": 0.5, \"w_i\": 1, \"b\": 0, \"h_init\": 2},\n    {\"w_h\": 1.5, \"w_i\": 1, \"b\": 0, \"h_init\": 2},\n]\n</pre> experiment_params = [     {\"w_h\": 0.5, \"w_i\": 1, \"b\": 0, \"h_init\": 0.1},     {\"w_h\": 1.5, \"w_i\": 1, \"b\": 0, \"h_init\": 0.1},     {\"w_h\": 0.5, \"w_i\": 1, \"b\": 0, \"h_init\": 2},     {\"w_h\": 1.5, \"w_i\": 1, \"b\": 0, \"h_init\": 2}, ] In\u00a0[\u00a0]: Copied! <pre>rnn_inference_1d_visual(\n    dataframe_experiment=rnn_inference(\n        rnn_params=experiment_params, z=np.ones(10) * 0.5\n    ),\n    title=\"RNN Inference for Long Forecast Horizon (Constant Input)\",\n)\n</pre> rnn_inference_1d_visual(     dataframe_experiment=rnn_inference(         rnn_params=experiment_params, z=np.ones(10) * 0.5     ),     title=\"RNN Inference for Long Forecast Horizon (Constant Input)\", ) In\u00a0[\u00a0]: Copied! <pre>rnn_inference_1d_visual(\n    dataframe_experiment=rnn_inference(\n        rnn_params=experiment_params, z=np.linspace(0, 10, 101)\n    ),\n    title=\"RNN Inference for Long Forecast Horizon (Linear Input)\",\n)\n</pre> rnn_inference_1d_visual(     dataframe_experiment=rnn_inference(         rnn_params=experiment_params, z=np.linspace(0, 10, 101)     ),     title=\"RNN Inference for Long Forecast Horizon (Linear Input)\", ) In\u00a0[\u00a0]: Copied! <pre>rnn_inference_1d_visual(\n    dataframe_experiment=rnn_inference(\n        rnn_params=experiment_params, z=np.random.rand(20)\n    ),\n    title=\"RNN Inference for Long Forecast Horizon (Random Input)\",\n)\n</pre> rnn_inference_1d_visual(     dataframe_experiment=rnn_inference(         rnn_params=experiment_params, z=np.random.rand(20)     ),     title=\"RNN Inference for Long Forecast Horizon (Random Input)\", ) In\u00a0[\u00a0]: Copied! <pre>rnn_inference_1d_visual(\n    dataframe_experiment=rnn_inference(\n        rnn_params=experiment_params, z=np.sin(np.linspace(0, 10, 51))\n    ),\n    title=\"RNN Inference for Long Forecast Horizon (Sin Input)\",\n)\n</pre> rnn_inference_1d_visual(     dataframe_experiment=rnn_inference(         rnn_params=experiment_params, z=np.sin(np.linspace(0, 10, 51))     ),     title=\"RNN Inference for Long Forecast Horizon (Sin Input)\", ) In\u00a0[\u00a0]: Copied! <pre># z_2 = np.random.rand(100, 2) * 0.1\n# z_2 = np.ones((100, 2))\n# z_2 = np.zeros((1000, 2))\nz_2 = (\n    np.random.rand(100, 2) * 0.1 + np.stack([np.zeros(100), np.linspace(0, 99, 100)]).T\n)\n</pre> # z_2 = np.random.rand(100, 2) * 0.1 # z_2 = np.ones((100, 2)) # z_2 = np.zeros((1000, 2)) z_2 = (     np.random.rand(100, 2) * 0.1 + np.stack([np.zeros(100), np.linspace(0, 99, 100)]).T ) In\u00a0[\u00a0]: Copied! <pre>experiment_2d_params = [\n    {\n        \"w_h\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n        \"w_i\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n        \"b\": np.array([0, 0]),\n        \"h_init\": np.array([0.5, 0.5]),\n    },\n    {\n        \"w_h\": np.array([[1.5, 0.5], [0.5, 1.5]]),\n        \"w_i\": np.array([[1.5, 0.5], [0.5, 1.5]]),\n        \"b\": np.array([0, 0]),\n        \"h_init\": np.array([0.5, 0.5]),\n    },\n]\n\nexperiments_2d = []\nfor p in experiment_2d_params:\n    experiments_2d.append(RNNState(**p).states(z=z_2))\n</pre> experiment_2d_params = [     {         \"w_h\": np.array([[0.5, 0.5], [0.5, 0.5]]),         \"w_i\": np.array([[0.5, 0.5], [0.5, 0.5]]),         \"b\": np.array([0, 0]),         \"h_init\": np.array([0.5, 0.5]),     },     {         \"w_h\": np.array([[1.5, 0.5], [0.5, 1.5]]),         \"w_i\": np.array([[1.5, 0.5], [0.5, 1.5]]),         \"b\": np.array([0, 0]),         \"h_init\": np.array([0.5, 0.5]),     }, ]  experiments_2d = [] for p in experiment_2d_params:     experiments_2d.append(RNNState(**p).states(z=z_2)) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\nexperiments_2d_colors = sns.color_palette(\"husl\", len(experiments_2d))\n\nfor idx, i in enumerate(experiments_2d):\n    ax.plot(\n        i[\"t\"][2:],\n        i[\"h\"][2:, 1],\n        marker=\".\",\n        color=experiments_2d_colors[idx],\n        label=i[\"experiment\"][0],\n    )\n    ax.plot(\n        i[\"t\"][2:],\n        i[\"h\"][2:, 0],\n        marker=\"x\",\n        color=experiments_2d_colors[idx],\n        label=i[\"experiment\"][0],\n    )\n\nplt.legend()\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  experiments_2d_colors = sns.color_palette(\"husl\", len(experiments_2d))  for idx, i in enumerate(experiments_2d):     ax.plot(         i[\"t\"][2:],         i[\"h\"][2:, 1],         marker=\".\",         color=experiments_2d_colors[idx],         label=i[\"experiment\"][0],     )     ax.plot(         i[\"t\"][2:],         i[\"h\"][2:, 0],         marker=\"x\",         color=experiments_2d_colors[idx],         label=i[\"experiment\"][0],     )  plt.legend()"}, {"location": "notebooks/rnn_phase_space/#rnn-dynamics-during-inference", "title": "RNN Dynamics During Inference\u00b6", "text": "<p>We have the intuition that RNN inference is similar to a first order differential equation. Here we explore and echo on this idea using numerical simulations.</p>"}, {"location": "notebooks/rnn_phase_space/#rnn", "title": "RNN\u00b6", "text": ""}, {"location": "notebooks/rnn_phase_space/#one-dimensional-state", "title": "One Dimensional State\u00b6", "text": ""}, {"location": "notebooks/rnn_phase_space/#two-dimensional-state", "title": "Two dimensional state\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/", "title": "RNN for Univariate Time Series Forecasting", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\nfrom typing import Dict, List, Tuple\n\nimport lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom loguru import logger\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule\nfrom ts_dl_utils.evaluation.evaluator import Evaluator\nfrom ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster\n</pre> from functools import cached_property from typing import Dict, List, Tuple  import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from loguru import logger from torch import nn from torch.utils.data import DataLoader, Dataset from ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule from ts_dl_utils.evaluation.evaluator import Evaluator from ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster  In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\n</pre> pen = Pendulum(length=100) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001))\n</pre> df = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001)) <p>Since the damping constant is very small, the data generated is mostly a sin wave.</p> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass TSRNNParams:\n    \"\"\"A dataclass to be served as our parameters for the model.\n\n    :param hidden_size: number of dimensions in the hidden state\n    :param input_size: input dim\n    :param num_layers: number of units stacked\n    \"\"\"\n\n    input_size: int\n    hidden_size: int\n    num_layers: int = 1\n\n\nclass TSRNN(nn.Module):\n    \"\"\"RNN for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param rnn_params: the parameters for the RNN network.\n    \"\"\"\n\n    def __init__(self, history_length: int, horizon: int, rnn_params: TSRNNParams):\n        super().__init__()\n        self.rnn_params = rnn_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.regulate_input = nn.Linear(self.history_length, self.rnn_params.input_size)\n\n        self.rnn = nn.RNN(\n            input_size=self.rnn_params.input_size,\n            hidden_size=self.rnn_params.hidden_size,\n            num_layers=self.rnn_params.num_layers,\n            batch_first=True,\n        )\n\n        self.regulate_output = nn.Linear(self.rnn_params.hidden_size, self.horizon)\n\n    @property\n    def rnn_config(self):\n        return dataclasses.asdict(self.rnn_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.regulate_input(x)\n        x, _ = self.rnn(x)\n\n        return self.regulate_output(x)\n</pre> @dataclasses.dataclass class TSRNNParams:     \"\"\"A dataclass to be served as our parameters for the model.      :param hidden_size: number of dimensions in the hidden state     :param input_size: input dim     :param num_layers: number of units stacked     \"\"\"      input_size: int     hidden_size: int     num_layers: int = 1   class TSRNN(nn.Module):     \"\"\"RNN for univaraite time series modeling.      :param history_length: the length of the input history.     :param horizon: the number of steps to be forecasted.     :param rnn_params: the parameters for the RNN network.     \"\"\"      def __init__(self, history_length: int, horizon: int, rnn_params: TSRNNParams):         super().__init__()         self.rnn_params = rnn_params         self.history_length = history_length         self.horizon = horizon          self.regulate_input = nn.Linear(self.history_length, self.rnn_params.input_size)          self.rnn = nn.RNN(             input_size=self.rnn_params.input_size,             hidden_size=self.rnn_params.hidden_size,             num_layers=self.rnn_params.num_layers,             batch_first=True,         )          self.regulate_output = nn.Linear(self.rnn_params.hidden_size, self.horizon)      @property     def rnn_config(self):         return dataclasses.asdict(self.rnn_params)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         x = self.regulate_input(x)         x, _ = self.rnn(x)          return self.regulate_output(x) <p>We use lightning to train our model.</p> In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\n\ngap = 10\n</pre> history_length_1_step = 100 horizon_1_step = 1  gap = 10 <p>We will build a few utilities</p> <ol> <li>To be able to feed the data into our model, we build a class (<code>DataFrameDataset</code>) that converts the pandas dataframe into a Dataset for pytorch.</li> <li>To make the lightning training code simpler, we will build a LightningDataModule (<code>PendulumDataModule</code>) and a LightningModule (<code>RNNForecaster</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>class RNNForecaster(L.LightningModule):\n    def __init__(self, rnn: nn.Module):\n        super().__init__()\n        self.rnn = rnn\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n\n        loss = nn.functional.l1_loss(y_hat, y)\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n\n        loss = nn.functional.l1_loss(y_hat, y)\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n        return x, y_hat\n\n    def forward(self, x):\n        x = x.squeeze().type(self.dtype)\n        return x, self.rnn(x)\n</pre> class RNNForecaster(L.LightningModule):     def __init__(self, rnn: nn.Module):         super().__init__()         self.rnn = rnn      def configure_optimizers(self):         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)         return optimizer      def training_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)          loss = nn.functional.l1_loss(y_hat, y)         self.log_dict({\"train_loss\": loss}, prog_bar=True)         return loss      def validation_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)          loss = nn.functional.l1_loss(y_hat, y)         self.log_dict({\"val_loss\": loss}, prog_bar=True)         return loss      def predict_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)         return x, y_hat      def forward(self, x):         x = x.squeeze().type(self.dtype)         return x, self.rnn(x) In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = PendulumDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    gap=gap,\n    dataframe=df[[\"theta\"]],\n)\n</pre> pdm_1_step = PendulumDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     gap=gap,     dataframe=df[[\"theta\"]], ) In\u00a0[\u00a0]: Copied! <pre>ts_rnn_params_1_step = TSRNNParams(input_size=96, hidden_size=64, num_layers=1)\n\nts_rnn_1_step = TSRNN(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    rnn_params=ts_rnn_params_1_step,\n)\n\nts_rnn_1_step\n</pre> ts_rnn_params_1_step = TSRNNParams(input_size=96, hidden_size=64, num_layers=1)  ts_rnn_1_step = TSRNN(     history_length=history_length_1_step,     horizon=horizon_1_step,     rnn_params=ts_rnn_params_1_step, )  ts_rnn_1_step In\u00a0[\u00a0]: Copied! <pre>rnn_forecaster_1_step = RNNForecaster(rnn=ts_rnn_1_step)\n</pre> rnn_forecaster_1_step = RNNForecaster(rnn=ts_rnn_1_step) In\u00a0[\u00a0]: Copied! <pre>logger_1_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"rnn_ts_1_step\"\n)\n\ntrainer_1_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-5, patience=2)\n    ],\n    logger=logger_1_step,\n)\n</pre> logger_1_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"rnn_ts_1_step\" )  trainer_1_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-5, patience=2)     ],     logger=logger_1_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=rnn_forecaster_1_step, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=rnn_forecaster_1_step, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=rnn_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=rnn_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = L.Trainer(precision=\"64\")  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>history_length_m_step = 100\nhorizon_m_step = 3\n</pre> history_length_m_step = 100 horizon_m_step = 3 In\u00a0[\u00a0]: Copied! <pre>pdm_m_step = PendulumDataModule(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_m_step = PendulumDataModule(     history_length=history_length_m_step,     horizon=horizon_m_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_rnn_params_m_step = TSRNNParams(input_size=96, hidden_size=64, num_layers=1)\n\nts_rnn_m_step = TSRNN(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    rnn_params=ts_rnn_params_m_step,\n)\n\nts_rnn_m_step\n</pre> ts_rnn_params_m_step = TSRNNParams(input_size=96, hidden_size=64, num_layers=1)  ts_rnn_m_step = TSRNN(     history_length=history_length_m_step,     horizon=horizon_m_step,     rnn_params=ts_rnn_params_m_step, )  ts_rnn_m_step In\u00a0[\u00a0]: Copied! <pre>rnn_forecaster_m_step = RNNForecaster(rnn=ts_rnn_m_step)\n</pre> rnn_forecaster_m_step = RNNForecaster(rnn=ts_rnn_m_step) In\u00a0[\u00a0]: Copied! <pre>logger_m_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"rnn_ts_m_step\"\n)\n\ntrainer_m_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-5, patience=2)\n    ],\n    logger=logger_m_step,\n)\n</pre> logger_m_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"rnn_ts_m_step\" )  trainer_m_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-5, patience=2)     ],     logger=logger_m_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_m_step.fit(model=rnn_forecaster_m_step, datamodule=pdm_m_step)\n</pre> trainer_m_step.fit(model=rnn_forecaster_m_step, datamodule=pdm_m_step) In\u00a0[\u00a0]: Copied! <pre>predictions_m_step = trainer_m_step.predict(\n    model=rnn_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> predictions_m_step = trainer_m_step.predict(     model=rnn_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_m_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step)\nlobs_m_step_predictions = trainer_naive_m_step.predict(\n    model=lobs_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> trainer_naive_m_step = L.Trainer(precision=\"64\")  lobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step) lobs_m_step_predictions = trainer_naive_m_step.predict(     model=lobs_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step = Evaluator(step=2, gap=gap)\n</pre> evaluator_m_step = Evaluator(step=2, gap=gap) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\n\nfor i in np.arange(0, 1000, 120):\n    evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i)\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))   for i in np.arange(0, 1000, 120):     evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())"}, {"location": "notebooks/rnn_timeseries/#rnn-for-univariate-time-series-forecasting", "title": "RNN for Univariate Time Series Forecasting\u00b6", "text": "<p>In this notebook, we build a RNN using pytorch to forecast $\\sin$ function as a time series.</p>"}, {"location": "notebooks/rnn_timeseries/#data", "title": "Data\u00b6", "text": "<p>We create a dataset that models a damped pendulum. The pendulum is modelled as a damped harmonic oscillator, i.e.,</p> $$ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), $$<p>where $\\theta(t)$ is the angle of the pendulum at time $t$. The period $p$ is calculated using</p> $$ p = 2 \\pi \\sqrt(L / g), $$<p>with $L$ being the length of the pendulum and $g$ being the surface gravity.</p>"}, {"location": "notebooks/rnn_timeseries/#model", "title": "Model\u00b6", "text": "<p>In this section, we create the RNN model.</p>"}, {"location": "notebooks/rnn_timeseries/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#training-utilities", "title": "Training Utilities\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#data-model-and-training", "title": "Data, Model and Training\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#datamodule", "title": "DataModule\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#lightningmodule", "title": "LightningModule\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#trainer", "title": "Trainer\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#fitting", "title": "Fitting\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#retrieving-predictions", "title": "Retrieving Predictions\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#multi-horizon-forecast-h3", "title": "Multi-horizon Forecast (h=3)\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#train-a-model", "title": "Train a Model\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/", "title": "Comparing RNN Models for Time Series Forecasting", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>import lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom torch import nn\nfrom ts_bolt.datamodules.pandas import DataFrameDataModule\nfrom ts_bolt.evaluation.evaluator import Evaluator\nfrom ts_bolt.naive_forecasters.last_observation import LastObservationForecaster\n</pre> import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from torch import nn from ts_bolt.datamodules.pandas import DataFrameDataModule from ts_bolt.evaluation.evaluator import Evaluator from ts_bolt.naive_forecasters.last_observation import LastObservationForecaster In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(\n    {\"t\": np.linspace(0, 100, 501), \"y\": np.sin(np.linspace(0, 100, 501))}\n)\n</pre> df = pd.DataFrame(     {\"t\": np.linspace(0, 100, 501), \"y\": np.sin(np.linspace(0, 100, 501))} ) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"y\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"y\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass TSRNNParams:\n    \"\"\"A dataclass to be served as our parameters\n    for the model.\n\n    :param hidden_size: number of dimensions in the hidden state\n    :param input_size: input dim\n    :param num_layers: number of units stacked\n    \"\"\"\n\n    hidden_size: int\n    input_size: int = 1\n    num_layers: int = 1\n\n\nclass TSRNN(nn.Module):\n    \"\"\"RNN for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param rnn_params: the parameters for the RNN network.\n    \"\"\"\n\n    def __init__(self, history_length: int, horizon: int, rnn_params: TSRNNParams):\n        super().__init__()\n        self.rnn_params = rnn_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.rnn = nn.RNN(\n            input_size=self.rnn_params.input_size,\n            hidden_size=self.rnn_params.hidden_size,\n            num_layers=self.rnn_params.num_layers,\n            batch_first=True,\n        )\n\n        self.regulate_output = nn.Linear(self.rnn_params.hidden_size, self.horizon)\n\n    @property\n    def rnn_config(self):\n        return dataclasses.asdict(self.rnn_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # x = self.regulate_input(x)\n        x, _ = self.rnn(x)\n\n        return self.regulate_output(x)\n</pre> @dataclasses.dataclass class TSRNNParams:     \"\"\"A dataclass to be served as our parameters     for the model.      :param hidden_size: number of dimensions in the hidden state     :param input_size: input dim     :param num_layers: number of units stacked     \"\"\"      hidden_size: int     input_size: int = 1     num_layers: int = 1   class TSRNN(nn.Module):     \"\"\"RNN for univaraite time series modeling.      :param history_length: the length of the input history.     :param horizon: the number of steps to be forecasted.     :param rnn_params: the parameters for the RNN network.     \"\"\"      def __init__(self, history_length: int, horizon: int, rnn_params: TSRNNParams):         super().__init__()         self.rnn_params = rnn_params         self.history_length = history_length         self.horizon = horizon          self.rnn = nn.RNN(             input_size=self.rnn_params.input_size,             hidden_size=self.rnn_params.hidden_size,             num_layers=self.rnn_params.num_layers,             batch_first=True,         )          self.regulate_output = nn.Linear(self.rnn_params.hidden_size, self.horizon)      @property     def rnn_config(self):         return dataclasses.asdict(self.rnn_params)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         # x = self.regulate_input(x)         x, _ = self.rnn(x)          return self.regulate_output(x) <p>We use lightning to train our model.</p> In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\n</pre> history_length_1_step = 100 horizon_1_step = 1 <p>We will build a few utilities</p> <ol> <li>To be able to feed the data into our model, we build a class (<code>DataFrameDataset</code>) that converts the pandas dataframe into a Dataset for pytorch.</li> <li>To make the lightning training code simpler, we will build a LightningDataModule (<code>DataFrameDataModule</code>) and a LightningModule (<code>RNNForecaster</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>class RNNForecaster(L.LightningModule):\n    def __init__(self, rnn: nn.Module):\n        super().__init__()\n        self.rnn = rnn\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n\n        loss = nn.functional.l1_loss(y_hat, y)\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n\n        loss = nn.functional.l1_loss(y_hat, y)\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.squeeze().type(self.dtype)\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.rnn(x)\n        return x, y_hat\n\n    def forward(self, x):\n        x = x.squeeze().type(self.dtype)\n        return x, self.rnn(x)\n</pre> class RNNForecaster(L.LightningModule):     def __init__(self, rnn: nn.Module):         super().__init__()         self.rnn = rnn      def configure_optimizers(self):         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)         return optimizer      def training_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)          loss = nn.functional.l1_loss(y_hat, y)         self.log_dict({\"train_loss\": loss}, prog_bar=True)         return loss      def validation_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)          loss = nn.functional.l1_loss(y_hat, y)         self.log_dict({\"val_loss\": loss}, prog_bar=True)         return loss      def predict_step(self, batch, batch_idx):         x, y = batch         x = x.squeeze().type(self.dtype)         y = y.squeeze(-1).type(self.dtype)          y_hat = self.rnn(x)         return x, y_hat      def forward(self, x):         x = x.squeeze().type(self.dtype)         return x, self.rnn(x) In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = DataFrameDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    dataframe=df[[\"y\"]],\n)\n</pre> pdm_1_step = DataFrameDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     dataframe=df[[\"y\"]], ) In\u00a0[\u00a0]: Copied! <pre>ts_rnn_params_1_step = TSRNNParams(input_size=1, hidden_size=64, num_layers=1)\n\nts_rnn_1_step = TSRNN(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    rnn_params=ts_rnn_params_1_step,\n)\n\nts_rnn_1_step\n</pre> ts_rnn_params_1_step = TSRNNParams(input_size=1, hidden_size=64, num_layers=1)  ts_rnn_1_step = TSRNN(     history_length=history_length_1_step,     horizon=horizon_1_step,     rnn_params=ts_rnn_params_1_step, )  ts_rnn_1_step In\u00a0[\u00a0]: Copied! <pre>rnn_forecaster_1_step = RNNForecaster(rnn=ts_rnn_1_step)\n</pre> rnn_forecaster_1_step = RNNForecaster(rnn=ts_rnn_1_step) In\u00a0[\u00a0]: Copied! <pre>logger_1_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"rnn_ts_1_step\"\n)\n\ntrainer_1_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)\n    ],\n    logger=logger_1_step,\n)\n</pre> logger_1_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"rnn_ts_1_step\" )  trainer_1_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)     ],     logger=logger_1_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=rnn_forecaster_1_step, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=rnn_forecaster_1_step, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=rnn_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=rnn_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = L.Trainer(precision=\"64\")  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader())\n</pre> evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>pd.merge(\n    evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()),\n    evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()),\n    how=\"inner\",\n    left_index=True,\n    right_index=True,\n    suffixes=[\"_rnn\", \"_last_obs\"],\n)\n</pre> pd.merge(     evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()),     evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()),     how=\"inner\",     left_index=True,     right_index=True,     suffixes=[\"_rnn\", \"_last_obs\"], ) In\u00a0[\u00a0]: Copied! <pre>df_ap = pd.read_csv(\n    \"https://raw.githubusercontent.com/DataHerb/dataset-airpassenger/main/dataset/AirPassengers.csv\"\n)\n</pre> df_ap = pd.read_csv(     \"https://raw.githubusercontent.com/DataHerb/dataset-airpassenger/main/dataset/AirPassengers.csv\" )"}, {"location": "notebooks/rnn_timeseries_comparison/#comparing-rnn-models-for-time-series-forecasting", "title": "Comparing RNN Models for Time Series Forecasting\u00b6", "text": "<p>In this notebook, we compare different RNN models for time series forecasting.</p>"}, {"location": "notebooks/rnn_timeseries_comparison/#data", "title": "Data\u00b6", "text": "<p>We prepare a toy dataset using <code>sin</code></p>"}, {"location": "notebooks/rnn_timeseries_comparison/#model", "title": "Model\u00b6", "text": "<p>In this section, we create the RNN model.</p>"}, {"location": "notebooks/rnn_timeseries_comparison/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#training-utilities", "title": "Training Utilities\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#data-model-and-training", "title": "Data, Model and Training\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#datamodule", "title": "DataModule\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#lightningmodule", "title": "LightningModule\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#trainer", "title": "Trainer\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#fitting", "title": "Fitting\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#retrieving-predictions", "title": "Retrieving Predictions\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/rnn_timeseries_comparison/#real-life", "title": "Real Life\u00b6", "text": "<p>We take the Airpassenger dataset.</p>"}, {"location": "notebooks/tabpfn/", "title": "TabPFN as Forecaster", "text": "In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom ts_dl_utils.datasets.pendulum import Pendulum\n</pre> import pandas as pd from ts_dl_utils.datasets.pendulum import Pendulum In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\n</pre> pen = Pendulum(length=100) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(300, 30, initial_angle=1, beta=0.00001))\n</pre> df = pd.DataFrame(pen(300, 30, initial_angle=1, beta=0.00001)) In\u00a0[\u00a0]: Copied! <pre>def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:\n    \"\"\"embed time series into a time delay embedding space\n\n    Time column `t` is required in the input data frame.\n\n    :param df: original time series data frame\n    :param window_size: window size for the time delay embedding\n    \"\"\"\n    dfs_embedded = []\n\n    for i in df.rolling(window_size):\n        i_t = i.t.iloc[0]\n        dfs_embedded.append(\n            pd.DataFrame(i.reset_index(drop=True))\n            .drop(columns=[\"t\"])\n            .T.reset_index(drop=True)\n            # .rename(columns={\"index\": \"name\"})\n            # .assign(t=i_t)\n        )\n\n    df_embedded = pd.concat(dfs_embedded[window_size - 1 :])\n\n    return df_embedded\n</pre> def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:     \"\"\"embed time series into a time delay embedding space      Time column `t` is required in the input data frame.      :param df: original time series data frame     :param window_size: window size for the time delay embedding     \"\"\"     dfs_embedded = []      for i in df.rolling(window_size):         i_t = i.t.iloc[0]         dfs_embedded.append(             pd.DataFrame(i.reset_index(drop=True))             .drop(columns=[\"t\"])             .T.reset_index(drop=True)             # .rename(columns={\"index\": \"name\"})             # .assign(t=i_t)         )      df_embedded = pd.concat(dfs_embedded[window_size - 1 :])      return df_embedded In\u00a0[\u00a0]: Copied! <pre>df = time_delay_embed(df, 11)\n</pre> df = time_delay_embed(df, 11)"}, {"location": "notebooks/tabpfn/#tabpfn-as-forecaster", "title": "TabPFN as Forecaster\u00b6", "text": ""}, {"location": "notebooks/time-series-data-generation/", "title": "Time Series Data Generation", "text": "In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport plotly.express as px\n</pre> import numpy as np import pandas as pd import plotly.express as px In\u00a0[\u00a0]: Copied! <pre>def profile_sin(t: np.ndarray, lambda_min: float, lambda_max: float) -&gt; np.ndarray:\n    \"\"\"generate a sin wave profile for\n    the expected number of visitors\n    in every 10min for each hour during a day\n\n    :param t: time in minutes\n    :param lambda_min: minimum number of visitors\n    :param lambda_max: maximum number of visitors\n    \"\"\"\n    amplitude = lambda_max - lambda_min\n    t_rescaled = (t - t.min()) / t.max() * np.pi\n\n    return amplitude * np.sin(t_rescaled) + lambda_min\n\n\nclass KioskVisitors:\n    \"\"\"generate number of visitors for a kiosk store\n\n    :param daily_profile: expectations of visitors\n        in every 10min for each hour during a day\n    \"\"\"\n\n    def __init__(self, daily_profile: np.ndarray):\n        self.daily_profile = daily_profile\n        self.daily_segments = len(daily_profile)\n\n    def __call__(self, n_days: int) -&gt; pd.DataFrame:\n        \"\"\"generate number of visitors for n_days\n\n        :param n_days: number of days to generate visitors\n        \"\"\"\n        visitors = np.concatenate(\n            [np.random.poisson(self.daily_profile) for _ in range(n_days)]\n        )\n\n        df = pd.DataFrame(\n            {\n                \"visitors\": visitors,\n                \"time\": np.arange(len(visitors)),\n                \"expectation\": np.tile(self.daily_profile, n_days),\n            }\n        )\n\n        return df\n</pre> def profile_sin(t: np.ndarray, lambda_min: float, lambda_max: float) -&gt; np.ndarray:     \"\"\"generate a sin wave profile for     the expected number of visitors     in every 10min for each hour during a day      :param t: time in minutes     :param lambda_min: minimum number of visitors     :param lambda_max: maximum number of visitors     \"\"\"     amplitude = lambda_max - lambda_min     t_rescaled = (t - t.min()) / t.max() * np.pi      return amplitude * np.sin(t_rescaled) + lambda_min   class KioskVisitors:     \"\"\"generate number of visitors for a kiosk store      :param daily_profile: expectations of visitors         in every 10min for each hour during a day     \"\"\"      def __init__(self, daily_profile: np.ndarray):         self.daily_profile = daily_profile         self.daily_segments = len(daily_profile)      def __call__(self, n_days: int) -&gt; pd.DataFrame:         \"\"\"generate number of visitors for n_days          :param n_days: number of days to generate visitors         \"\"\"         visitors = np.concatenate(             [np.random.poisson(self.daily_profile) for _ in range(n_days)]         )          df = pd.DataFrame(             {                 \"visitors\": visitors,                 \"time\": np.arange(len(visitors)),                 \"expectation\": np.tile(self.daily_profile, n_days),             }         )          return df <p>Create a sin profile</p> In\u00a0[\u00a0]: Copied! <pre>t = np.arange(0, 12 * 60 / 5, 1)\n\ndaily_profile = profile_sin(t, lambda_min=0.5, lambda_max=10)\n</pre> t = np.arange(0, 12 * 60 / 5, 1)  daily_profile = profile_sin(t, lambda_min=0.5, lambda_max=10) <p>Generate a time series data representing the number of visitors to a Kiosk.</p> In\u00a0[\u00a0]: Copied! <pre>kiosk_visitors = KioskVisitors(daily_profile=daily_profile)\n</pre> kiosk_visitors = KioskVisitors(daily_profile=daily_profile) In\u00a0[\u00a0]: Copied! <pre>df_visitors = kiosk_visitors(n_days=10)\n</pre> df_visitors = kiosk_visitors(n_days=10) In\u00a0[\u00a0]: Copied! <pre>px.line(\n    df_visitors,\n    x=\"time\",\n    y=[\"visitors\", \"expectation\"],\n)\n</pre> px.line(     df_visitors,     x=\"time\",     y=[\"visitors\", \"expectation\"], )"}, {"location": "notebooks/time-series-data-generation/#time-series-data-generation", "title": "Time Series Data Generation\u00b6", "text": ""}, {"location": "notebooks/time_series_data_and_embedding/", "title": "Time Series Data and Embeddings", "text": "In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import numpy as np In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport plotly.express as px\n</pre> import pandas as pd import plotly.express as px  In\u00a0[\u00a0]: Copied! <pre>def plot_arrow_chart(\n    dataframe: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    ax: plt.Axes,\n    color: str = \"k\",\n    alpha: float = 0.7,\n    marker: str = \".\",\n    linestyle: str = \"-\",\n    arrow_head_width: int = 4000,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot an arrow chart for the 'Total' and its lagged values\n    within a specified date range.\n    \"\"\"\n\n    x = dataframe[x_col].values\n    y = dataframe[y_col].values\n\n    ax.plot(x, y, marker=marker, linestyle=linestyle, color=color, alpha=alpha)\n\n    step = max(1, len(x) // 100)\n    for i in range(0, len(x) - 1, step):\n        ax.arrow(\n            x[i],\n            y[i],\n            x[i + 1] - x[i],\n            y[i + 1] - y[i],\n            shape=\"full\",\n            lw=0,\n            length_includes_head=True,\n            head_width=arrow_head_width,\n            color=color,\n            alpha=alpha,\n        )\n\n    return ax\n</pre> def plot_arrow_chart(     dataframe: pd.DataFrame,     x_col: str,     y_col: str,     ax: plt.Axes,     color: str = \"k\",     alpha: float = 0.7,     marker: str = \".\",     linestyle: str = \"-\",     arrow_head_width: int = 4000, ) -&gt; plt.Axes:     \"\"\"     Plot an arrow chart for the 'Total' and its lagged values     within a specified date range.     \"\"\"      x = dataframe[x_col].values     y = dataframe[y_col].values      ax.plot(x, y, marker=marker, linestyle=linestyle, color=color, alpha=alpha)      step = max(1, len(x) // 100)     for i in range(0, len(x) - 1, step):         ax.arrow(             x[i],             y[i],             x[i + 1] - x[i],             y[i + 1] - y[i],             shape=\"full\",             lw=0,             length_includes_head=True,             head_width=arrow_head_width,             color=color,             alpha=alpha,         )      return ax In\u00a0[\u00a0]: Copied! <pre>from ts_dl_utils.datasets.pendulum import Pendulum\n</pre> from ts_dl_utils.datasets.pendulum import Pendulum In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=20)\n\ndf_pen = pd.DataFrame(pen(3, 100, initial_angle=1, beta=0.01))\n\ndf_pen[\"theta_1\"] = df_pen[\"theta\"].shift()\ndf_pen[\"theta_diff\"] = df_pen[\"theta\"].diff()\n\ndf_pen\n</pre> pen = Pendulum(length=20)  df_pen = pd.DataFrame(pen(3, 100, initial_angle=1, beta=0.01))  df_pen[\"theta_1\"] = df_pen[\"theta\"].shift() df_pen[\"theta_diff\"] = df_pen[\"theta\"].diff()  df_pen In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(10, 8), layout=\"constrained\")\nspec = fig.add_gridspec(2, 2)\n\nax0 = fig.add_subplot(spec[0, :])\nax10 = fig.add_subplot(spec[1, 0])\nax11 = fig.add_subplot(spec[1, 1])\n\nax0.plot(\n    df_pen.t,\n    df_pen.theta,\n    marker=\".\",\n    linestyle=\"-\",\n    color=\"k\",\n)\n# Make x-ticks readable\nax0.xaxis.set_major_locator(plt.MaxNLocator(8))\n# fig.autofmt_xdate(rotation=30)\nax0.set_title(\"Swing Angle\")\n\n\nax10 = plot_arrow_chart(\n    df_pen, x_col=\"theta\", y_col=\"theta_1\", ax=ax10, arrow_head_width=0.00001\n)\nax10.set_xlabel(\"Swing Angle\")\nax10.set_ylabel(\"Swing Angle 0.05 seconds ago\")\nax10.set_title(\"Swing Angle and Angle 0.05 seconds ago\")\n\n\nax11 = plot_arrow_chart(\n    df_pen, x_col=\"theta\", y_col=\"theta_diff\", ax=ax11, arrow_head_width=0.00001\n)\n\nax11.set_xlabel(\"Swing Angle\")\nax11.set_ylabel(\"Swing Angle Change Rate\")\nax11.set_title(\"Phase Portrait\")\n\nplt.tight_layout()\n</pre> fig = plt.figure(figsize=(10, 8), layout=\"constrained\") spec = fig.add_gridspec(2, 2)  ax0 = fig.add_subplot(spec[0, :]) ax10 = fig.add_subplot(spec[1, 0]) ax11 = fig.add_subplot(spec[1, 1])  ax0.plot(     df_pen.t,     df_pen.theta,     marker=\".\",     linestyle=\"-\",     color=\"k\", ) # Make x-ticks readable ax0.xaxis.set_major_locator(plt.MaxNLocator(8)) # fig.autofmt_xdate(rotation=30) ax0.set_title(\"Swing Angle\")   ax10 = plot_arrow_chart(     df_pen, x_col=\"theta\", y_col=\"theta_1\", ax=ax10, arrow_head_width=0.00001 ) ax10.set_xlabel(\"Swing Angle\") ax10.set_ylabel(\"Swing Angle 0.05 seconds ago\") ax10.set_title(\"Swing Angle and Angle 0.05 seconds ago\")   ax11 = plot_arrow_chart(     df_pen, x_col=\"theta\", y_col=\"theta_diff\", ax=ax11, arrow_head_width=0.00001 )  ax11.set_xlabel(\"Swing Angle\") ax11.set_ylabel(\"Swing Angle Change Rate\") ax11.set_title(\"Phase Portrait\")  plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>df_ecdc_covid = pd.read_csv(\n    \"https://gist.githubusercontent.com/emptymalei/\"\n    \"90869e811b4aa118a7d28a5944587a64/raw\"\n    \"/1534670c8a3859ab3a6ae8e9ead6795248a3e664\"\n    \"/ecdc%2520covid%252019%2520data\"\n)\n</pre> df_ecdc_covid = pd.read_csv(     \"https://gist.githubusercontent.com/emptymalei/\"     \"90869e811b4aa118a7d28a5944587a64/raw\"     \"/1534670c8a3859ab3a6ae8e9ead6795248a3e664\"     \"/ecdc%2520covid%252019%2520data\" ) In\u00a0[\u00a0]: Copied! <pre>px.line(df_ecdc_covid, x=\"datetime\", y=\"Total\")\n</pre> px.line(df_ecdc_covid, x=\"datetime\", y=\"Total\") In\u00a0[\u00a0]: Copied! <pre>df_ecdc_covid\n</pre> df_ecdc_covid In\u00a0[\u00a0]: Copied! <pre>df_ecdc_covid_arrow_chart = df_ecdc_covid.loc[\n    pd.to_datetime(df_ecdc_covid.datetime).between(\"2020-08-01\", \"2020-12-01\")\n].copy()\n\ndf_ecdc_covid_arrow_chart[\"Total_1\"] = df_ecdc_covid_arrow_chart[\"Total\"].shift()\ndf_ecdc_covid_arrow_chart[\"Total_diff\"] = df_ecdc_covid_arrow_chart[\"Total\"].diff()\n</pre> df_ecdc_covid_arrow_chart = df_ecdc_covid.loc[     pd.to_datetime(df_ecdc_covid.datetime).between(\"2020-08-01\", \"2020-12-01\") ].copy()  df_ecdc_covid_arrow_chart[\"Total_1\"] = df_ecdc_covid_arrow_chart[\"Total\"].shift() df_ecdc_covid_arrow_chart[\"Total_diff\"] = df_ecdc_covid_arrow_chart[\"Total\"].diff() In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(10, 8), layout=\"constrained\")\nspec = fig.add_gridspec(2, 2)\n\nax0 = fig.add_subplot(spec[0, :])\nax10 = fig.add_subplot(spec[1, 0])\nax11 = fig.add_subplot(spec[1, 1])\n\nax0.plot(\n    df_ecdc_covid_arrow_chart.datetime,\n    df_ecdc_covid_arrow_chart.Total,\n    marker=\".\",\n    linestyle=\"-\",\n    color=\"k\",\n)\n# Make x-ticks readable\nax0.xaxis.set_major_locator(plt.MaxNLocator(8))\n# fig.autofmt_xdate(rotation=30)\nax0.set_title(\"Covid Cases in EU Over Time\")\n\n\nax10 = plot_arrow_chart(\n    df_ecdc_covid_arrow_chart, x_col=\"Total\", y_col=\"Total_1\", ax=ax10\n)\nax10.set_xlabel(\"Total Cases\")\nax10.set_ylabel(\"Total Cases Lagged by 1 Day\")\nax10.set_title(\"Covid Cases and Lagged Values\")\n\n\nax11 = plot_arrow_chart(\n    df_ecdc_covid_arrow_chart, x_col=\"Total\", y_col=\"Total_diff\", ax=ax11\n)\n\nax11.set_xlabel(\"Total Cases\")\nax11.set_ylabel(\"Total Cases Change\")\nax11.set_title(\"Covid Cases in EU Phase Portrait\")\nax11.set_ylim(-100_000, 100_000)\n\nplt.tight_layout()\n</pre> fig = plt.figure(figsize=(10, 8), layout=\"constrained\") spec = fig.add_gridspec(2, 2)  ax0 = fig.add_subplot(spec[0, :]) ax10 = fig.add_subplot(spec[1, 0]) ax11 = fig.add_subplot(spec[1, 1])  ax0.plot(     df_ecdc_covid_arrow_chart.datetime,     df_ecdc_covid_arrow_chart.Total,     marker=\".\",     linestyle=\"-\",     color=\"k\", ) # Make x-ticks readable ax0.xaxis.set_major_locator(plt.MaxNLocator(8)) # fig.autofmt_xdate(rotation=30) ax0.set_title(\"Covid Cases in EU Over Time\")   ax10 = plot_arrow_chart(     df_ecdc_covid_arrow_chart, x_col=\"Total\", y_col=\"Total_1\", ax=ax10 ) ax10.set_xlabel(\"Total Cases\") ax10.set_ylabel(\"Total Cases Lagged by 1 Day\") ax10.set_title(\"Covid Cases and Lagged Values\")   ax11 = plot_arrow_chart(     df_ecdc_covid_arrow_chart, x_col=\"Total\", y_col=\"Total_diff\", ax=ax11 )  ax11.set_xlabel(\"Total Cases\") ax11.set_ylabel(\"Total Cases Change\") ax11.set_title(\"Covid Cases in EU Phase Portrait\") ax11.set_ylim(-100_000, 100_000)  plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>df_walmart = pd.read_csv(\n    \"https://raw.githubusercontent.com/datumorphism/\"\n    \"dataset-m5-simplified/refs/heads/main/dataset/\"\n    \"m5_store_sales.csv\"\n)\n</pre> df_walmart = pd.read_csv(     \"https://raw.githubusercontent.com/datumorphism/\"     \"dataset-m5-simplified/refs/heads/main/dataset/\"     \"m5_store_sales.csv\" ) In\u00a0[\u00a0]: Copied! <pre>df_walmart\n</pre> df_walmart In\u00a0[\u00a0]: Copied! <pre>px.line(df_walmart, x=\"date\", y=\"CA\")\n</pre> px.line(df_walmart, x=\"date\", y=\"CA\") In\u00a0[\u00a0]: Copied! <pre>df_walmart_total = df_walmart[[\"date\", \"CA\", \"TX\", \"WI\"]].copy()\n</pre> df_walmart_total = df_walmart[[\"date\", \"CA\", \"TX\", \"WI\"]].copy() In\u00a0[\u00a0]: Copied! <pre>df_walmart_total[\"total\"] = (\n    df_walmart_total.CA + df_walmart_total.TX + df_walmart_total.WI\n)\n\ndf_walmart_total[\"datetime\"] = pd.to_datetime(df_walmart_total.date, format=\"%Y-%m-%d\")\ndf_walmart_total[\"timestamp\"] = df_walmart_total.datetime.astype(int) // 10**9\n</pre> df_walmart_total[\"total\"] = (     df_walmart_total.CA + df_walmart_total.TX + df_walmart_total.WI )  df_walmart_total[\"datetime\"] = pd.to_datetime(df_walmart_total.date, format=\"%Y-%m-%d\") df_walmart_total[\"timestamp\"] = df_walmart_total.datetime.astype(int) // 10**9 In\u00a0[\u00a0]: Copied! <pre>df_walmart_total[\"total_1\"] = df_walmart_total.total.shift()\ndf_walmart_total[\"total_diff\"] = df_walmart_total.total.diff()\n</pre> df_walmart_total[\"total_1\"] = df_walmart_total.total.shift() df_walmart_total[\"total_diff\"] = df_walmart_total.total.diff() In\u00a0[\u00a0]: Copied! <pre>px.scatter(\n    df_walmart_total.loc[pd.to_datetime(df_walmart_total.date).dt.year == 2016],\n    x=\"total\",\n    y=\"total_1\",\n    color=\"timestamp\",\n)\n</pre> px.scatter(     df_walmart_total.loc[pd.to_datetime(df_walmart_total.date).dt.year == 2016],     x=\"total\",     y=\"total_1\",     color=\"timestamp\", ) In\u00a0[\u00a0]: Copied! <pre>df_walmart_arrow_chart = df_walmart_total.loc[\n    pd.to_datetime(df_walmart_total.date).between(\"2016-01-01\", \"2016-03-01\")\n].copy()\n</pre> df_walmart_arrow_chart = df_walmart_total.loc[     pd.to_datetime(df_walmart_total.date).between(\"2016-01-01\", \"2016-03-01\") ].copy() In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(10, 8), layout=\"constrained\")\nspec = fig.add_gridspec(2, 2)\n\nax0 = fig.add_subplot(spec[0, :])\nax10 = fig.add_subplot(spec[1, 0])\nax11 = fig.add_subplot(spec[1, 1])\n\nax0.plot(\n    df_walmart_arrow_chart.datetime,\n    df_walmart_arrow_chart.total,\n    marker=\".\",\n    linestyle=\"-\",\n    color=\"k\",\n)\n# Make x-ticks readable\nax0.xaxis.set_major_locator(plt.MaxNLocator(8))\n# fig.autofmt_xdate(rotation=30)\nax0.set_title(\"Walmart Sales Over Time\")\n\nax10 = plot_arrow_chart(\n    df_walmart_arrow_chart,\n    x_col=\"total\",\n    y_col=\"total_1\",\n    ax=ax10,\n    arrow_head_width=500,\n)\nax10.set_xlabel(\"Total Sales\")\nax10.set_ylabel(\"Total Sales Lagged by 1 Day\")\nax10.set_title(\"Walmart Sales and Lagged Sales\")\n\nax11 = plot_arrow_chart(\n    df_walmart_arrow_chart,\n    x_col=\"total\",\n    y_col=\"total_diff\",\n    ax=ax11,\n    arrow_head_width=500,\n)\nax11.set_xlabel(\"Total Sales\")\nax11.set_ylabel(\"Total Sales Change\")\nax11.set_title(\"Walmart Sales Phase Portrait\")\n\n\nplt.tight_layout()\n</pre> fig = plt.figure(figsize=(10, 8), layout=\"constrained\") spec = fig.add_gridspec(2, 2)  ax0 = fig.add_subplot(spec[0, :]) ax10 = fig.add_subplot(spec[1, 0]) ax11 = fig.add_subplot(spec[1, 1])  ax0.plot(     df_walmart_arrow_chart.datetime,     df_walmart_arrow_chart.total,     marker=\".\",     linestyle=\"-\",     color=\"k\", ) # Make x-ticks readable ax0.xaxis.set_major_locator(plt.MaxNLocator(8)) # fig.autofmt_xdate(rotation=30) ax0.set_title(\"Walmart Sales Over Time\")  ax10 = plot_arrow_chart(     df_walmart_arrow_chart,     x_col=\"total\",     y_col=\"total_1\",     ax=ax10,     arrow_head_width=500, ) ax10.set_xlabel(\"Total Sales\") ax10.set_ylabel(\"Total Sales Lagged by 1 Day\") ax10.set_title(\"Walmart Sales and Lagged Sales\")  ax11 = plot_arrow_chart(     df_walmart_arrow_chart,     x_col=\"total\",     y_col=\"total_diff\",     ax=ax11,     arrow_head_width=500, ) ax11.set_xlabel(\"Total Sales\") ax11.set_ylabel(\"Total Sales Change\") ax11.set_title(\"Walmart Sales Phase Portrait\")   plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>import io\nimport zipfile\n</pre> import io import zipfile In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>import requests\n\n# Download from remote URL\ndata_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\"\n\nr = requests.get(data_uri)\n\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall(\"tmp/data/uci_electricity/\")\n</pre> import requests  # Download from remote URL data_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\"  r = requests.get(data_uri)  z = zipfile.ZipFile(io.BytesIO(r.content)) z.extractall(\"tmp/data/uci_electricity/\")  In\u00a0[\u00a0]: Copied! <pre># Load as pandas dataframe\ndf_electricity = (\n    pd.read_csv(\"tmp/data/uci_electricity/LD2011_2014.txt\", delimiter=\";\", decimal=\",\")\n    .rename(columns={\"Unnamed: 0\": \"date\"})\n    .set_index(\"date\")\n)\ndf_electricity.index = pd.to_datetime(df_electricity.index)\n</pre> # Load as pandas dataframe df_electricity = (     pd.read_csv(\"tmp/data/uci_electricity/LD2011_2014.txt\", delimiter=\";\", decimal=\",\")     .rename(columns={\"Unnamed: 0\": \"date\"})     .set_index(\"date\") ) df_electricity.index = pd.to_datetime(df_electricity.index) In\u00a0[\u00a0]: Copied! <pre>df_electricity\n</pre> df_electricity In\u00a0[\u00a0]: Copied! <pre>df_electricity.loc[\n    (df_electricity.index &gt;= \"2012-01-01\") &amp; (df_electricity.index &lt; \"2012-02-01\")\n][[\"MT_001\"]].plot()\n</pre> df_electricity.loc[     (df_electricity.index &gt;= \"2012-01-01\") &amp; (df_electricity.index &lt; \"2012-02-01\") ][[\"MT_001\"]].plot()"}, {"location": "notebooks/time_series_data_and_embedding/#time-series-data-and-embeddings", "title": "Time Series Data and Embeddings\u00b6", "text": ""}, {"location": "notebooks/time_series_data_and_embedding/#pendulum", "title": "Pendulum\u00b6", "text": ""}, {"location": "notebooks/time_series_data_and_embedding/#covid", "title": "Covid\u00b6", "text": ""}, {"location": "notebooks/time_series_data_and_embedding/#walmart", "title": "Walmart\u00b6", "text": ""}, {"location": "notebooks/time_series_data_and_embedding/#electricity-data", "title": "Electricity Data\u00b6", "text": ""}, {"location": "notebooks/time_vae/", "title": "TimeVAE", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\nfrom typing import Dict, List, Tuple\n\nimport lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom loguru import logger\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom ts_dl_utils.datasets.pendulum import Pendulum\n</pre> from functools import cached_property from typing import Dict, List, Tuple  import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from loguru import logger from torch import nn from torch.utils.data import DataLoader, Dataset from ts_dl_utils.datasets.pendulum import Pendulum  In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\n</pre> pen = Pendulum(length=100) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(300, 30, initial_angle=1, beta=0.00001))\n</pre> df = pd.DataFrame(pen(300, 30, initial_angle=1, beta=0.00001)) In\u00a0[\u00a0]: Copied! <pre>df[\"theta\"] = df[\"theta\"] + 2\n</pre> df[\"theta\"] = df[\"theta\"] + 2 In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.head(100).plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.head(100).plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre>def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:\n    \"\"\"embed time series into a time delay embedding space\n\n    Time column `t` is required in the input data frame.\n\n    :param df: original time series data frame\n    :param window_size: window size for the time delay embedding\n    \"\"\"\n    dfs_embedded = []\n\n    for i in df.rolling(window_size):\n        i_t = i.t.iloc[0]\n        dfs_embedded.append(\n            pd.DataFrame(i.reset_index(drop=True))\n            .drop(columns=[\"t\"])\n            .T.reset_index(drop=True)\n            # .rename(columns={\"index\": \"name\"})\n            # .assign(t=i_t)\n        )\n\n    df_embedded = pd.concat(dfs_embedded[window_size - 1 :])\n\n    return df_embedded\n</pre> def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:     \"\"\"embed time series into a time delay embedding space      Time column `t` is required in the input data frame.      :param df: original time series data frame     :param window_size: window size for the time delay embedding     \"\"\"     dfs_embedded = []      for i in df.rolling(window_size):         i_t = i.t.iloc[0]         dfs_embedded.append(             pd.DataFrame(i.reset_index(drop=True))             .drop(columns=[\"t\"])             .T.reset_index(drop=True)             # .rename(columns={\"index\": \"name\"})             # .assign(t=i_t)         )      df_embedded = pd.concat(dfs_embedded[window_size - 1 :])      return df_embedded In\u00a0[\u00a0]: Copied! <pre>time_delay_embed(df, 3)\n</pre> time_delay_embed(df, 3) In\u00a0[\u00a0]: Copied! <pre>class TimeVAEDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param window_size: length of time series slicing chunks\n    \"\"\"\n\n    def __init__(\n        self,\n        dataframe: pd.DataFrame,\n        window_size: int,\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.window_size = window_size\n        self.dataframe_rows = len(self.dataframe)\n        self.length = self.dataframe_rows - self.window_size + 1\n\n    def moving_slicing(self, idx: int) -&gt; np.ndarray:\n        return self.dataframe[idx : self.window_size + idx].values\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [self.moving_slicing(i) for i in range(idx.start, idx.stop, step)]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</pre> class TimeVAEDataset(Dataset):     \"\"\"A dataset from a pandas dataframe.      For a given pandas dataframe, this generates a pytorch     compatible dataset by sliding in time dimension.      ```python     ds = DataFrameDataset(         dataframe=df, history_length=10, horizon=2     )     ```      :param dataframe: input dataframe with a DatetimeIndex.     :param window_size: length of time series slicing chunks     \"\"\"      def __init__(         self,         dataframe: pd.DataFrame,         window_size: int,     ):         super().__init__()         self.dataframe = dataframe         self.window_size = window_size         self.dataframe_rows = len(self.dataframe)         self.length = self.dataframe_rows - self.window_size + 1      def moving_slicing(self, idx: int) -&gt; np.ndarray:         return self.dataframe[idx : self.window_size + idx].values      def _validate_dataframe(self) -&gt; None:         \"\"\"Validate the input dataframe.          - We require the dataframe index to be DatetimeIndex.         - This dataset is null aversion.         - Dataframe index should be sorted.         \"\"\"          if not isinstance(             self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex         ):             raise TypeError(                 \"Type of the dataframe index is not DatetimeIndex\"                 f\": {type(self.dataframe.index)}\"             )          has_na = self.dataframe.isnull().values.any()          if has_na:             logger.warning(\"Dataframe has null\")          has_index_sorted = self.dataframe.index.equals(             self.dataframe.index.sort_values()         )          if not has_index_sorted:             logger.warning(\"Dataframe index is not sorted\")      def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:         if isinstance(idx, slice):             if (idx.start &lt; 0) or (idx.stop &gt;= self.length):                 raise IndexError(f\"Slice out of range: {idx}\")             step = idx.step if idx.step is not None else 1             return [self.moving_slicing(i) for i in range(idx.start, idx.stop, step)]         else:             if idx &gt;= self.length:                 raise IndexError(\"End of dataset\")             return self.moving_slicing(idx)      def __len__(self) -&gt; int:         return self.length In\u00a0[\u00a0]: Copied! <pre>class TimeVAEDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule for Time Series VAE.\n\n    This data module takes a pandas dataframe and generates\n    the corresponding dataloaders for training, validation and\n    testing.\n\n    ```python\n    time_vae_dm_example = TimeVAEDataModule(\n        window_size=30, dataframe=df[[\"theta\"]], batch_size=32\n    )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int,\n        dataframe: pd.DataFrame,\n        test_fraction: float = 0.3,\n        val_fraction: float = 0.1,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.batch_size = batch_size\n        self.dataframe = dataframe\n        self.test_fraction = test_fraction\n        self.val_fraction = val_fraction\n        self.num_workers = num_workers\n\n        self.train_dataset, self.val_dataset = self.split_train_val(\n            self.train_val_dataset\n        )\n\n    @cached_property\n    def df_length(self):\n        return len(self.dataframe)\n\n    @cached_property\n    def df_test_length(self):\n        return int(self.df_length * self.test_fraction)\n\n    @cached_property\n    def df_train_val_length(self):\n        return self.df_length - self.df_test_length\n\n    @cached_property\n    def train_val_dataframe(self):\n        return self.dataframe.iloc[: self.df_train_val_length]\n\n    @cached_property\n    def test_dataframe(self):\n        return self.dataframe.iloc[self.df_train_val_length :]\n\n    @cached_property\n    def train_val_dataset(self):\n        return TimeVAEDataset(\n            dataframe=self.train_val_dataframe,\n            window_size=self.window_size,\n        )\n\n    @cached_property\n    def test_dataset(self):\n        return TimeVAEDataset(\n            dataframe=self.test_dataframe,\n            window_size=self.window_size,\n        )\n\n    def split_train_val(self, dataset: Dataset):\n        return torch.utils.data.random_split(\n            dataset, [1 - self.val_fraction, self.val_fraction]\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            persistent_workers=(True if self.num_workers &gt; 0 else False),\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            persistent_workers=(True if self.num_workers &gt; 0 else False),\n        )\n\n    def predict_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False\n        )\n</pre> class TimeVAEDataModule(L.LightningDataModule):     \"\"\"Lightning DataModule for Time Series VAE.      This data module takes a pandas dataframe and generates     the corresponding dataloaders for training, validation and     testing.      ```python     time_vae_dm_example = TimeVAEDataModule(         window_size=30, dataframe=df[[\"theta\"]], batch_size=32     )     ```     \"\"\"      def __init__(         self,         window_size: int,         dataframe: pd.DataFrame,         test_fraction: float = 0.3,         val_fraction: float = 0.1,         batch_size: int = 32,         num_workers: int = 0,     ):         super().__init__()         self.window_size = window_size         self.batch_size = batch_size         self.dataframe = dataframe         self.test_fraction = test_fraction         self.val_fraction = val_fraction         self.num_workers = num_workers          self.train_dataset, self.val_dataset = self.split_train_val(             self.train_val_dataset         )      @cached_property     def df_length(self):         return len(self.dataframe)      @cached_property     def df_test_length(self):         return int(self.df_length * self.test_fraction)      @cached_property     def df_train_val_length(self):         return self.df_length - self.df_test_length      @cached_property     def train_val_dataframe(self):         return self.dataframe.iloc[: self.df_train_val_length]      @cached_property     def test_dataframe(self):         return self.dataframe.iloc[self.df_train_val_length :]      @cached_property     def train_val_dataset(self):         return TimeVAEDataset(             dataframe=self.train_val_dataframe,             window_size=self.window_size,         )      @cached_property     def test_dataset(self):         return TimeVAEDataset(             dataframe=self.test_dataframe,             window_size=self.window_size,         )      def split_train_val(self, dataset: Dataset):         return torch.utils.data.random_split(             dataset, [1 - self.val_fraction, self.val_fraction]         )      def train_dataloader(self):         return DataLoader(             dataset=self.train_dataset,             batch_size=self.batch_size,             shuffle=True,             num_workers=self.num_workers,             persistent_workers=(True if self.num_workers &gt; 0 else False),         )      def test_dataloader(self):         return DataLoader(             dataset=self.test_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,         )      def val_dataloader(self):         return DataLoader(             dataset=self.val_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,             persistent_workers=(True if self.num_workers &gt; 0 else False),         )      def predict_dataloader(self):         return DataLoader(             dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False         ) In\u00a0[\u00a0]: Copied! <pre>time_vae_dm_example = TimeVAEDataModule(\n    window_size=30, dataframe=df[[\"theta\"]], batch_size=32\n)\n</pre> time_vae_dm_example = TimeVAEDataModule(     window_size=30, dataframe=df[[\"theta\"]], batch_size=32 ) In\u00a0[\u00a0]: Copied! <pre>len(list(time_vae_dm_example.train_dataloader()))\n</pre> len(list(time_vae_dm_example.train_dataloader())) In\u00a0[\u00a0]: Copied! <pre>list(time_vae_dm_example.train_dataloader())[0].shape\n</pre> list(time_vae_dm_example.train_dataloader())[0].shape In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass VAEParams:\n    \"\"\"Parameters for VAEEncoder and VAEDecoder\n\n    :param hidden_layer_sizes: list of hidden layer sizes\n    :param latent_size: latent space dimension\n    :param sequence_length: input sequence length\n    :param n_features: number of features\n    \"\"\"\n\n    hidden_layer_sizes: List[int]\n    latent_size: int\n    sequence_length: int\n    n_features: int = 1\n\n    @cached_property\n    def data_size(self) -&gt; int:\n        \"\"\"The dimension of the input data\n        when flattened.\n        \"\"\"\n        return self.sequence_length * self.n_features\n\n    def asdict(self) -&gt; dict:\n        return dataclasses.asdict(self)\n</pre> @dataclasses.dataclass class VAEParams:     \"\"\"Parameters for VAEEncoder and VAEDecoder      :param hidden_layer_sizes: list of hidden layer sizes     :param latent_size: latent space dimension     :param sequence_length: input sequence length     :param n_features: number of features     \"\"\"      hidden_layer_sizes: List[int]     latent_size: int     sequence_length: int     n_features: int = 1      @cached_property     def data_size(self) -&gt; int:         \"\"\"The dimension of the input data         when flattened.         \"\"\"         return self.sequence_length * self.n_features      def asdict(self) -&gt; dict:         return dataclasses.asdict(self) In\u00a0[\u00a0]: Copied! <pre>class VAEMLPEncoder(nn.Module):\n    \"\"\"MLP Encoder of TimeVAE\"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n\n        encode_layer_sizes = [self.params.data_size] + self.params.hidden_layer_sizes\n        self.layers_used_to_encode = [\n            self._linear_block(size_in, size_out)\n            for size_in, size_out in zip(\n                encode_layer_sizes[:-1], encode_layer_sizes[1:]\n            )\n        ]\n        self.encode = nn.Sequential(*self.layers_used_to_encode)\n        encoded_size = self.params.hidden_layer_sizes[-1]\n        self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)\n        self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_size, _, _ = x.size()\n        x = x.transpose(1, 2)\n        x = self.encode(x)\n\n        z_mean = self.z_mean_layer(x)\n        z_log_var = self.z_log_var_layer(x)\n        epsilon = torch.randn(\n            batch_size, self.params.n_features, self.params.latent_size\n        ).type_as(x)\n        z = z_mean + torch.exp(0.5 * z_log_var) * epsilon\n\n        return z_mean, z_log_var, z\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])\n</pre> class VAEMLPEncoder(nn.Module):     \"\"\"MLP Encoder of TimeVAE\"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params          encode_layer_sizes = [self.params.data_size] + self.params.hidden_layer_sizes         self.layers_used_to_encode = [             self._linear_block(size_in, size_out)             for size_in, size_out in zip(                 encode_layer_sizes[:-1], encode_layer_sizes[1:]             )         ]         self.encode = nn.Sequential(*self.layers_used_to_encode)         encoded_size = self.params.hidden_layer_sizes[-1]         self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)         self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)      def forward(         self, x: torch.Tensor     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         batch_size, _, _ = x.size()         x = x.transpose(1, 2)         x = self.encode(x)          z_mean = self.z_mean_layer(x)         z_log_var = self.z_log_var_layer(x)         epsilon = torch.randn(             batch_size, self.params.n_features, self.params.latent_size         ).type_as(x)         z = z_mean + torch.exp(0.5 * z_log_var) * epsilon          return z_mean, z_log_var, z      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()]) In\u00a0[\u00a0]: Copied! <pre>class VAEEncoder(nn.Module):\n    \"\"\"Encoder of TimeVAE\n\n    ```python\n    encoder = VAEEncoder(\n        VAEParams(\n            hidden_layer_sizes=[40, 30],\n            latent_size=10,\n            sequence_length=50\n        )\n    )\n    ```\n\n    :param params: parameters for the encoder\n    \"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        encode_layer_sizes = [self.params.n_features] + self.params.hidden_layer_sizes\n        self.layers_used_to_encode = [\n            self._conv_block(size_in, size_out)\n            for size_in, size_out in zip(\n                encode_layer_sizes[:-1], encode_layer_sizes[1:]\n            )\n        ] + [nn.Flatten()]\n        self.encode = nn.Sequential(*self.layers_used_to_encode)\n        encoded_size = self.cal_conv1d_output_dim() * self.params.hidden_layer_sizes[-1]\n        self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)\n        self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_size, _, _ = x.size()\n        x = x.transpose(1, 2)\n        x = self.encode(x)\n\n        z_mean = self.z_mean_layer(x).view(\n            batch_size, self.params.n_features, self.params.latent_size\n        )\n        z_log_var = self.z_log_var_layer(x).view(\n            batch_size, self.params.n_features, self.params.latent_size\n        )\n        epsilon = torch.randn(\n            batch_size, self.params.n_features, self.params.latent_size\n        ).type_as(x)\n        z = z_mean + torch.exp(0.5 * z_log_var) * epsilon\n\n        return z_mean, z_log_var, z\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])\n\n    def _conv_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(\n            *[\n                nn.Conv1d(size_in, size_out, kernel_size=3, stride=2, padding=1),\n                nn.ReLU(),\n            ]\n        )\n\n    def cal_conv1d_output_dim(self) -&gt; int:\n        \"\"\"the output dimension of all the Conv1d layers\"\"\"\n        output_size = self.params.sequence_length * self.params.n_features\n\n        for l in self.layers_used_to_encode:\n            if l._get_name() == \"Conv1d\":\n                output_size = self._conv1d_output_dim(l, output_size)\n            elif l._get_name() == \"Sequential\":\n                for l2 in l:\n                    if l2._get_name() == \"Conv1d\":\n                        output_size = self._conv1d_output_dim(l2, output_size)\n\n        return output_size\n\n    def _conv1d_output_dim(self, layer: nn.Module, input_size: int) -&gt; int:\n        \"\"\"Formula to calculate\n        the output size of Conv1d layer\n        \"\"\"\n        return (\n            (input_size + 2 * layer.padding[0] - layer.kernel_size[0])\n            // layer.stride[0]\n        ) + 1\n</pre> class VAEEncoder(nn.Module):     \"\"\"Encoder of TimeVAE      ```python     encoder = VAEEncoder(         VAEParams(             hidden_layer_sizes=[40, 30],             latent_size=10,             sequence_length=50         )     )     ```      :param params: parameters for the encoder     \"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          encode_layer_sizes = [self.params.n_features] + self.params.hidden_layer_sizes         self.layers_used_to_encode = [             self._conv_block(size_in, size_out)             for size_in, size_out in zip(                 encode_layer_sizes[:-1], encode_layer_sizes[1:]             )         ] + [nn.Flatten()]         self.encode = nn.Sequential(*self.layers_used_to_encode)         encoded_size = self.cal_conv1d_output_dim() * self.params.hidden_layer_sizes[-1]         self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)         self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)      def forward(         self, x: torch.Tensor     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         batch_size, _, _ = x.size()         x = x.transpose(1, 2)         x = self.encode(x)          z_mean = self.z_mean_layer(x).view(             batch_size, self.params.n_features, self.params.latent_size         )         z_log_var = self.z_log_var_layer(x).view(             batch_size, self.params.n_features, self.params.latent_size         )         epsilon = torch.randn(             batch_size, self.params.n_features, self.params.latent_size         ).type_as(x)         z = z_mean + torch.exp(0.5 * z_log_var) * epsilon          return z_mean, z_log_var, z      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])      def _conv_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(             *[                 nn.Conv1d(size_in, size_out, kernel_size=3, stride=2, padding=1),                 nn.ReLU(),             ]         )      def cal_conv1d_output_dim(self) -&gt; int:         \"\"\"the output dimension of all the Conv1d layers\"\"\"         output_size = self.params.sequence_length * self.params.n_features          for l in self.layers_used_to_encode:             if l._get_name() == \"Conv1d\":                 output_size = self._conv1d_output_dim(l, output_size)             elif l._get_name() == \"Sequential\":                 for l2 in l:                     if l2._get_name() == \"Conv1d\":                         output_size = self._conv1d_output_dim(l2, output_size)          return output_size      def _conv1d_output_dim(self, layer: nn.Module, input_size: int) -&gt; int:         \"\"\"Formula to calculate         the output size of Conv1d layer         \"\"\"         return (             (input_size + 2 * layer.padding[0] - layer.kernel_size[0])             // layer.stride[0]         ) + 1 In\u00a0[\u00a0]: Copied! <pre>mlp_encoder = VAEMLPEncoder(\n    VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50)\n)\n\n[i.size() for i in mlp_encoder(torch.ones(32, 50, 1))], mlp_encoder(\n    torch.ones(32, 50, 1)\n)[-1]\n</pre> mlp_encoder = VAEMLPEncoder(     VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50) )  [i.size() for i in mlp_encoder(torch.ones(32, 50, 1))], mlp_encoder(     torch.ones(32, 50, 1) )[-1] In\u00a0[\u00a0]: Copied! <pre>encoder = VAEEncoder(\n    VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50)\n)\n\n[i.size() for i in encoder(torch.ones(32, 50, 1))], encoder(torch.ones(32, 50, 1))[-1]\n</pre> encoder = VAEEncoder(     VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50) )  [i.size() for i in encoder(torch.ones(32, 50, 1))], encoder(torch.ones(32, 50, 1))[-1] In\u00a0[\u00a0]: Copied! <pre>class VAEDecoder(nn.Module):\n    \"\"\"Decoder of TimeVAE\n\n    ```python\n    decoder = VAEDecoder(\n        VAEParams(\n            hidden_layer_sizes=[30, 40],\n            latent_size=10,\n            sequence_length=50,\n        )\n    )\n    ```\n\n    :param params: parameters for the decoder\n    \"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        decode_layer_sizes = (\n            [self.params.latent_size]\n            + self.params.hidden_layer_sizes\n            + [self.params.data_size]\n        )\n\n        self.decode = nn.Sequential(\n            *[\n                self._linear_block(size_in, size_out)\n                for size_in, size_out in zip(\n                    decode_layer_sizes[:-1], decode_layer_sizes[1:]\n                )\n            ]\n        )\n\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        output = self.decode(z)\n        return output.view(-1, self.params.sequence_length, self.params.n_features)\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        \"\"\"create linear block based on the specified sizes\"\"\"\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.Softplus()])\n</pre> class VAEDecoder(nn.Module):     \"\"\"Decoder of TimeVAE      ```python     decoder = VAEDecoder(         VAEParams(             hidden_layer_sizes=[30, 40],             latent_size=10,             sequence_length=50,         )     )     ```      :param params: parameters for the decoder     \"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          decode_layer_sizes = (             [self.params.latent_size]             + self.params.hidden_layer_sizes             + [self.params.data_size]         )          self.decode = nn.Sequential(             *[                 self._linear_block(size_in, size_out)                 for size_in, size_out in zip(                     decode_layer_sizes[:-1], decode_layer_sizes[1:]                 )             ]         )      def forward(self, z: torch.Tensor) -&gt; torch.Tensor:         output = self.decode(z)         return output.view(-1, self.params.sequence_length, self.params.n_features)      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         \"\"\"create linear block based on the specified sizes\"\"\"         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.Softplus()]) In\u00a0[\u00a0]: Copied! <pre>decoder = VAEDecoder(\n    VAEParams(hidden_layer_sizes=[30, 40], latent_size=10, sequence_length=50)\n)\n\ndecoder(torch.ones(32, 1, 10)).size()\n</pre> decoder = VAEDecoder(     VAEParams(hidden_layer_sizes=[30, 40], latent_size=10, sequence_length=50) )  decoder(torch.ones(32, 1, 10)).size() In\u00a0[\u00a0]: Copied! <pre>class VAE(nn.Module):\n    \"\"\"VAE model with encoder and decoder\n\n    :param encoder: encoder module\n    :param decoder: decoder module\n    \"\"\"\n\n    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.hparams = {\n            **{f\"encoder_{k}\": v for k, v in self.encoder.hparams.items()},\n            **{f\"decoder_{k}\": v for k, v in self.decoder.hparams.items()},\n        }\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        z_mean, z_log_var, z = self.encoder(x)\n        x_reconstructed = self.decoder(z)\n        return x_reconstructed, z_mean, z_log_var\n</pre> class VAE(nn.Module):     \"\"\"VAE model with encoder and decoder      :param encoder: encoder module     :param decoder: decoder module     \"\"\"      def __init__(self, encoder: nn.Module, decoder: nn.Module):         super().__init__()         self.encoder = encoder         self.decoder = decoder         self.hparams = {             **{f\"encoder_{k}\": v for k, v in self.encoder.hparams.items()},             **{f\"decoder_{k}\": v for k, v in self.decoder.hparams.items()},         }      def forward(         self, x: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         z_mean, z_log_var, z = self.encoder(x)         x_reconstructed = self.decoder(z)         return x_reconstructed, z_mean, z_log_var In\u00a0[\u00a0]: Copied! <pre>class VAEModel(L.LightningModule):\n    \"\"\"VAE model using VAEEncoder, VAEDecoder, and VAE\n\n    :param model: VAE model\n    :param reconstruction_weight: weight for the reconstruction loss\n    :param learning_rate: learning rate for the optimizer\n    :param scheduler_max_epochs: maximum epochs for the scheduler\n    \"\"\"\n\n    def __init__(\n        self,\n        model: VAE,\n        reconstruction_weight: float = 1.0,\n        learning_rate: float = 1e-3,\n        scheduler_max_epochs: int = 10000,\n    ):\n        super().__init__()\n        self.model = model\n        self.reconstruction_weight = reconstruction_weight\n        self.learning_rate = learning_rate\n        self.scheduler_max_epochs = scheduler_max_epochs\n\n        self.hparams.update(\n            {\n                **model.hparams,\n                **{\n                    \"reconstruction_weight\": reconstruction_weight,\n                    \"learning_rate\": learning_rate,\n                    \"scheduler_max_epochs\": scheduler_max_epochs,\n                },\n            }\n        )\n        self.save_hyperparameters(self.hparams)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n\n        self.log_dict(\n            {\n                \"train_loss_total\": loss_total,\n                \"train_loss_reconstruction\": loss_reconstruction,\n                \"train_loss_kl\": loss_kl,\n            }\n        )\n\n        return loss_total\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n        self.log_dict(\n            {\n                \"val_loss_total\": loss_total,\n                \"val_loss_reconstruction\": loss_reconstruction,\n                \"val_loss_kl\": loss_kl,\n            }\n        )\n\n        return loss_total\n\n    def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n        self.log_dict(\n            {\n                \"test_loss_total\": loss_total,\n                \"test_loss_reconstruction\": loss_reconstruction,\n                \"test_loss_kl\": loss_kl,\n            }\n        )\n        return loss_total\n\n    def loss(\n        self,\n        x: torch.Tensor,\n        x_reconstructed: torch.Tensor,\n        z_log_var: torch.Tensor,\n        z_mean: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        loss_reconstruction = self.reconstruction_loss(x, x_reconstructed)\n        loss_kl = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - z_log_var.exp())\n        loss_total = self.reconstruction_weight * loss_reconstruction + loss_kl\n\n        return (\n            loss_total / x.size(0),\n            loss_reconstruction / x.size(0),\n            loss_kl / x.size(0),\n        )\n\n    def reconstruction_loss(\n        self, x: torch.Tensor, x_reconstructed: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstruction loss for VAE.\n\n        $$\n        \\sum_{i=1}^{N} (x_i - x_{reconstructed_i})^2\n        + \\sum_{i=1}^{N} (\\mu_i - \\mu_{reconstructed_i})^2\n        $$\n        \"\"\"\n        loss = torch.sum((x - x_reconstructed) ** 2) + torch.sum(\n            (torch.mean(x, dim=1) - torch.mean(x_reconstructed, dim=1)) ** 2\n        )\n\n        return loss\n\n    def configure_optimizers(self) -&gt; dict:\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=self.scheduler_max_epochs\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"train_loss\",\n                \"interval\": \"step\",\n                \"frequency\": 1,\n            },\n        }\n</pre> class VAEModel(L.LightningModule):     \"\"\"VAE model using VAEEncoder, VAEDecoder, and VAE      :param model: VAE model     :param reconstruction_weight: weight for the reconstruction loss     :param learning_rate: learning rate for the optimizer     :param scheduler_max_epochs: maximum epochs for the scheduler     \"\"\"      def __init__(         self,         model: VAE,         reconstruction_weight: float = 1.0,         learning_rate: float = 1e-3,         scheduler_max_epochs: int = 10000,     ):         super().__init__()         self.model = model         self.reconstruction_weight = reconstruction_weight         self.learning_rate = learning_rate         self.scheduler_max_epochs = scheduler_max_epochs          self.hparams.update(             {                 **model.hparams,                 **{                     \"reconstruction_weight\": reconstruction_weight,                     \"learning_rate\": learning_rate,                     \"scheduler_max_epochs\": scheduler_max_epochs,                 },             }         )         self.save_hyperparameters(self.hparams)      def forward(self, x):         return self.model(x)      def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )          self.log_dict(             {                 \"train_loss_total\": loss_total,                 \"train_loss_reconstruction\": loss_reconstruction,                 \"train_loss_kl\": loss_kl,             }         )          return loss_total      def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )         self.log_dict(             {                 \"val_loss_total\": loss_total,                 \"val_loss_reconstruction\": loss_reconstruction,                 \"val_loss_kl\": loss_kl,             }         )          return loss_total      def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )         self.log_dict(             {                 \"test_loss_total\": loss_total,                 \"test_loss_reconstruction\": loss_reconstruction,                 \"test_loss_kl\": loss_kl,             }         )         return loss_total      def loss(         self,         x: torch.Tensor,         x_reconstructed: torch.Tensor,         z_log_var: torch.Tensor,         z_mean: torch.Tensor,     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         loss_reconstruction = self.reconstruction_loss(x, x_reconstructed)         loss_kl = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - z_log_var.exp())         loss_total = self.reconstruction_weight * loss_reconstruction + loss_kl          return (             loss_total / x.size(0),             loss_reconstruction / x.size(0),             loss_kl / x.size(0),         )      def reconstruction_loss(         self, x: torch.Tensor, x_reconstructed: torch.Tensor     ) -&gt; torch.Tensor:         \"\"\"Reconstruction loss for VAE.          $$         \\sum_{i=1}^{N} (x_i - x_{reconstructed_i})^2         + \\sum_{i=1}^{N} (\\mu_i - \\mu_{reconstructed_i})^2         $$         \"\"\"         loss = torch.sum((x - x_reconstructed) ** 2) + torch.sum(             (torch.mean(x, dim=1) - torch.mean(x_reconstructed, dim=1)) ** 2         )          return loss      def configure_optimizers(self) -&gt; dict:         optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(             optimizer, T_max=self.scheduler_max_epochs         )          return {             \"optimizer\": optimizer,             \"lr_scheduler\": {                 \"scheduler\": scheduler,                 \"monitor\": \"train_loss\",                 \"interval\": \"step\",                 \"frequency\": 1,             },         } In\u00a0[\u00a0]: Copied! <pre>window_size = 24\nmax_epochs = 2000\n\ntime_vae_dm = TimeVAEDataModule(\n    window_size=window_size, dataframe=df[[\"theta\"]], batch_size=32\n)\n</pre> window_size = 24 max_epochs = 2000  time_vae_dm = TimeVAEDataModule(     window_size=window_size, dataframe=df[[\"theta\"]], batch_size=32 ) In\u00a0[\u00a0]: Copied! <pre>vae = VAE(\n    encoder=VAEEncoder(\n        VAEParams(\n            hidden_layer_sizes=[200, 100, 50],\n            latent_size=8,\n            sequence_length=window_size,\n        )\n    ),\n    decoder=VAEDecoder(\n        VAEParams(\n            hidden_layer_sizes=[30, 50, 100], latent_size=8, sequence_length=window_size\n        )\n    ),\n)\n\nvae_model = VAEModel(\n    vae,\n    reconstruction_weight=3,\n    scheduler_max_epochs=max_epochs * len(time_vae_dm.train_dataloader()),\n)\n</pre> vae = VAE(     encoder=VAEEncoder(         VAEParams(             hidden_layer_sizes=[200, 100, 50],             latent_size=8,             sequence_length=window_size,         )     ),     decoder=VAEDecoder(         VAEParams(             hidden_layer_sizes=[30, 50, 100], latent_size=8, sequence_length=window_size         )     ), )  vae_model = VAEModel(     vae,     reconstruction_weight=3,     scheduler_max_epochs=max_epochs * len(time_vae_dm.train_dataloader()), ) In\u00a0[\u00a0]: Copied! <pre>trainer = L.Trainer(\n    precision=\"64\",\n    max_epochs=max_epochs,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_loss_total\", mode=\"min\", min_delta=1e-10, patience=10\n        )\n    ],\n    logger=L.pytorch.loggers.TensorBoardLogger(\n        save_dir=\"lightning_logs\", name=\"time_vae_naive\"\n    ),\n)\n</pre> trainer = L.Trainer(     precision=\"64\",     max_epochs=max_epochs,     min_epochs=5,     callbacks=[         EarlyStopping(             monitor=\"val_loss_total\", mode=\"min\", min_delta=1e-10, patience=10         )     ],     logger=L.pytorch.loggers.TensorBoardLogger(         save_dir=\"lightning_logs\", name=\"time_vae_naive\"     ), ) In\u00a0[\u00a0]: Copied! <pre>trainer.fit(model=vae_model, datamodule=time_vae_dm)\n</pre> trainer.fit(model=vae_model, datamodule=time_vae_dm) In\u00a0[\u00a0]: Copied! <pre>IS_RELOAD = True\n</pre> IS_RELOAD = True In\u00a0[\u00a0]: Copied! <pre>if IS_RELOAD:\n    checkpoint_path = \"lightning_logs/time_vae_naive/version_29/checkpoints/epoch=1999-step=354000.ckpt\"\n    vae_model_reloaded = VAEModel.load_from_checkpoint(checkpoint_path, model=vae)\nelse:\n    vae_model_reloaded = vae_model\n</pre> if IS_RELOAD:     checkpoint_path = \"lightning_logs/time_vae_naive/version_29/checkpoints/epoch=1999-step=354000.ckpt\"     vae_model_reloaded = VAEModel.load_from_checkpoint(checkpoint_path, model=vae) else:     vae_model_reloaded = vae_model In\u00a0[\u00a0]: Copied! <pre>for pred_batch in time_vae_dm.predict_dataloader():\n    print(pred_batch.size())\n    i_pred = vae_model_reloaded.model(pred_batch.float().cuda())\n    break\n</pre> for pred_batch in time_vae_dm.predict_dataloader():     print(pred_batch.size())     i_pred = vae_model_reloaded.model(pred_batch.float().cuda())     break In\u00a0[\u00a0]: Copied! <pre>i_pred[0].size()\n</pre> i_pred[0].size() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nelement = 4\n\nax.plot(pred_batch.detach().numpy()[element, :, 0])\nax.plot(i_pred[0].cpu().detach().numpy()[element, :, 0], \"x-\")\n</pre> _, ax = plt.subplots()  element = 4  ax.plot(pred_batch.detach().numpy()[element, :, 0]) ax.plot(i_pred[0].cpu().detach().numpy()[element, :, 0], \"x-\") <p>Data generation using the decoder.</p> In\u00a0[\u00a0]: Copied! <pre>sampling_z = torch.randn(\n    pred_batch.size(0), vae_model_reloaded.model.encoder.params.latent_size\n).type_as(vae_model_reloaded.model.encoder.z_mean_layer.weight)\ngenerated_samples_x = (\n    vae_model_reloaded.model.decoder(sampling_z).cpu().detach().numpy().squeeze()\n)\n</pre> sampling_z = torch.randn(     pred_batch.size(0), vae_model_reloaded.model.encoder.params.latent_size ).type_as(vae_model_reloaded.model.encoder.z_mean_layer.weight) generated_samples_x = (     vae_model_reloaded.model.decoder(sampling_z).cpu().detach().numpy().squeeze() ) In\u00a0[\u00a0]: Copied! <pre>generated_samples_x.size()\n</pre> generated_samples_x.size() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nfor i in range(min(len(generated_samples_x), 4)):\n    ax.plot(generated_samples_x[i, :], \"x-\")\n</pre> _, ax = plt.subplots()  for i in range(min(len(generated_samples_x), 4)):     ax.plot(generated_samples_x[i, :], \"x-\")  In\u00a0[\u00a0]: Copied! <pre>from openTSNE import TSNE\n</pre> from openTSNE import TSNE In\u00a0[\u00a0]: Copied! <pre>n_tsne_samples = 100\n</pre> n_tsne_samples = 100 In\u00a0[\u00a0]: Copied! <pre>original_samples = pred_batch.cpu().detach().numpy().squeeze()[:n_tsne_samples]\noriginal_samples.shape\n</pre> original_samples = pred_batch.cpu().detach().numpy().squeeze()[:n_tsne_samples] original_samples.shape In\u00a0[\u00a0]: Copied! <pre>tsne = TSNE(\n    perplexity=30,\n    metric=\"euclidean\",\n    n_jobs=8,\n    random_state=42,\n    verbose=True,\n)\n</pre> tsne = TSNE(     perplexity=30,     metric=\"euclidean\",     n_jobs=8,     random_state=42,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>original_samples_embedding = tsne.fit(original_samples)\n</pre> original_samples_embedding = tsne.fit(original_samples) In\u00a0[\u00a0]: Copied! <pre>generated_samples_x[:n_tsne_samples]\n</pre> generated_samples_x[:n_tsne_samples] In\u00a0[\u00a0]: Copied! <pre>generated_samples_embedding = original_samples_embedding.transform(\n    generated_samples_x[:n_tsne_samples]\n)\n</pre> generated_samples_embedding = original_samples_embedding.transform(     generated_samples_x[:n_tsne_samples] ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 7))\n\nax.scatter(\n    original_samples_embedding[:, 0],\n    original_samples_embedding[:, 1],\n    color=\"black\",\n    marker=\".\",\n    label=\"original\",\n)\n\nax.scatter(\n    generated_samples_embedding[:, 0],\n    generated_samples_embedding[:, 1],\n    color=\"red\",\n    marker=\"x\",\n    label=\"generated\",\n)\n\nax.set_title(\"t-SNE of original and generated samples\")\nax.set_xlabel(\"t-SNE 1\")\nax.set_ylabel(\"t-SNE 2\")\n</pre> fig, ax = plt.subplots(figsize=(7, 7))  ax.scatter(     original_samples_embedding[:, 0],     original_samples_embedding[:, 1],     color=\"black\",     marker=\".\",     label=\"original\", )  ax.scatter(     generated_samples_embedding[:, 0],     generated_samples_embedding[:, 1],     color=\"red\",     marker=\"x\",     label=\"generated\", )  ax.set_title(\"t-SNE of original and generated samples\") ax.set_xlabel(\"t-SNE 1\") ax.set_ylabel(\"t-SNE 2\")"}, {"location": "notebooks/time_vae/#timevae", "title": "TimeVAE\u00b6", "text": "<p>Use VAE to generate time series data. In this example, we will train a VAE model to sinusoidal time series data. The overall structure of the model is shown below:</p> <pre><code>mermaid\ngraph TD\ndata[\"Time Series Chunks\"] --&gt; E[Encoder]\n    E --&gt; L[Latent Space]\n    L --&gt; D[Decoder]\n    D --&gt; gen[\"Generated Time Series Chunks\"]\n</code></pre> <p>Reference: https://github.com/wangyz1999/timeVAE-pytorch</p>"}, {"location": "notebooks/time_vae/#data", "title": "Data\u00b6", "text": "<p>We will reuse our classic pendulum dataset.</p>"}, {"location": "notebooks/time_vae/#model", "title": "Model\u00b6", "text": ""}, {"location": "notebooks/time_vae/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/time_vae/#fitted-model", "title": "Fitted Model\u00b6", "text": ""}, {"location": "notebooks/time_vae_poison/", "title": "TimeVAE", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\nfrom numbers import Number\nfrom typing import Dict, List, Tuple\n\nimport lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom loguru import logger\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n</pre> from functools import cached_property from numbers import Number from typing import Dict, List, Tuple  import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from loguru import logger from torch import nn from torch.utils.data import DataLoader, Dataset  In\u00a0[\u00a0]: Copied! <pre>class KioskVisitors:\n    \"\"\"generate number of visitors for a kiosk store\n\n    :param daily_profile: expectations of visitors\n        within a fixed time window during a day\n    \"\"\"\n\n    def __init__(self, daily_profile: np.ndarray):\n        self.daily_profile = daily_profile\n        self.daily_segments = len(daily_profile)\n\n    def __call__(\n        self,\n        n_days: int,\n        seed: Number = 42,\n    ) -&gt; pd.DataFrame:\n        \"\"\"generate number of visitors for n_days\n\n        :param n_days: number of days to generate visitors\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        visitors = np.concatenate(\n            [rng.poisson(self.daily_profile) for _ in range(n_days)]\n        )\n\n        df = pd.DataFrame(\n            {\n                \"visitors\": visitors,\n                \"t\": np.arange(len(visitors)),\n                \"expectation\": np.tile(self.daily_profile, n_days),\n            }\n        )\n\n        return df\n</pre> class KioskVisitors:     \"\"\"generate number of visitors for a kiosk store      :param daily_profile: expectations of visitors         within a fixed time window during a day     \"\"\"      def __init__(self, daily_profile: np.ndarray):         self.daily_profile = daily_profile         self.daily_segments = len(daily_profile)      def __call__(         self,         n_days: int,         seed: Number = 42,     ) -&gt; pd.DataFrame:         \"\"\"generate number of visitors for n_days          :param n_days: number of days to generate visitors         \"\"\"         rng = np.random.default_rng(seed)         visitors = np.concatenate(             [rng.poisson(self.daily_profile) for _ in range(n_days)]         )          df = pd.DataFrame(             {                 \"visitors\": visitors,                 \"t\": np.arange(len(visitors)),                 \"expectation\": np.tile(self.daily_profile, n_days),             }         )          return df In\u00a0[\u00a0]: Copied! <pre>def profile_sin(\n    t: np.ndarray,\n    lambda_min: float,\n    lambda_max: float,\n) -&gt; np.ndarray:\n    \"\"\"generate a sin wave profile for\n    the expected number of visitors\n    within a few min for each hour during a day\n\n    :param t: time in minutes\n    :param lambda_min: minimum number of visitors\n    :param lambda_max: maximum number of visitors\n    \"\"\"\n    amplitude = lambda_max - lambda_min\n    t_rescaled = (t - t.min()) / t.max() * np.pi\n\n    return amplitude * np.sin(t_rescaled) + lambda_min\n</pre> def profile_sin(     t: np.ndarray,     lambda_min: float,     lambda_max: float, ) -&gt; np.ndarray:     \"\"\"generate a sin wave profile for     the expected number of visitors     within a few min for each hour during a day      :param t: time in minutes     :param lambda_min: minimum number of visitors     :param lambda_max: maximum number of visitors     \"\"\"     amplitude = lambda_max - lambda_min     t_rescaled = (t - t.min()) / t.max() * np.pi      return amplitude * np.sin(t_rescaled) + lambda_min In\u00a0[\u00a0]: Copied! <pre>daily_profile = profile_sin(np.arange(0, 12 * 60 / 5, 1), lambda_min=9, lambda_max=10)\n\nkiosk_visitors = KioskVisitors(daily_profile=daily_profile)\n\ndf_visitors = kiosk_visitors(n_days=30).melt(\n    id_vars=\"t\",\n    value_vars=[\"visitors\", \"expectation\"],\n    var_name=\"data\",\n    value_name=\"count\",\n)\n\ndf_visitors.head()\n_, ax = plt.subplots(figsize=(6, 6.18 * 0.6))\n\nsns.lineplot(\n    df_visitors,\n    x=\"t\",\n    y=\"count\",\n    style=\"data\",\n    hue=\"data\",\n    ax=ax,\n)\n</pre> daily_profile = profile_sin(np.arange(0, 12 * 60 / 5, 1), lambda_min=9, lambda_max=10)  kiosk_visitors = KioskVisitors(daily_profile=daily_profile)  df_visitors = kiosk_visitors(n_days=30).melt(     id_vars=\"t\",     value_vars=[\"visitors\", \"expectation\"],     var_name=\"data\",     value_name=\"count\", )  df_visitors.head() _, ax = plt.subplots(figsize=(6, 6.18 * 0.6))  sns.lineplot(     df_visitors,     x=\"t\",     y=\"count\",     style=\"data\",     hue=\"data\",     ax=ax, ) In\u00a0[\u00a0]: Copied! <pre>df = df_visitors.loc[df_visitors[\"data\"] == \"visitors\"][[\"t\", \"count\"]]\ndf[\"count\"] = df[\"count\"] + 2\n</pre> df = df_visitors.loc[df_visitors[\"data\"] == \"visitors\"][[\"t\", \"count\"]] df[\"count\"] = df[\"count\"] + 2 In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.head(100).plot(x=\"t\", y=\"count\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.head(100).plot(x=\"t\", y=\"count\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"count\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"count\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre>def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:\n    \"\"\"embed time series into a time delay embedding space\n\n    Time column `t` is required in the input data frame.\n\n    :param df: original time series data frame\n    :param window_size: window size for the time delay embedding\n    \"\"\"\n    dfs_embedded = []\n\n    for i in df.rolling(window_size):\n        i_t = i.t.iloc[0]\n        dfs_embedded.append(\n            pd.DataFrame(i.reset_index(drop=True))\n            .drop(columns=[\"t\"])\n            .T.reset_index(drop=True)\n            # .rename(columns={\"index\": \"name\"})\n            # .assign(t=i_t)\n        )\n\n    df_embedded = pd.concat(dfs_embedded[window_size - 1 :])\n\n    return df_embedded\n</pre> def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:     \"\"\"embed time series into a time delay embedding space      Time column `t` is required in the input data frame.      :param df: original time series data frame     :param window_size: window size for the time delay embedding     \"\"\"     dfs_embedded = []      for i in df.rolling(window_size):         i_t = i.t.iloc[0]         dfs_embedded.append(             pd.DataFrame(i.reset_index(drop=True))             .drop(columns=[\"t\"])             .T.reset_index(drop=True)             # .rename(columns={\"index\": \"name\"})             # .assign(t=i_t)         )      df_embedded = pd.concat(dfs_embedded[window_size - 1 :])      return df_embedded In\u00a0[\u00a0]: Copied! <pre>time_delay_embed(df, 3)\n</pre> time_delay_embed(df, 3) In\u00a0[\u00a0]: Copied! <pre>class TimeVAEDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param window_size: length of time series slicing chunks\n    \"\"\"\n\n    def __init__(\n        self,\n        dataframe: pd.DataFrame,\n        window_size: int,\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.window_size = window_size\n        self.dataframe_rows = len(self.dataframe)\n        self.length = self.dataframe_rows - self.window_size + 1\n\n    def moving_slicing(self, idx: int) -&gt; np.ndarray:\n        return self.dataframe[idx : self.window_size + idx].values\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [self.moving_slicing(i) for i in range(idx.start, idx.stop, step)]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</pre> class TimeVAEDataset(Dataset):     \"\"\"A dataset from a pandas dataframe.      For a given pandas dataframe, this generates a pytorch     compatible dataset by sliding in time dimension.      ```python     ds = DataFrameDataset(         dataframe=df, history_length=10, horizon=2     )     ```      :param dataframe: input dataframe with a DatetimeIndex.     :param window_size: length of time series slicing chunks     \"\"\"      def __init__(         self,         dataframe: pd.DataFrame,         window_size: int,     ):         super().__init__()         self.dataframe = dataframe         self.window_size = window_size         self.dataframe_rows = len(self.dataframe)         self.length = self.dataframe_rows - self.window_size + 1      def moving_slicing(self, idx: int) -&gt; np.ndarray:         return self.dataframe[idx : self.window_size + idx].values      def _validate_dataframe(self) -&gt; None:         \"\"\"Validate the input dataframe.          - We require the dataframe index to be DatetimeIndex.         - This dataset is null aversion.         - Dataframe index should be sorted.         \"\"\"          if not isinstance(             self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex         ):             raise TypeError(                 \"Type of the dataframe index is not DatetimeIndex\"                 f\": {type(self.dataframe.index)}\"             )          has_na = self.dataframe.isnull().values.any()          if has_na:             logger.warning(\"Dataframe has null\")          has_index_sorted = self.dataframe.index.equals(             self.dataframe.index.sort_values()         )          if not has_index_sorted:             logger.warning(\"Dataframe index is not sorted\")      def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:         if isinstance(idx, slice):             if (idx.start &lt; 0) or (idx.stop &gt;= self.length):                 raise IndexError(f\"Slice out of range: {idx}\")             step = idx.step if idx.step is not None else 1             return [self.moving_slicing(i) for i in range(idx.start, idx.stop, step)]         else:             if idx &gt;= self.length:                 raise IndexError(\"End of dataset\")             return self.moving_slicing(idx)      def __len__(self) -&gt; int:         return self.length In\u00a0[\u00a0]: Copied! <pre>class TimeVAEDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule for Time Series VAE.\n\n    This data module takes a pandas dataframe and generates\n    the corresponding dataloaders for training, validation and\n    testing.\n\n    ```python\n    time_vae_dm_example = TimeVAEDataModule(\n        window_size=30, dataframe=df[[\"count\"]], batch_size=32\n    )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int,\n        dataframe: pd.DataFrame,\n        test_fraction: float = 0.3,\n        val_fraction: float = 0.1,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.batch_size = batch_size\n        self.dataframe = dataframe\n        self.test_fraction = test_fraction\n        self.val_fraction = val_fraction\n        self.num_workers = num_workers\n\n        self.train_dataset, self.val_dataset = self.split_train_val(\n            self.train_val_dataset\n        )\n\n    @cached_property\n    def df_length(self):\n        return len(self.dataframe)\n\n    @cached_property\n    def df_test_length(self):\n        return int(self.df_length * self.test_fraction)\n\n    @cached_property\n    def df_train_val_length(self):\n        return self.df_length - self.df_test_length\n\n    @cached_property\n    def train_val_dataframe(self):\n        return self.dataframe.iloc[: self.df_train_val_length]\n\n    @cached_property\n    def test_dataframe(self):\n        return self.dataframe.iloc[self.df_train_val_length :]\n\n    @cached_property\n    def train_val_dataset(self):\n        return TimeVAEDataset(\n            dataframe=self.train_val_dataframe,\n            window_size=self.window_size,\n        )\n\n    @cached_property\n    def test_dataset(self):\n        return TimeVAEDataset(\n            dataframe=self.test_dataframe,\n            window_size=self.window_size,\n        )\n\n    def split_train_val(self, dataset: Dataset):\n        return torch.utils.data.random_split(\n            dataset, [1 - self.val_fraction, self.val_fraction]\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            persistent_workers=(True if self.num_workers &gt; 0 else False),\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            persistent_workers=(True if self.num_workers &gt; 0 else False),\n        )\n\n    def predict_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False\n        )\n</pre> class TimeVAEDataModule(L.LightningDataModule):     \"\"\"Lightning DataModule for Time Series VAE.      This data module takes a pandas dataframe and generates     the corresponding dataloaders for training, validation and     testing.      ```python     time_vae_dm_example = TimeVAEDataModule(         window_size=30, dataframe=df[[\"count\"]], batch_size=32     )     ```     \"\"\"      def __init__(         self,         window_size: int,         dataframe: pd.DataFrame,         test_fraction: float = 0.3,         val_fraction: float = 0.1,         batch_size: int = 32,         num_workers: int = 0,     ):         super().__init__()         self.window_size = window_size         self.batch_size = batch_size         self.dataframe = dataframe         self.test_fraction = test_fraction         self.val_fraction = val_fraction         self.num_workers = num_workers          self.train_dataset, self.val_dataset = self.split_train_val(             self.train_val_dataset         )      @cached_property     def df_length(self):         return len(self.dataframe)      @cached_property     def df_test_length(self):         return int(self.df_length * self.test_fraction)      @cached_property     def df_train_val_length(self):         return self.df_length - self.df_test_length      @cached_property     def train_val_dataframe(self):         return self.dataframe.iloc[: self.df_train_val_length]      @cached_property     def test_dataframe(self):         return self.dataframe.iloc[self.df_train_val_length :]      @cached_property     def train_val_dataset(self):         return TimeVAEDataset(             dataframe=self.train_val_dataframe,             window_size=self.window_size,         )      @cached_property     def test_dataset(self):         return TimeVAEDataset(             dataframe=self.test_dataframe,             window_size=self.window_size,         )      def split_train_val(self, dataset: Dataset):         return torch.utils.data.random_split(             dataset, [1 - self.val_fraction, self.val_fraction]         )      def train_dataloader(self):         return DataLoader(             dataset=self.train_dataset,             batch_size=self.batch_size,             shuffle=True,             num_workers=self.num_workers,             persistent_workers=(True if self.num_workers &gt; 0 else False),         )      def test_dataloader(self):         return DataLoader(             dataset=self.test_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,         )      def val_dataloader(self):         return DataLoader(             dataset=self.val_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,             persistent_workers=(True if self.num_workers &gt; 0 else False),         )      def predict_dataloader(self):         return DataLoader(             dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False         ) In\u00a0[\u00a0]: Copied! <pre>time_vae_dm_example = TimeVAEDataModule(\n    window_size=30, dataframe=df[[\"count\"]], batch_size=32\n)\n</pre> time_vae_dm_example = TimeVAEDataModule(     window_size=30, dataframe=df[[\"count\"]], batch_size=32 ) In\u00a0[\u00a0]: Copied! <pre>len(list(time_vae_dm_example.train_dataloader()))\n</pre> len(list(time_vae_dm_example.train_dataloader())) In\u00a0[\u00a0]: Copied! <pre>list(time_vae_dm_example.train_dataloader())[0].shape\n</pre> list(time_vae_dm_example.train_dataloader())[0].shape In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass VAEParams:\n    \"\"\"Parameters for VAEEncoder and VAEDecoder\n\n    :param hidden_layer_sizes: list of hidden layer sizes\n    :param latent_size: latent space dimension\n    :param sequence_length: input sequence length\n    :param n_features: number of features\n    \"\"\"\n\n    hidden_layer_sizes: List[int]\n    latent_size: int\n    sequence_length: int\n    n_features: int = 1\n\n    @cached_property\n    def data_size(self) -&gt; int:\n        \"\"\"The dimension of the input data\n        when flattened.\n        \"\"\"\n        return self.sequence_length * self.n_features\n\n    def asdict(self) -&gt; dict:\n        return dataclasses.asdict(self)\n</pre> @dataclasses.dataclass class VAEParams:     \"\"\"Parameters for VAEEncoder and VAEDecoder      :param hidden_layer_sizes: list of hidden layer sizes     :param latent_size: latent space dimension     :param sequence_length: input sequence length     :param n_features: number of features     \"\"\"      hidden_layer_sizes: List[int]     latent_size: int     sequence_length: int     n_features: int = 1      @cached_property     def data_size(self) -&gt; int:         \"\"\"The dimension of the input data         when flattened.         \"\"\"         return self.sequence_length * self.n_features      def asdict(self) -&gt; dict:         return dataclasses.asdict(self) In\u00a0[\u00a0]: Copied! <pre>class VAEMLPEncoder(nn.Module):\n    \"\"\"MLP Encoder of TimeVAE\"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n\n        encode_layer_sizes = [self.params.data_size] + self.params.hidden_layer_sizes\n        self.layers_used_to_encode = [\n            self._linear_block(size_in, size_out)\n            for size_in, size_out in zip(\n                encode_layer_sizes[:-1], encode_layer_sizes[1:]\n            )\n        ]\n        self.encode = nn.Sequential(*self.layers_used_to_encode)\n        encoded_size = self.params.hidden_layer_sizes[-1]\n        self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)\n        self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_size, _, _ = x.size()\n        x = x.transpose(1, 2)\n        x = self.encode(x)\n\n        z_mean = self.z_mean_layer(x)\n        z_log_var = self.z_log_var_layer(x)\n        epsilon = torch.randn(\n            batch_size, self.params.n_features, self.params.latent_size\n        ).type_as(x)\n        z = z_mean + torch.exp(0.5 * z_log_var) * epsilon\n\n        return z_mean, z_log_var, z\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])\n</pre> class VAEMLPEncoder(nn.Module):     \"\"\"MLP Encoder of TimeVAE\"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params          encode_layer_sizes = [self.params.data_size] + self.params.hidden_layer_sizes         self.layers_used_to_encode = [             self._linear_block(size_in, size_out)             for size_in, size_out in zip(                 encode_layer_sizes[:-1], encode_layer_sizes[1:]             )         ]         self.encode = nn.Sequential(*self.layers_used_to_encode)         encoded_size = self.params.hidden_layer_sizes[-1]         self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)         self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)      def forward(         self, x: torch.Tensor     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         batch_size, _, _ = x.size()         x = x.transpose(1, 2)         x = self.encode(x)          z_mean = self.z_mean_layer(x)         z_log_var = self.z_log_var_layer(x)         epsilon = torch.randn(             batch_size, self.params.n_features, self.params.latent_size         ).type_as(x)         z = z_mean + torch.exp(0.5 * z_log_var) * epsilon          return z_mean, z_log_var, z      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()]) In\u00a0[\u00a0]: Copied! <pre>class VAEEncoder(nn.Module):\n    \"\"\"Encoder of TimeVAE\n\n    ```python\n    encoder = VAEEncoder(\n        VAEParams(\n            hidden_layer_sizes=[40, 30],\n            latent_size=10,\n            sequence_length=50\n        )\n    )\n    ```\n\n    :param params: parameters for the encoder\n    \"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        encode_layer_sizes = [self.params.n_features] + self.params.hidden_layer_sizes\n        self.layers_used_to_encode = [\n            self._conv_block(size_in, size_out)\n            for size_in, size_out in zip(\n                encode_layer_sizes[:-1], encode_layer_sizes[1:]\n            )\n        ] + [nn.Flatten()]\n        self.encode = nn.Sequential(*self.layers_used_to_encode)\n        encoded_size = self.cal_conv1d_output_dim() * self.params.hidden_layer_sizes[-1]\n        self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)\n        self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_size, _, _ = x.size()\n        x = x.transpose(1, 2)\n        x = self.encode(x)\n\n        z_mean = self.z_mean_layer(x).view(\n            batch_size, self.params.n_features, self.params.latent_size\n        )\n        z_log_var = self.z_log_var_layer(x).view(\n            batch_size, self.params.n_features, self.params.latent_size\n        )\n        epsilon = torch.randn(\n            batch_size, self.params.n_features, self.params.latent_size\n        ).type_as(x)\n        z = z_mean + torch.exp(0.5 * z_log_var) * epsilon\n\n        return z_mean, z_log_var, z\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])\n\n    def _conv_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        return nn.Sequential(\n            *[\n                nn.Conv1d(size_in, size_out, kernel_size=3, stride=2, padding=1),\n                nn.ReLU(),\n            ]\n        )\n\n    def cal_conv1d_output_dim(self) -&gt; int:\n        \"\"\"the output dimension of all the Conv1d layers\"\"\"\n        output_size = self.params.sequence_length * self.params.n_features\n\n        for l in self.layers_used_to_encode:\n            if l._get_name() == \"Conv1d\":\n                output_size = self._conv1d_output_dim(l, output_size)\n            elif l._get_name() == \"Sequential\":\n                for l2 in l:\n                    if l2._get_name() == \"Conv1d\":\n                        output_size = self._conv1d_output_dim(l2, output_size)\n\n        return output_size\n\n    def _conv1d_output_dim(self, layer: nn.Module, input_size: int) -&gt; int:\n        \"\"\"Formula to calculate\n        the output size of Conv1d layer\n        \"\"\"\n        return (\n            (input_size + 2 * layer.padding[0] - layer.kernel_size[0])\n            // layer.stride[0]\n        ) + 1\n</pre> class VAEEncoder(nn.Module):     \"\"\"Encoder of TimeVAE      ```python     encoder = VAEEncoder(         VAEParams(             hidden_layer_sizes=[40, 30],             latent_size=10,             sequence_length=50         )     )     ```      :param params: parameters for the encoder     \"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          encode_layer_sizes = [self.params.n_features] + self.params.hidden_layer_sizes         self.layers_used_to_encode = [             self._conv_block(size_in, size_out)             for size_in, size_out in zip(                 encode_layer_sizes[:-1], encode_layer_sizes[1:]             )         ] + [nn.Flatten()]         self.encode = nn.Sequential(*self.layers_used_to_encode)         encoded_size = self.cal_conv1d_output_dim() * self.params.hidden_layer_sizes[-1]         self.z_mean_layer = nn.Linear(encoded_size, self.params.latent_size)         self.z_log_var_layer = nn.Linear(encoded_size, self.params.latent_size)      def forward(         self, x: torch.Tensor     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         batch_size, _, _ = x.size()         x = x.transpose(1, 2)         x = self.encode(x)          z_mean = self.z_mean_layer(x).view(             batch_size, self.params.n_features, self.params.latent_size         )         z_log_var = self.z_log_var_layer(x).view(             batch_size, self.params.n_features, self.params.latent_size         )         epsilon = torch.randn(             batch_size, self.params.n_features, self.params.latent_size         ).type_as(x)         z = z_mean + torch.exp(0.5 * z_log_var) * epsilon          return z_mean, z_log_var, z      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.ReLU()])      def _conv_block(self, size_in: int, size_out: int) -&gt; nn.Module:         return nn.Sequential(             *[                 nn.Conv1d(size_in, size_out, kernel_size=3, stride=2, padding=1),                 nn.ReLU(),             ]         )      def cal_conv1d_output_dim(self) -&gt; int:         \"\"\"the output dimension of all the Conv1d layers\"\"\"         output_size = self.params.sequence_length * self.params.n_features          for l in self.layers_used_to_encode:             if l._get_name() == \"Conv1d\":                 output_size = self._conv1d_output_dim(l, output_size)             elif l._get_name() == \"Sequential\":                 for l2 in l:                     if l2._get_name() == \"Conv1d\":                         output_size = self._conv1d_output_dim(l2, output_size)          return output_size      def _conv1d_output_dim(self, layer: nn.Module, input_size: int) -&gt; int:         \"\"\"Formula to calculate         the output size of Conv1d layer         \"\"\"         return (             (input_size + 2 * layer.padding[0] - layer.kernel_size[0])             // layer.stride[0]         ) + 1 In\u00a0[\u00a0]: Copied! <pre>mlp_encoder = VAEMLPEncoder(\n    VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50)\n)\n\n[i.size() for i in mlp_encoder(torch.ones(32, 50, 1))], mlp_encoder(\n    torch.ones(32, 50, 1)\n)[-1]\n</pre> mlp_encoder = VAEMLPEncoder(     VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50) )  [i.size() for i in mlp_encoder(torch.ones(32, 50, 1))], mlp_encoder(     torch.ones(32, 50, 1) )[-1] In\u00a0[\u00a0]: Copied! <pre>encoder = VAEEncoder(\n    VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50)\n)\n\n[i.size() for i in encoder(torch.ones(32, 50, 1))], encoder(torch.ones(32, 50, 1))[-1]\n</pre> encoder = VAEEncoder(     VAEParams(hidden_layer_sizes=[40, 30], latent_size=10, sequence_length=50) )  [i.size() for i in encoder(torch.ones(32, 50, 1))], encoder(torch.ones(32, 50, 1))[-1] In\u00a0[\u00a0]: Copied! <pre>class VAEDecoder(nn.Module):\n    \"\"\"Decoder of TimeVAE\n\n    ```python\n    decoder = VAEDecoder(\n        VAEParams(\n            hidden_layer_sizes=[30, 40],\n            latent_size=10,\n            sequence_length=50,\n        )\n    )\n    ```\n\n    :param params: parameters for the decoder\n    \"\"\"\n\n    def __init__(self, params: VAEParams):\n        super().__init__()\n\n        self.params = params\n        self.hparams = params.asdict()\n\n        decode_layer_sizes = (\n            [self.params.latent_size]\n            + self.params.hidden_layer_sizes\n            + [self.params.data_size]\n        )\n\n        self.decode = nn.Sequential(\n            *[\n                self._linear_block(size_in, size_out)\n                for size_in, size_out in zip(\n                    decode_layer_sizes[:-1], decode_layer_sizes[1:]\n                )\n            ]\n        )\n\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        output = self.decode(z)\n        return output.view(-1, self.params.sequence_length, self.params.n_features)\n\n    def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:\n        \"\"\"create linear block based on the specified sizes\"\"\"\n        return nn.Sequential(*[nn.Linear(size_in, size_out), nn.Softplus()])\n</pre> class VAEDecoder(nn.Module):     \"\"\"Decoder of TimeVAE      ```python     decoder = VAEDecoder(         VAEParams(             hidden_layer_sizes=[30, 40],             latent_size=10,             sequence_length=50,         )     )     ```      :param params: parameters for the decoder     \"\"\"      def __init__(self, params: VAEParams):         super().__init__()          self.params = params         self.hparams = params.asdict()          decode_layer_sizes = (             [self.params.latent_size]             + self.params.hidden_layer_sizes             + [self.params.data_size]         )          self.decode = nn.Sequential(             *[                 self._linear_block(size_in, size_out)                 for size_in, size_out in zip(                     decode_layer_sizes[:-1], decode_layer_sizes[1:]                 )             ]         )      def forward(self, z: torch.Tensor) -&gt; torch.Tensor:         output = self.decode(z)         return output.view(-1, self.params.sequence_length, self.params.n_features)      def _linear_block(self, size_in: int, size_out: int) -&gt; nn.Module:         \"\"\"create linear block based on the specified sizes\"\"\"         return nn.Sequential(*[nn.Linear(size_in, size_out), nn.Softplus()]) In\u00a0[\u00a0]: Copied! <pre>decoder = VAEDecoder(\n    VAEParams(hidden_layer_sizes=[30, 40], latent_size=10, sequence_length=50)\n)\n\ndecoder(torch.ones(32, 1, 10)).size(), decoder(torch.ones(32, 1, 10))\n</pre> decoder = VAEDecoder(     VAEParams(hidden_layer_sizes=[30, 40], latent_size=10, sequence_length=50) )  decoder(torch.ones(32, 1, 10)).size(), decoder(torch.ones(32, 1, 10)) In\u00a0[\u00a0]: Copied! <pre>class VAE(nn.Module):\n    \"\"\"VAE model with encoder and decoder\n\n    :param encoder: encoder module\n    :param decoder: decoder module\n    \"\"\"\n\n    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.hparams = {\n            **{f\"encoder_{k}\": v for k, v in self.encoder.hparams.items()},\n            **{f\"decoder_{k}\": v for k, v in self.decoder.hparams.items()},\n        }\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        z_mean, z_log_var, z = self.encoder(x)\n        x_reconstructed = self.decoder(z)\n        return x_reconstructed, z_mean, z_log_var\n</pre> class VAE(nn.Module):     \"\"\"VAE model with encoder and decoder      :param encoder: encoder module     :param decoder: decoder module     \"\"\"      def __init__(self, encoder: nn.Module, decoder: nn.Module):         super().__init__()         self.encoder = encoder         self.decoder = decoder         self.hparams = {             **{f\"encoder_{k}\": v for k, v in self.encoder.hparams.items()},             **{f\"decoder_{k}\": v for k, v in self.decoder.hparams.items()},         }      def forward(         self, x: torch.Tensor     ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         z_mean, z_log_var, z = self.encoder(x)         x_reconstructed = self.decoder(z)         return x_reconstructed, z_mean, z_log_var In\u00a0[\u00a0]: Copied! <pre>class VAEModel(L.LightningModule):\n    \"\"\"VAE model using VAEEncoder, VAEDecoder, and VAE\n\n    :param model: VAE model\n    :param reconstruction_weight: weight for the reconstruction loss\n    :param learning_rate: learning rate for the optimizer\n    :param scheduler_max_epochs: maximum epochs for the scheduler\n    \"\"\"\n\n    def __init__(\n        self,\n        model: VAE,\n        reconstruction_weight: float = 1.0,\n        learning_rate: float = 1e-3,\n        scheduler_max_epochs: int = 10000,\n    ):\n        super().__init__()\n        self.model = model\n        self.reconstruction_weight = reconstruction_weight\n        self.learning_rate = learning_rate\n        self.scheduler_max_epochs = scheduler_max_epochs\n\n        self.hparams.update(\n            {\n                **model.hparams,\n                **{\n                    \"reconstruction_weight\": reconstruction_weight,\n                    \"learning_rate\": learning_rate,\n                    \"scheduler_max_epochs\": scheduler_max_epochs,\n                },\n            }\n        )\n        self.save_hyperparameters(self.hparams)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n\n        self.log_dict(\n            {\n                \"train_loss_total\": loss_total,\n                \"train_loss_reconstruction\": loss_reconstruction,\n                \"train_loss_kl\": loss_kl,\n            }\n        )\n\n        return loss_total\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n        self.log_dict(\n            {\n                \"val_loss_total\": loss_total,\n                \"val_loss_reconstruction\": loss_reconstruction,\n                \"val_loss_kl\": loss_kl,\n            }\n        )\n\n        return loss_total\n\n    def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:\n        batch_reconstructed, z_mean, z_log_var = self.model(batch)\n        loss_total, loss_reconstruction, loss_kl = self.loss(\n            x=batch,\n            x_reconstructed=batch_reconstructed,\n            z_mean=z_mean,\n            z_log_var=z_log_var,\n        )\n        self.log_dict(\n            {\n                \"test_loss_total\": loss_total,\n                \"test_loss_reconstruction\": loss_reconstruction,\n                \"test_loss_kl\": loss_kl,\n            }\n        )\n        return loss_total\n\n    def loss(\n        self,\n        x: torch.Tensor,\n        x_reconstructed: torch.Tensor,\n        z_log_var: torch.Tensor,\n        z_mean: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        loss_reconstruction = self.reconstruction_loss(x, x_reconstructed)\n        loss_kl = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - z_log_var.exp())\n        loss_total = self.reconstruction_weight * loss_reconstruction + loss_kl\n\n        return (\n            loss_total / x.size(0),\n            loss_reconstruction / x.size(0),\n            loss_kl / x.size(0),\n        )\n\n    def reconstruction_loss(\n        self, x: torch.Tensor, x_reconstructed: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Reconstruction loss for VAE.\n\n        $$\n        \\sum_{i=1}^{N} (x_i - x_{reconstructed_i})^2\n        + \\sum_{i=1}^{N} (\\mu_i - \\mu_{reconstructed_i})^2\n        $$\n        \"\"\"\n        loss = torch.sum((x - x_reconstructed) ** 2) + torch.sum(\n            (torch.mean(x, dim=1) - torch.mean(x_reconstructed, dim=1)) ** 2\n        )\n\n        return loss\n\n    def configure_optimizers(self) -&gt; dict:\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=self.scheduler_max_epochs\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"train_loss\",\n                \"interval\": \"step\",\n                \"frequency\": 1,\n            },\n        }\n</pre> class VAEModel(L.LightningModule):     \"\"\"VAE model using VAEEncoder, VAEDecoder, and VAE      :param model: VAE model     :param reconstruction_weight: weight for the reconstruction loss     :param learning_rate: learning rate for the optimizer     :param scheduler_max_epochs: maximum epochs for the scheduler     \"\"\"      def __init__(         self,         model: VAE,         reconstruction_weight: float = 1.0,         learning_rate: float = 1e-3,         scheduler_max_epochs: int = 10000,     ):         super().__init__()         self.model = model         self.reconstruction_weight = reconstruction_weight         self.learning_rate = learning_rate         self.scheduler_max_epochs = scheduler_max_epochs          self.hparams.update(             {                 **model.hparams,                 **{                     \"reconstruction_weight\": reconstruction_weight,                     \"learning_rate\": learning_rate,                     \"scheduler_max_epochs\": scheduler_max_epochs,                 },             }         )         self.save_hyperparameters(self.hparams)      def forward(self, x):         return self.model(x)      def training_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )          self.log_dict(             {                 \"train_loss_total\": loss_total,                 \"train_loss_reconstruction\": loss_reconstruction,                 \"train_loss_kl\": loss_kl,             }         )          return loss_total      def validation_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )         self.log_dict(             {                 \"val_loss_total\": loss_total,                 \"val_loss_reconstruction\": loss_reconstruction,                 \"val_loss_kl\": loss_kl,             }         )          return loss_total      def test_step(self, batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor:         batch_reconstructed, z_mean, z_log_var = self.model(batch)         loss_total, loss_reconstruction, loss_kl = self.loss(             x=batch,             x_reconstructed=batch_reconstructed,             z_mean=z_mean,             z_log_var=z_log_var,         )         self.log_dict(             {                 \"test_loss_total\": loss_total,                 \"test_loss_reconstruction\": loss_reconstruction,                 \"test_loss_kl\": loss_kl,             }         )         return loss_total      def loss(         self,         x: torch.Tensor,         x_reconstructed: torch.Tensor,         z_log_var: torch.Tensor,         z_mean: torch.Tensor,     ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:         loss_reconstruction = self.reconstruction_loss(x, x_reconstructed)         loss_kl = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - z_log_var.exp())         loss_total = self.reconstruction_weight * loss_reconstruction + loss_kl          return (             loss_total / x.size(0),             loss_reconstruction / x.size(0),             loss_kl / x.size(0),         )      def reconstruction_loss(         self, x: torch.Tensor, x_reconstructed: torch.Tensor     ) -&gt; torch.Tensor:         \"\"\"Reconstruction loss for VAE.          $$         \\sum_{i=1}^{N} (x_i - x_{reconstructed_i})^2         + \\sum_{i=1}^{N} (\\mu_i - \\mu_{reconstructed_i})^2         $$         \"\"\"         loss = torch.sum((x - x_reconstructed) ** 2) + torch.sum(             (torch.mean(x, dim=1) - torch.mean(x_reconstructed, dim=1)) ** 2         )          return loss      def configure_optimizers(self) -&gt; dict:         optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(             optimizer, T_max=self.scheduler_max_epochs         )          return {             \"optimizer\": optimizer,             \"lr_scheduler\": {                 \"scheduler\": scheduler,                 \"monitor\": \"train_loss\",                 \"interval\": \"step\",                 \"frequency\": 1,             },         } In\u00a0[\u00a0]: Copied! <pre>window_size = 24\nmax_epochs = 2000\n\ntime_vae_dm = TimeVAEDataModule(\n    window_size=window_size, dataframe=df[[\"count\"]], batch_size=32\n)\n</pre> window_size = 24 max_epochs = 2000  time_vae_dm = TimeVAEDataModule(     window_size=window_size, dataframe=df[[\"count\"]], batch_size=32 ) In\u00a0[\u00a0]: Copied! <pre>vae = VAE(\n    encoder=VAEEncoder(\n        VAEParams(\n            hidden_layer_sizes=[200, 100, 50],\n            latent_size=8,\n            sequence_length=window_size,\n        )\n    ),\n    decoder=VAEDecoder(\n        VAEParams(\n            hidden_layer_sizes=[30, 50, 100], latent_size=8, sequence_length=window_size\n        )\n    ),\n)\n\nvae_model = VAEModel(\n    vae,\n    reconstruction_weight=1,\n    learning_rate=5e-4,\n    scheduler_max_epochs=max_epochs * len(time_vae_dm.train_dataloader()),\n)\n</pre> vae = VAE(     encoder=VAEEncoder(         VAEParams(             hidden_layer_sizes=[200, 100, 50],             latent_size=8,             sequence_length=window_size,         )     ),     decoder=VAEDecoder(         VAEParams(             hidden_layer_sizes=[30, 50, 100], latent_size=8, sequence_length=window_size         )     ), )  vae_model = VAEModel(     vae,     reconstruction_weight=1,     learning_rate=5e-4,     scheduler_max_epochs=max_epochs * len(time_vae_dm.train_dataloader()), ) In\u00a0[\u00a0]: Copied! <pre>trainer = L.Trainer(\n    precision=\"64\",\n    max_epochs=max_epochs,\n    min_epochs=5,\n    # callbacks=[\n    #     EarlyStopping(\n    #         monitor=\"val_loss_total\", mode=\"min\", min_delta=1e-10, patience=10\n    #     )\n    # ],\n    logger=L.pytorch.loggers.TensorBoardLogger(\n        save_dir=\"lightning_logs\", name=\"time_vae_koisk\"\n    ),\n)\n</pre> trainer = L.Trainer(     precision=\"64\",     max_epochs=max_epochs,     min_epochs=5,     # callbacks=[     #     EarlyStopping(     #         monitor=\"val_loss_total\", mode=\"min\", min_delta=1e-10, patience=10     #     )     # ],     logger=L.pytorch.loggers.TensorBoardLogger(         save_dir=\"lightning_logs\", name=\"time_vae_koisk\"     ), ) In\u00a0[\u00a0]: Copied! <pre>trainer.fit(model=vae_model, datamodule=time_vae_dm)\n</pre> trainer.fit(model=vae_model, datamodule=time_vae_dm) In\u00a0[\u00a0]: Copied! <pre>vae.encoder(pred_batch.cuda())\n</pre> vae.encoder(pred_batch.cuda()) In\u00a0[\u00a0]: Copied! <pre>IS_RELOAD = False\n</pre> IS_RELOAD = False In\u00a0[\u00a0]: Copied! <pre>if IS_RELOAD:\n    checkpoint_path = \"lightning_logs/time_vae_naive/version_29/checkpoints/epoch=1999-step=354000.ckpt\"\n    vae_model_reloaded = VAEModel.load_from_checkpoint(checkpoint_path, model=vae)\nelse:\n    vae_model_reloaded = vae_model\n</pre> if IS_RELOAD:     checkpoint_path = \"lightning_logs/time_vae_naive/version_29/checkpoints/epoch=1999-step=354000.ckpt\"     vae_model_reloaded = VAEModel.load_from_checkpoint(checkpoint_path, model=vae) else:     vae_model_reloaded = vae_model In\u00a0[\u00a0]: Copied! <pre>for pred_batch in time_vae_dm.predict_dataloader():\n    print(pred_batch.size())\n    i_pred = vae_model_reloaded.model(pred_batch.float().cuda())\n    break\n</pre> for pred_batch in time_vae_dm.predict_dataloader():     print(pred_batch.size())     i_pred = vae_model_reloaded.model(pred_batch.float().cuda())     break In\u00a0[\u00a0]: Copied! <pre>i_pred[0].size()\n</pre> i_pred[0].size() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nelement = 4\n\nax.plot(pred_batch.detach().numpy()[element, :, 0])\nax.plot(i_pred[0].cpu().detach().numpy()[element, :, 0], \"x-\")\n</pre> _, ax = plt.subplots()  element = 4  ax.plot(pred_batch.detach().numpy()[element, :, 0]) ax.plot(i_pred[0].cpu().detach().numpy()[element, :, 0], \"x-\") <p>Data generation using the decoder.</p> In\u00a0[\u00a0]: Copied! <pre>sampling_z = torch.randn(\n    pred_batch.size(0), vae_model_reloaded.model.encoder.params.latent_size\n).type_as(vae_model_reloaded.model.encoder.z_mean_layer.weight)\ngenerated_samples_x = (\n    vae_model_reloaded.model.decoder(sampling_z).cpu().detach().numpy().squeeze()\n)\n</pre> sampling_z = torch.randn(     pred_batch.size(0), vae_model_reloaded.model.encoder.params.latent_size ).type_as(vae_model_reloaded.model.encoder.z_mean_layer.weight) generated_samples_x = (     vae_model_reloaded.model.decoder(sampling_z).cpu().detach().numpy().squeeze() ) In\u00a0[\u00a0]: Copied! <pre>generated_samples_x.size()\n</pre> generated_samples_x.size() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nfor i in range(min(len(generated_samples_x), 4)):\n    ax.plot(generated_samples_x[i, :], \"x-\")\n</pre> _, ax = plt.subplots()  for i in range(min(len(generated_samples_x), 4)):     ax.plot(generated_samples_x[i, :], \"x-\")  In\u00a0[\u00a0]: Copied! <pre>from openTSNE import TSNE\n</pre> from openTSNE import TSNE In\u00a0[\u00a0]: Copied! <pre>n_tsne_samples = 100\n</pre> n_tsne_samples = 100 In\u00a0[\u00a0]: Copied! <pre>original_samples = pred_batch.cpu().detach().numpy().squeeze()[:n_tsne_samples]\noriginal_samples.shape\n</pre> original_samples = pred_batch.cpu().detach().numpy().squeeze()[:n_tsne_samples] original_samples.shape In\u00a0[\u00a0]: Copied! <pre>tsne = TSNE(\n    perplexity=30,\n    metric=\"euclidean\",\n    n_jobs=8,\n    random_state=42,\n    verbose=True,\n)\n</pre> tsne = TSNE(     perplexity=30,     metric=\"euclidean\",     n_jobs=8,     random_state=42,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>original_samples_embedding = tsne.fit(original_samples)\n</pre> original_samples_embedding = tsne.fit(original_samples) In\u00a0[\u00a0]: Copied! <pre>generated_samples_x[:n_tsne_samples]\n</pre> generated_samples_x[:n_tsne_samples] In\u00a0[\u00a0]: Copied! <pre>generated_samples_embedding = original_samples_embedding.transform(\n    generated_samples_x[:n_tsne_samples]\n)\n</pre> generated_samples_embedding = original_samples_embedding.transform(     generated_samples_x[:n_tsne_samples] ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 7))\n\nax.scatter(\n    original_samples_embedding[:, 0],\n    original_samples_embedding[:, 1],\n    color=\"black\",\n    marker=\".\",\n    label=\"original\",\n)\n\nax.scatter(\n    generated_samples_embedding[:, 0],\n    generated_samples_embedding[:, 1],\n    color=\"red\",\n    marker=\"x\",\n    label=\"generated\",\n)\n\nax.set_title(\"t-SNE of original and generated samples\")\nax.set_xlabel(\"t-SNE 1\")\nax.set_ylabel(\"t-SNE 2\")\n</pre> fig, ax = plt.subplots(figsize=(7, 7))  ax.scatter(     original_samples_embedding[:, 0],     original_samples_embedding[:, 1],     color=\"black\",     marker=\".\",     label=\"original\", )  ax.scatter(     generated_samples_embedding[:, 0],     generated_samples_embedding[:, 1],     color=\"red\",     marker=\"x\",     label=\"generated\", )  ax.set_title(\"t-SNE of original and generated samples\") ax.set_xlabel(\"t-SNE 1\") ax.set_ylabel(\"t-SNE 2\")"}, {"location": "notebooks/time_vae_poison/#timevae", "title": "TimeVAE\u00b6", "text": "<p>Use VAE to generate time series data. In this example, we will train a VAE model to sinusoidal time series data. The overall structure of the model is shown below:</p> <pre><code>mermaid\ngraph TD\ndata[\"Time Series Chunks\"] --&gt; E[Encoder]\n    E --&gt; L[Latent Space]\n    L --&gt; D[Decoder]\n    D --&gt; gen[\"Generated Time Series Chunks\"]\n</code></pre> <p>Reference: https://github.com/wangyz1999/timeVAE-pytorch</p>"}, {"location": "notebooks/time_vae_poison/#data", "title": "Data\u00b6", "text": "<p>We will reuse our classic pendulum dataset.</p>"}, {"location": "notebooks/time_vae_poison/#model", "title": "Model\u00b6", "text": ""}, {"location": "notebooks/time_vae_poison/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/time_vae_poison/#fitted-model", "title": "Fitted Model\u00b6", "text": ""}, {"location": "notebooks/timeseries-comparison/", "title": "Comparing Time Series with Each Other", "text": "In\u00a0[\u00a0]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>from dtaidistance import dtw\nfrom dtaidistance import dtw_visualisation as dtwvis\n\nsns.set_theme()\n\nplt.rcParams.update(\n    {\n        \"font.size\": 18,  # General font size\n        \"axes.titlesize\": 20,  # Title font size\n        \"axes.labelsize\": 16,  # Axis label font size\n        \"xtick.labelsize\": 14,  # X-axis tick label font size\n        \"ytick.labelsize\": 14,  # Y-axis tick label font size\n        \"legend.fontsize\": 14,  # Legend font size\n        \"figure.titlesize\": 20,  # Figure title font size\n    }\n)\n</pre> from dtaidistance import dtw from dtaidistance import dtw_visualisation as dtwvis  sns.set_theme()  plt.rcParams.update(     {         \"font.size\": 18,  # General font size         \"axes.titlesize\": 20,  # Title font size         \"axes.labelsize\": 16,  # Axis label font size         \"xtick.labelsize\": 14,  # X-axis tick label font size         \"ytick.labelsize\": 14,  # Y-axis tick label font size         \"legend.fontsize\": 14,  # Legend font size         \"figure.titlesize\": 20,  # Figure title font size     } ) In\u00a0[\u00a0]: Copied! <pre>t = np.arange(0, 20, 0.1)\nts_original = np.sin(t)\n</pre> t = np.arange(0, 20, 0.1) ts_original = np.sin(t) <p>We apply different transformations to the original time series.</p> In\u00a0[\u00a0]: Copied! <pre>ts_shifted = np.roll(ts_original, 10)\nts_jitter = ts_original + np.random.normal(0, 0.1, len(ts_original))\nts_flipped = ts_original[::-1]\nts_shortened = ts_original[::2]\nts_raise_level = ts_original + 0.5\nts_outlier = ts_original + np.append(np.zeros(len(ts_original) - 1), [10])\n</pre> ts_shifted = np.roll(ts_original, 10) ts_jitter = ts_original + np.random.normal(0, 0.1, len(ts_original)) ts_flipped = ts_original[::-1] ts_shortened = ts_original[::2] ts_raise_level = ts_original + 0.5 ts_outlier = ts_original + np.append(np.zeros(len(ts_original) - 1), [10]) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(\n    {\n        \"t\": t,\n        \"original\": ts_original,\n        \"shifted\": ts_shifted,\n        \"jitter\": ts_jitter,\n        \"flipped\": ts_flipped,\n        \"shortened\": np.pad(\n            ts_shortened, (0, len(ts_original) - len(ts_shortened)), constant_values=0\n        ),\n        \"raise_level\": ts_raise_level,\n        \"outlier\": ts_outlier,\n    }\n)\n</pre> df = pd.DataFrame(     {         \"t\": t,         \"original\": ts_original,         \"shifted\": ts_shifted,         \"jitter\": ts_jitter,         \"flipped\": ts_flipped,         \"shortened\": np.pad(             ts_shortened, (0, len(ts_original) - len(ts_shortened)), constant_values=0         ),         \"raise_level\": ts_raise_level,         \"outlier\": ts_outlier,     } ) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nfor s in df.columns[1:]:\n    sns.lineplot(df, x=\"t\", y=s, ax=ax, label=s)\n</pre> _, ax = plt.subplots()  for s in df.columns[1:]:     sns.lineplot(df, x=\"t\", y=s, ax=ax, label=s) In\u00a0[\u00a0]: Copied! <pre>distances = {\n    \"series\": df.columns[1:],\n}\n\nfor s in df.columns[1:]:\n    distances[\"dtw\"] = distances.get(\"dtw\", []) + [dtw.distance(df.original, df[s])]\n    distances[\"euclidean\"] = distances.get(\"euclidean\", []) + [\n        np.linalg.norm(df.original - df[s])\n    ]\n\n\n_, ax = plt.subplots(figsize=(10, 6.18 * 2), nrows=2)\n\npd.DataFrame(distances).set_index(\"series\").plot.bar(ax=ax[0])\n\ncolors = sns.color_palette(\"husl\", len(distances[\"series\"]))\npd.DataFrame(distances).plot.scatter(x=\"dtw\", y=\"euclidean\", ax=ax[1], c=colors, s=100)\n\nfor i, txt in enumerate(distances[\"series\"]):\n    ax[1].annotate(txt, (distances[\"dtw\"][i], distances[\"euclidean\"][i]), fontsize=12)\n\nax[1].legend(distances[\"series\"], loc=\"best\")\n</pre> distances = {     \"series\": df.columns[1:], }  for s in df.columns[1:]:     distances[\"dtw\"] = distances.get(\"dtw\", []) + [dtw.distance(df.original, df[s])]     distances[\"euclidean\"] = distances.get(\"euclidean\", []) + [         np.linalg.norm(df.original - df[s])     ]   _, ax = plt.subplots(figsize=(10, 6.18 * 2), nrows=2)  pd.DataFrame(distances).set_index(\"series\").plot.bar(ax=ax[0])  colors = sns.color_palette(\"husl\", len(distances[\"series\"])) pd.DataFrame(distances).plot.scatter(x=\"dtw\", y=\"euclidean\", ax=ax[1], c=colors, s=100)  for i, txt in enumerate(distances[\"series\"]):     ax[1].annotate(txt, (distances[\"dtw\"][i], distances[\"euclidean\"][i]), fontsize=12)  ax[1].legend(distances[\"series\"], loc=\"best\") In\u00a0[\u00a0]: Copied! <pre>def dtw_map(s1, s2, window=None):\n    if window is None:\n        window = len(s1)\n    d, paths = dtw.warping_paths(s1, s2, window=window, psi=2)\n    best_path = dtw.best_path(paths)\n\n    return dtwvis.plot_warpingpaths(s1, s2, paths, best_path)\n</pre> def dtw_map(s1, s2, window=None):     if window is None:         window = len(s1)     d, paths = dtw.warping_paths(s1, s2, window=window, psi=2)     best_path = dtw.best_path(paths)      return dtwvis.plot_warpingpaths(s1, s2, paths, best_path) In\u00a0[\u00a0]: Copied! <pre>dtw_map(df.original, df.jitter)\n</pre> dtw_map(df.original, df.jitter) In\u00a0[\u00a0]: Copied! <pre>for s in df.columns[1:]:\n    fig, ax = dtw_map(df.original, df[s])\n    fig.suptitle(s, y=1.05)\n</pre> for s in df.columns[1:]:     fig, ax = dtw_map(df.original, df[s])     fig.suptitle(s, y=1.05) In\u00a0[\u00a0]: Copied! <pre>def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:\n    \"\"\"embed time series into a time delay embedding space\n\n    Time column `t` is required in the input data frame.\n\n    :param df: original time series data frame\n    :param window_size: window size for the time delay embedding\n    \"\"\"\n    dfs_embedded = []\n\n    for i in df.rolling(window_size):\n        i_t = i.t.iloc[0]\n        dfs_embedded.append(\n            pd.DataFrame(i.reset_index(drop=True))\n            .drop(columns=[\"t\"])\n            .T.reset_index()\n            .rename(columns={\"index\": \"name\"})\n            .assign(t=i_t)\n        )\n\n    df_embedded = pd.concat(dfs_embedded[window_size - 1 :])\n\n    return df_embedded\n</pre> def time_delay_embed(df: pd.DataFrame, window_size: int) -&gt; pd.DataFrame:     \"\"\"embed time series into a time delay embedding space      Time column `t` is required in the input data frame.      :param df: original time series data frame     :param window_size: window size for the time delay embedding     \"\"\"     dfs_embedded = []      for i in df.rolling(window_size):         i_t = i.t.iloc[0]         dfs_embedded.append(             pd.DataFrame(i.reset_index(drop=True))             .drop(columns=[\"t\"])             .T.reset_index()             .rename(columns={\"index\": \"name\"})             .assign(t=i_t)         )      df_embedded = pd.concat(dfs_embedded[window_size - 1 :])      return df_embedded In\u00a0[\u00a0]: Copied! <pre>df_embedded_2 = time_delay_embed(df, window_size=2)\n</pre> df_embedded_2 = time_delay_embed(df, window_size=2) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\n(\n    df_embedded_2.loc[df_embedded_2.name == \"original\"].plot.line(\n        x=0, y=1, ax=ax, legend=False\n    )\n)\n(\n    df_embedded_2.loc[df_embedded_2.name == \"original\"].plot.scatter(\n        x=0, y=1, c=\"t\", colormap=\"viridis\", ax=ax\n    )\n)\n</pre> _, ax = plt.subplots()  (     df_embedded_2.loc[df_embedded_2.name == \"original\"].plot.line(         x=0, y=1, ax=ax, legend=False     ) ) (     df_embedded_2.loc[df_embedded_2.name == \"original\"].plot.scatter(         x=0, y=1, c=\"t\", colormap=\"viridis\", ax=ax     ) ) <p>We choose a higher window size to track longer time dependency and for the dimension reduction methods to function properly.</p> In\u00a0[\u00a0]: Copied! <pre>df_embedded = time_delay_embed(df, window_size=5)\n</pre> df_embedded = time_delay_embed(df, window_size=5) In\u00a0[\u00a0]: Copied! <pre>from sklearn.decomposition import PCA\n</pre> from sklearn.decomposition import PCA In\u00a0[\u00a0]: Copied! <pre>pca = PCA(n_components=2)\n</pre> pca = PCA(n_components=2) In\u00a0[\u00a0]: Copied! <pre>df_embedded_pca = pd.concat(\n    [\n        pd.DataFrame(\n            pca.fit_transform(\n                df_embedded.loc[df_embedded.name == n].drop(columns=[\"name\", \"t\"])\n            ),\n            columns=[\"pca_0\", \"pca_1\"],\n        ).assign(name=n)\n        for n in df_embedded.name.unique()\n    ]\n)\n</pre> df_embedded_pca = pd.concat(     [         pd.DataFrame(             pca.fit_transform(                 df_embedded.loc[df_embedded.name == n].drop(columns=[\"name\", \"t\"])             ),             columns=[\"pca_0\", \"pca_1\"],         ).assign(name=n)         for n in df_embedded.name.unique()     ] ) In\u00a0[\u00a0]: Copied! <pre>sns.scatterplot(data=df_embedded_pca, x=\"pca_0\", y=\"pca_1\", hue=\"name\")\n</pre> sns.scatterplot(data=df_embedded_pca, x=\"pca_0\", y=\"pca_1\", hue=\"name\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nsns.scatterplot(\n    data=df_embedded_pca.loc[\n        df_embedded_pca.name.isin(\n            [\"original\", \"jitter\", \"flipped\", \"raise_level\", \"shifted\", \"shortened\"]\n        )\n    ],\n    x=\"pca_0\",\n    y=\"pca_1\",\n    hue=\"name\",\n    style=\"name\",\n    ax=ax,\n)\n\nax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.4), ncol=3)\n</pre> _, ax = plt.subplots()  sns.scatterplot(     data=df_embedded_pca.loc[         df_embedded_pca.name.isin(             [\"original\", \"jitter\", \"flipped\", \"raise_level\", \"shifted\", \"shortened\"]         )     ],     x=\"pca_0\",     y=\"pca_1\",     hue=\"name\",     style=\"name\",     ax=ax, )  ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.4), ncol=3) In\u00a0[\u00a0]: Copied! <pre>from sklearn.manifold import TSNE\n</pre> from sklearn.manifold import TSNE In\u00a0[\u00a0]: Copied! <pre>t_sne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3)\n</pre> t_sne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3) In\u00a0[\u00a0]: Copied! <pre>df_embedded.name.unique()\n</pre> df_embedded.name.unique() In\u00a0[\u00a0]: Copied! <pre>df_embedded_tsne = pd.concat(\n    [\n        pd.DataFrame(\n            t_sne.fit_transform(\n                df_embedded.loc[df_embedded.name == n].drop(columns=[\"name\", \"t\"])\n            ),\n            columns=[\"tsne_0\", \"tsne_1\"],\n        ).assign(name=n)\n        for n in df_embedded.name.unique()\n    ]\n)\n</pre> df_embedded_tsne = pd.concat(     [         pd.DataFrame(             t_sne.fit_transform(                 df_embedded.loc[df_embedded.name == n].drop(columns=[\"name\", \"t\"])             ),             columns=[\"tsne_0\", \"tsne_1\"],         ).assign(name=n)         for n in df_embedded.name.unique()     ] ) In\u00a0[\u00a0]: Copied! <pre>df_embedded_tsne.loc[df_embedded_tsne.name == \"original\"]\n</pre> df_embedded_tsne.loc[df_embedded_tsne.name == \"original\"] In\u00a0[\u00a0]: Copied! <pre>sns.scatterplot(data=df_embedded_tsne, x=\"tsne_0\", y=\"tsne_1\", hue=\"name\")\n</pre> sns.scatterplot(data=df_embedded_tsne, x=\"tsne_0\", y=\"tsne_1\", hue=\"name\") In\u00a0[\u00a0]: Copied! <pre>sns.scatterplot(\n    data=df_embedded_tsne.loc[\n        df_embedded_tsne.name.isin([\"original\", \"jitter\", \"outlier\"])\n    ],\n    x=\"tsne_0\",\n    y=\"tsne_1\",\n    hue=\"name\",\n)\n</pre> sns.scatterplot(     data=df_embedded_tsne.loc[         df_embedded_tsne.name.isin([\"original\", \"jitter\", \"outlier\"])     ],     x=\"tsne_0\",     y=\"tsne_1\",     hue=\"name\", )"}, {"location": "notebooks/timeseries-comparison/#comparing-time-series-with-each-other", "title": "Comparing Time Series with Each Other\u00b6", "text": "<p>Time series data involves a time dimension, and it is not that intuitive to see the difference between two time series. In this notebook, We will show you how to compare time series with each other.</p>"}, {"location": "notebooks/timeseries-comparison/#dtw", "title": "DTW\u00b6", "text": "<p>To illustrate how DTW can be used to compare time series, we will use the following datasets:</p>"}, {"location": "notebooks/timeseries-comparison/#dimension-reduction", "title": "Dimension Reduction\u00b6", "text": "<p>We embed the original time series into a time-delayed embedding space, then reduce the dimensionality of the embedded time series for visualizations.</p>"}, {"location": "notebooks/timeseries-comparison/#pca", "title": "PCA\u00b6", "text": ""}, {"location": "notebooks/timeseries-comparison/#t-sne", "title": "t-SNE\u00b6", "text": ""}, {"location": "notebooks/timeseries_data_box-cox/", "title": "Box-Cox Transformation", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import Any, Dict\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.dataprocessing.transformers import BoxCox\nfrom darts.datasets import AirPassengersDataset\nfrom darts.utils import statistics as dus\n</pre> from typing import Any, Dict  import matplotlib.pyplot as plt import pandas as pd from darts import TimeSeries from darts.dataprocessing.transformers import BoxCox from darts.datasets import AirPassengersDataset from darts.utils import statistics as dus In\u00a0[\u00a0]: Copied! <pre>ap_series = AirPassengersDataset().load()\n\n_, ax = plt.subplots(figsize=(10, 6.18))\nap_series.plot(label=f\"Air Passenger Original Data\", ax=ax)\n</pre> ap_series = AirPassengersDataset().load()  _, ax = plt.subplots(figsize=(10, 6.18)) ap_series.plot(label=f\"Air Passenger Original Data\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\nboxcox_opt = BoxCox()\nap_boxcox_opt_transformed = boxcox_opt.fit_transform(ap_series)\nap_boxcox_opt_transformed.plot(\n    label=f\"$\\lambda={boxcox_opt._fitted_params[0].item():0.3f}$\", ax=ax\n)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  boxcox_opt = BoxCox() ap_boxcox_opt_transformed = boxcox_opt.fit_transform(ap_series) ap_boxcox_opt_transformed.plot(     label=f\"$\\lambda={boxcox_opt._fitted_params[0].item():0.3f}$\", ax=ax ) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\nlmbda = 0.01\nboxcox = BoxCox(lmbda=lmbda)\nboxcox_transformed = boxcox.fit_transform(ap_series)\nboxcox_transformed.plot(label=f\"Box-Cox Transformed Data (lambdax={lmbda})\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18)) lmbda = 0.01 boxcox = BoxCox(lmbda=lmbda) boxcox_transformed = boxcox.fit_transform(ap_series) boxcox_transformed.plot(label=f\"Box-Cox Transformed Data (lambdax={lmbda})\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\nfor lmbda in [0.01, 0.1, 0.2]:\n    boxcox_lmbda = BoxCox(lmbda=lmbda)\n    boxcox_lmbda_transformed = boxcox_lmbda.fit_transform(ap_series)\n    boxcox_lmbda_transformed.plot(\n        label=f\"Box-Cox Transformed Data (lambda={lmbda})\", ax=ax\n    )\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  for lmbda in [0.01, 0.1, 0.2]:     boxcox_lmbda = BoxCox(lmbda=lmbda)     boxcox_lmbda_transformed = boxcox_lmbda.fit_transform(ap_series)     boxcox_lmbda_transformed.plot(         label=f\"Box-Cox Transformed Data (lambda={lmbda})\", ax=ax     ) In\u00a0[\u00a0]: Copied! <pre>def var_series(series: TimeSeries, window: int = 36) -&gt; TimeSeries:\n    series_rolling_var = TimeSeries.from_dataframe(\n        series.pd_dataframe().rolling(window=window).var().dropna()\n    )\n\n    return series_rolling_var\n\n\ndef check_stationary(series: TimeSeries) -&gt; Dict[str, Any]:\n    return {\n        \"is_stationary\": dus.stationarity_tests(series),\n        \"kpss\": dus.stationarity_test_kpss(series),\n        \"adf\": dus.stationarity_test_adf(series),\n    }\n</pre> def var_series(series: TimeSeries, window: int = 36) -&gt; TimeSeries:     series_rolling_var = TimeSeries.from_dataframe(         series.pd_dataframe().rolling(window=window).var().dropna()     )      return series_rolling_var   def check_stationary(series: TimeSeries) -&gt; Dict[str, Any]:     return {         \"is_stationary\": dus.stationarity_tests(series),         \"kpss\": dus.stationarity_test_kpss(series),         \"adf\": dus.stationarity_test_adf(series),     } In\u00a0[\u00a0]: Copied! <pre>boxcox_grid = []\nlmbdas = [0.01, 0.1]\nrolling_window = 12\n\nfor idx, lmbda in enumerate(lmbdas):\n    boxcox_lmbda = BoxCox(lmbda=lmbda)\n    boxcox_lmbda_transformed = boxcox_lmbda.fit_transform(ap_series)\n    var_lmbda_series = var_series(boxcox_lmbda_transformed, window=rolling_window)\n    _, ax = plt.subplots(figsize=(10, 6.18))\n    var_lmbda_series.plot(\n        label=f\"Variance (Rolling Window={rolling_window}) (Box-Cox lambda={lmbda})\",\n        ax=ax,\n    )\n    plt.show()\n</pre> boxcox_grid = [] lmbdas = [0.01, 0.1] rolling_window = 12  for idx, lmbda in enumerate(lmbdas):     boxcox_lmbda = BoxCox(lmbda=lmbda)     boxcox_lmbda_transformed = boxcox_lmbda.fit_transform(ap_series)     var_lmbda_series = var_series(boxcox_lmbda_transformed, window=rolling_window)     _, ax = plt.subplots(figsize=(10, 6.18))     var_lmbda_series.plot(         label=f\"Variance (Rolling Window={rolling_window}) (Box-Cox lambda={lmbda})\",         ax=ax,     )     plt.show() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\nvar_ap_series = var_series(ap_series, window=rolling_window)\nvar_ap_series.plot(label=f\"Variance (Rolling Window={rolling_window})\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18)) var_ap_series = var_series(ap_series, window=rolling_window) var_ap_series.plot(label=f\"Variance (Rolling Window={rolling_window})\", ax=ax)"}, {"location": "notebooks/timeseries_data_box-cox/#box-cox-transformation", "title": "Box-Cox Transformation\u00b6", "text": ""}, {"location": "notebooks/timeseries_data_box-cox/#plot-and-check-variance", "title": "Plot and Check Variance\u00b6", "text": ""}, {"location": "notebooks/timeseries_gan/", "title": "Utilities", "text": "In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\n</pre> import numpy as np import torch import torch.nn as nn import torch.nn.init as init import torch.optim as optim <p>from .data import batch_generator from utils import extract_time, random_generator, NormMinMax from .model import Encoder, Recovery, Generator, Discriminator, Supervisor</p> In\u00a0[\u00a0]: Copied! <pre>def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n    \"\"\"Random vector generation.\n\n    Args:\n      - batch_size: size of the random vector\n      - z_dim: dimension of random vector\n      - T_mb: time information for the random vector\n      - max_seq_len: maximum sequence length\n\n    Returns:\n      - Z_mb: generated random vector\n    \"\"\"\n    Z_mb = list()\n    for i in range(batch_size):\n        temp = np.zeros([max_seq_len, z_dim])\n        temp_Z = np.random.uniform(0.0, 1, [T_mb[i], z_dim])\n        temp[: T_mb[i], :] = temp_Z\n        Z_mb.append(temp_Z)\n    return Z_mb\n</pre> def random_generator(batch_size, z_dim, T_mb, max_seq_len):     \"\"\"Random vector generation.      Args:       - batch_size: size of the random vector       - z_dim: dimension of random vector       - T_mb: time information for the random vector       - max_seq_len: maximum sequence length      Returns:       - Z_mb: generated random vector     \"\"\"     Z_mb = list()     for i in range(batch_size):         temp = np.zeros([max_seq_len, z_dim])         temp_Z = np.random.uniform(0.0, 1, [T_mb[i], z_dim])         temp[: T_mb[i], :] = temp_Z         Z_mb.append(temp_Z)     return Z_mb In\u00a0[\u00a0]: Copied! <pre>def batch_generator(data, time, batch_size):\n    \"\"\"Mini-batch generator.\n\n    Args:\n      - data: time-series data\n      - time: time information\n      - batch_size: the number of samples in each batch\n\n    Returns:\n      - X_mb: time-series data in each batch\n      - T_mb: time information in each batch\n    \"\"\"\n    no = len(data)\n    idx = np.random.permutation(no)\n    train_idx = idx[:batch_size]\n\n    X_mb = list(data[i] for i in train_idx)\n    T_mb = list(time[i] for i in train_idx)\n\n    return X_mb, T_mb\n</pre> def batch_generator(data, time, batch_size):     \"\"\"Mini-batch generator.      Args:       - data: time-series data       - time: time information       - batch_size: the number of samples in each batch      Returns:       - X_mb: time-series data in each batch       - T_mb: time information in each batch     \"\"\"     no = len(data)     idx = np.random.permutation(no)     train_idx = idx[:batch_size]      X_mb = list(data[i] for i in train_idx)     T_mb = list(time[i] for i in train_idx)      return X_mb, T_mb In\u00a0[\u00a0]: Copied! <pre>def NormMinMax(data):\n    \"\"\"Min-Max Normalizer.\n\n    Args:\n      - data: raw data\n\n    Returns:\n      - norm_data: normalized data\n      - min_val: minimum values (for renormalization)\n      - max_val: maximum values (for renormalization)\n    \"\"\"\n    min_val = np.min(np.min(data, axis=0), axis=0)\n    data = data - min_val  # [3661, 24, 6]\n\n    max_val = np.max(np.max(data, axis=0), axis=0)\n    norm_data = data / (max_val + 1e-7)\n\n    return norm_data, min_val, max_val\n</pre> def NormMinMax(data):     \"\"\"Min-Max Normalizer.      Args:       - data: raw data      Returns:       - norm_data: normalized data       - min_val: minimum values (for renormalization)       - max_val: maximum values (for renormalization)     \"\"\"     min_val = np.min(np.min(data, axis=0), axis=0)     data = data - min_val  # [3661, 24, 6]      max_val = np.max(np.max(data, axis=0), axis=0)     norm_data = data / (max_val + 1e-7)      return norm_data, min_val, max_val In\u00a0[\u00a0]: Copied! <pre>def _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear):\n        init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0)\n    elif classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\"Norm\") != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find(\"GRU\") != -1:\n        for name, param in m.named_parameters():\n            if \"weight_ih\" in name:\n                init.xavier_uniform_(param.data)\n            elif \"weight_hh\" in name:\n                init.orthogonal_(param.data)\n            elif \"bias\" in name:\n                param.data.fill_(0)\n\n\nclass Encoder(nn.Module):\n    \"\"\"Embedding network between original feature space to latent space.\n\n    Args:\n      - input: input time-series features. (L, N, X) = (24, ?, 6)\n      - h3: (num_layers, N, H). [3, ?, 24]\n\n    Returns:\n      - H: embeddings\n    \"\"\"\n\n    def __init__(self, opt):\n        super(Encoder, self).__init__()\n        self.rnn = nn.GRU(\n            input_size=opt.z_dim, hidden_size=opt.hidden_dim, num_layers=opt.num_layer\n        )\n        self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(_weights_init)\n\n    def forward(self, input, sigmoid=True):\n        e_outputs, _ = self.rnn(input)\n        H = self.fc(e_outputs)\n        if sigmoid:\n            H = self.sigmoid(H)\n        return H\n\n\nclass Recovery(nn.Module):\n    \"\"\"Recovery network from latent space to original space.\n\n    Args:\n      - H: latent representation\n      - T: input time information\n\n    Returns:\n      - X_tilde: recovered data\n    \"\"\"\n\n    def __init__(self, opt):\n        super(Recovery, self).__init__()\n        self.rnn = nn.GRU(\n            input_size=opt.hidden_dim, hidden_size=opt.z_dim, num_layers=opt.num_layer\n        )\n\n        self.fc = nn.Linear(opt.z_dim, opt.z_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(_weights_init)\n\n    def forward(self, input, sigmoid=True):\n        r_outputs, _ = self.rnn(input)\n        X_tilde = self.fc(r_outputs)\n        if sigmoid:\n            X_tilde = self.sigmoid(X_tilde)\n        return X_tilde\n\n\nclass Generator(nn.Module):\n    \"\"\"Generator function: Generate time-series data in latent space.\n\n    Args:\n      - Z: random variables\n      - T: input time information\n\n    Returns:\n      - E: generated embedding\n    \"\"\"\n\n    def __init__(self, opt):\n        super(Generator, self).__init__()\n        self.rnn = nn.GRU(\n            input_size=opt.z_dim, hidden_size=opt.hidden_dim, num_layers=opt.num_layer\n        )\n        #   self.norm = nn.LayerNorm(opt.hidden_dim)\n        self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(_weights_init)\n\n    def forward(self, input, sigmoid=True):\n        g_outputs, _ = self.rnn(input)\n        #  g_outputs = self.norm(g_outputs)\n        E = self.fc(g_outputs)\n        if sigmoid:\n            E = self.sigmoid(E)\n        return E\n\n\nclass Supervisor(nn.Module):\n    \"\"\"Generate next sequence using the previous sequence.\n\n    Args:\n      - H: latent representation\n      - T: input time information\n\n    Returns:\n      - S: generated sequence based on the latent representations generated by the generator\n    \"\"\"\n\n    def __init__(self, opt):\n        super(Supervisor, self).__init__()\n        self.rnn = nn.GRU(\n            input_size=opt.hidden_dim,\n            hidden_size=opt.hidden_dim,\n            num_layers=opt.num_layer,\n        )\n        #  self.norm = nn.LayerNorm(opt.hidden_dim)\n        self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(_weights_init)\n\n    def forward(self, input, sigmoid=True):\n        s_outputs, _ = self.rnn(input)\n        #  s_outputs = self.norm(s_outputs)\n        S = self.fc(s_outputs)\n        if sigmoid:\n            S = self.sigmoid(S)\n        return S\n\n\nclass Discriminator(nn.Module):\n    \"\"\"Discriminate the original and synthetic time-series data.\n\n    Args:\n      - H: latent representation\n      - T: input time information\n\n    Returns:\n      - Y_hat: classification results between original and synthetic time-series\n    \"\"\"\n\n    def __init__(self, opt):\n        super(Discriminator, self).__init__()\n        self.rnn = nn.GRU(\n            input_size=opt.hidden_dim,\n            hidden_size=opt.hidden_dim,\n            num_layers=opt.num_layer,\n        )\n        #  self.norm = nn.LayerNorm(opt.hidden_dim)\n        self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(_weights_init)\n\n    def forward(self, input, sigmoid=True):\n        d_outputs, _ = self.rnn(input)\n        Y_hat = self.fc(d_outputs)\n        if sigmoid:\n            Y_hat = self.sigmoid(Y_hat)\n        return Y_hat\n</pre> def _weights_init(m):     classname = m.__class__.__name__     if isinstance(m, nn.Linear):         init.xavier_uniform_(m.weight)         m.bias.data.fill_(0)     elif classname.find(\"Conv\") != -1:         m.weight.data.normal_(0.0, 0.02)     elif classname.find(\"Norm\") != -1:         m.weight.data.normal_(1.0, 0.02)         m.bias.data.fill_(0)     elif classname.find(\"GRU\") != -1:         for name, param in m.named_parameters():             if \"weight_ih\" in name:                 init.xavier_uniform_(param.data)             elif \"weight_hh\" in name:                 init.orthogonal_(param.data)             elif \"bias\" in name:                 param.data.fill_(0)   class Encoder(nn.Module):     \"\"\"Embedding network between original feature space to latent space.      Args:       - input: input time-series features. (L, N, X) = (24, ?, 6)       - h3: (num_layers, N, H). [3, ?, 24]      Returns:       - H: embeddings     \"\"\"      def __init__(self, opt):         super(Encoder, self).__init__()         self.rnn = nn.GRU(             input_size=opt.z_dim, hidden_size=opt.hidden_dim, num_layers=opt.num_layer         )         self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)         self.sigmoid = nn.Sigmoid()         self.apply(_weights_init)      def forward(self, input, sigmoid=True):         e_outputs, _ = self.rnn(input)         H = self.fc(e_outputs)         if sigmoid:             H = self.sigmoid(H)         return H   class Recovery(nn.Module):     \"\"\"Recovery network from latent space to original space.      Args:       - H: latent representation       - T: input time information      Returns:       - X_tilde: recovered data     \"\"\"      def __init__(self, opt):         super(Recovery, self).__init__()         self.rnn = nn.GRU(             input_size=opt.hidden_dim, hidden_size=opt.z_dim, num_layers=opt.num_layer         )          self.fc = nn.Linear(opt.z_dim, opt.z_dim)         self.sigmoid = nn.Sigmoid()         self.apply(_weights_init)      def forward(self, input, sigmoid=True):         r_outputs, _ = self.rnn(input)         X_tilde = self.fc(r_outputs)         if sigmoid:             X_tilde = self.sigmoid(X_tilde)         return X_tilde   class Generator(nn.Module):     \"\"\"Generator function: Generate time-series data in latent space.      Args:       - Z: random variables       - T: input time information      Returns:       - E: generated embedding     \"\"\"      def __init__(self, opt):         super(Generator, self).__init__()         self.rnn = nn.GRU(             input_size=opt.z_dim, hidden_size=opt.hidden_dim, num_layers=opt.num_layer         )         #   self.norm = nn.LayerNorm(opt.hidden_dim)         self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)         self.sigmoid = nn.Sigmoid()         self.apply(_weights_init)      def forward(self, input, sigmoid=True):         g_outputs, _ = self.rnn(input)         #  g_outputs = self.norm(g_outputs)         E = self.fc(g_outputs)         if sigmoid:             E = self.sigmoid(E)         return E   class Supervisor(nn.Module):     \"\"\"Generate next sequence using the previous sequence.      Args:       - H: latent representation       - T: input time information      Returns:       - S: generated sequence based on the latent representations generated by the generator     \"\"\"      def __init__(self, opt):         super(Supervisor, self).__init__()         self.rnn = nn.GRU(             input_size=opt.hidden_dim,             hidden_size=opt.hidden_dim,             num_layers=opt.num_layer,         )         #  self.norm = nn.LayerNorm(opt.hidden_dim)         self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)         self.sigmoid = nn.Sigmoid()         self.apply(_weights_init)      def forward(self, input, sigmoid=True):         s_outputs, _ = self.rnn(input)         #  s_outputs = self.norm(s_outputs)         S = self.fc(s_outputs)         if sigmoid:             S = self.sigmoid(S)         return S   class Discriminator(nn.Module):     \"\"\"Discriminate the original and synthetic time-series data.      Args:       - H: latent representation       - T: input time information      Returns:       - Y_hat: classification results between original and synthetic time-series     \"\"\"      def __init__(self, opt):         super(Discriminator, self).__init__()         self.rnn = nn.GRU(             input_size=opt.hidden_dim,             hidden_size=opt.hidden_dim,             num_layers=opt.num_layer,         )         #  self.norm = nn.LayerNorm(opt.hidden_dim)         self.fc = nn.Linear(opt.hidden_dim, opt.hidden_dim)         self.sigmoid = nn.Sigmoid()         self.apply(_weights_init)      def forward(self, input, sigmoid=True):         d_outputs, _ = self.rnn(input)         Y_hat = self.fc(d_outputs)         if sigmoid:             Y_hat = self.sigmoid(Y_hat)         return Y_hat In\u00a0[\u00a0]: Copied! <pre>class BaseModel:\n    \"\"\"Base Model for timegan\"\"\"\n\n    def __init__(self, opt, ori_data):\n        # Seed for deterministic behavior\n        self.seed(opt.manualseed)\n\n        # Initalize variables.\n        self.opt = opt\n        self.ori_data, self.min_val, self.max_val = NormMinMax(ori_data)\n        self.ori_time, self.max_seq_len = extract_time(self.ori_data)\n        self.data_num, _, _ = np.asarray(ori_data).shape  # 3661; 24; 6\n        self.trn_dir = os.path.join(self.opt.outf, self.opt.name, \"train\")\n        self.tst_dir = os.path.join(self.opt.outf, self.opt.name, \"test\")\n        self.device = torch.device(\"cuda:0\" if self.opt.device != \"cpu\" else \"cpu\")\n\n    def seed(self, seed_value):\n        \"\"\"Seed\n\n        Arguments:\n            seed_value {int} -- [description]\n        \"\"\"\n        # Check if seed is default value\n        if seed_value == -1:\n            return\n\n        # Otherwise seed all functionality\n        import random\n\n        random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        np.random.seed(seed_value)\n        torch.backends.cudnn.deterministic = True\n\n    def save_weights(self, epoch):\n        \"\"\"Save net weights for the current epoch.\n\n        Args:\n            epoch ([int]): Current epoch number.\n        \"\"\"\n\n        weight_dir = os.path.join(self.opt.outf, self.opt.name, \"train\", \"weights\")\n        if not os.path.exists(weight_dir):\n            os.makedirs(weight_dir)\n\n        torch.save(\n            {\"epoch\": epoch + 1, \"state_dict\": self.nete.state_dict()},\n            \"%s/netE.pth\" % (weight_dir),\n        )\n        torch.save(\n            {\"epoch\": epoch + 1, \"state_dict\": self.netr.state_dict()},\n            \"%s/netR.pth\" % (weight_dir),\n        )\n        torch.save(\n            {\"epoch\": epoch + 1, \"state_dict\": self.netg.state_dict()},\n            \"%s/netG.pth\" % (weight_dir),\n        )\n        torch.save(\n            {\"epoch\": epoch + 1, \"state_dict\": self.netd.state_dict()},\n            \"%s/netD.pth\" % (weight_dir),\n        )\n        torch.save(\n            {\"epoch\": epoch + 1, \"state_dict\": self.nets.state_dict()},\n            \"%s/netS.pth\" % (weight_dir),\n        )\n\n    def train_one_iter_er(self):\n        \"\"\"Train the model for one epoch.\"\"\"\n\n        self.nete.train()\n        self.netr.train()\n\n        # set mini-batch\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)\n\n        # train encoder &amp; decoder\n        self.optimize_params_er()\n\n    def train_one_iter_er_(self):\n        \"\"\"Train the model for one epoch.\"\"\"\n\n        self.nete.train()\n        self.netr.train()\n\n        # set mini-batch\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)\n\n        # train encoder &amp; decoder\n        self.optimize_params_er_()\n\n    def train_one_iter_s(self):\n        \"\"\"Train the model for one epoch.\"\"\"\n\n        # self.nete.eval()\n        self.nets.train()\n\n        # set mini-batch\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)\n\n        # train superviser\n        self.optimize_params_s()\n\n    def train_one_iter_g(self):\n        \"\"\"Train the model for one epoch.\"\"\"\n\n        \"\"\"self.netr.eval()\n    self.nets.eval()\n    self.netd.eval()\"\"\"\n        self.netg.train()\n\n        # set mini-batch\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)\n        self.Z = random_generator(\n            self.opt.batch_size, self.opt.z_dim, self.T, self.max_seq_len\n        )\n\n        # train superviser\n        self.optimize_params_g()\n\n    def train_one_iter_d(self):\n        \"\"\"Train the model for one epoch.\"\"\"\n        \"\"\"self.nete.eval()\n    self.netr.eval()\n    self.nets.eval()\n    self.netg.eval()\"\"\"\n        self.netd.train()\n\n        # set mini-batch\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)\n        self.Z = random_generator(\n            self.opt.batch_size, self.opt.z_dim, self.T, self.max_seq_len\n        )\n\n        # train superviser\n        self.optimize_params_d()\n\n    def train(self):\n        \"\"\"Train the model\"\"\"\n\n        for iter in range(self.opt.iteration):\n            # Train for one iter\n            self.train_one_iter_er()\n\n            print(\"Encoder training step: \" + str(iter) + \"/\" + str(self.opt.iteration))\n\n        for iter in range(self.opt.iteration):\n            # Train for one iter\n            self.train_one_iter_s()\n\n            print(\n                \"Superviser training step: \" + str(iter) + \"/\" + str(self.opt.iteration)\n            )\n\n        for iter in range(self.opt.iteration):\n            # Train for one iter\n            for kk in range(2):\n                self.train_one_iter_g()\n                self.train_one_iter_er_()\n\n            self.train_one_iter_d()\n\n            print(\n                \"Superviser training step: \" + str(iter) + \"/\" + str(self.opt.iteration)\n            )\n\n        self.save_weights(self.opt.iteration)\n        self.generated_data = self.generation(self.opt.batch_size)\n        print(\"Finish Synthetic Data Generation\")\n\n    #  self.evaluation()\n\n    \"\"\"def evaluation(self):\n    ## Performance metrics\n    # Output initialization\n    metric_results = dict()\n\n    # 1. Discriminative Score\n    discriminative_score = list()\n    for _ in range(self.opt.metric_iteration):\n      temp_disc = discriminative_score_metrics(self.ori_data, self.generated_data)\n      discriminative_score.append(temp_disc)\n\n    metric_results['discriminative'] = np.mean(discriminative_score)\n\n    # 2. Predictive score\n    predictive_score = list()\n    for tt in range(self.opt.metric_iteration):\n      temp_pred = predictive_score_metrics(self.ori_data, self.generated_data)\n      predictive_score.append(temp_pred)\n\n    metric_results['predictive'] = np.mean(predictive_score)\n\n    # 3. Visualization (PCA and tSNE)\n    visualization(self.ori_data, self.generated_data, 'pca')\n    visualization(self.ori_data, self.generated_data, 'tsne')\n\n    ## Print discriminative and predictive scores\n    print(metric_results)\n\"\"\"\n\n    def generation(self, num_samples, mean=0.0, std=1.0):\n        if num_samples == 0:\n            return None, None\n        ## Synthetic data generation\n        self.X0, self.T = batch_generator(\n            self.ori_data, self.ori_time, self.opt.batch_size\n        )\n        self.Z = random_generator(\n            num_samples, self.opt.z_dim, self.T, self.max_seq_len  # , mean, std\n        )\n        self.Z = torch.tensor(self.Z, dtype=torch.float32).to(self.device)\n        self.E_hat = self.netg(self.Z)  # [?, 24, 24]\n        self.H_hat = self.nets(self.E_hat)  # [?, 24, 24]\n        generated_data_curr = (\n            self.netr(self.H_hat).cpu().detach().numpy()\n        )  # [?, 24, 24]\n\n        generated_data = list()\n        for i in range(num_samples):\n            temp = generated_data_curr[i, : self.ori_time[i], :]\n            generated_data.append(temp)\n\n        # Renormalization\n        generated_data = generated_data * self.max_val\n        generated_data = generated_data + self.min_val\n        return generated_data\n</pre>   class BaseModel:     \"\"\"Base Model for timegan\"\"\"      def __init__(self, opt, ori_data):         # Seed for deterministic behavior         self.seed(opt.manualseed)          # Initalize variables.         self.opt = opt         self.ori_data, self.min_val, self.max_val = NormMinMax(ori_data)         self.ori_time, self.max_seq_len = extract_time(self.ori_data)         self.data_num, _, _ = np.asarray(ori_data).shape  # 3661; 24; 6         self.trn_dir = os.path.join(self.opt.outf, self.opt.name, \"train\")         self.tst_dir = os.path.join(self.opt.outf, self.opt.name, \"test\")         self.device = torch.device(\"cuda:0\" if self.opt.device != \"cpu\" else \"cpu\")      def seed(self, seed_value):         \"\"\"Seed          Arguments:             seed_value {int} -- [description]         \"\"\"         # Check if seed is default value         if seed_value == -1:             return          # Otherwise seed all functionality         import random          random.seed(seed_value)         torch.manual_seed(seed_value)         torch.cuda.manual_seed_all(seed_value)         np.random.seed(seed_value)         torch.backends.cudnn.deterministic = True      def save_weights(self, epoch):         \"\"\"Save net weights for the current epoch.          Args:             epoch ([int]): Current epoch number.         \"\"\"          weight_dir = os.path.join(self.opt.outf, self.opt.name, \"train\", \"weights\")         if not os.path.exists(weight_dir):             os.makedirs(weight_dir)          torch.save(             {\"epoch\": epoch + 1, \"state_dict\": self.nete.state_dict()},             \"%s/netE.pth\" % (weight_dir),         )         torch.save(             {\"epoch\": epoch + 1, \"state_dict\": self.netr.state_dict()},             \"%s/netR.pth\" % (weight_dir),         )         torch.save(             {\"epoch\": epoch + 1, \"state_dict\": self.netg.state_dict()},             \"%s/netG.pth\" % (weight_dir),         )         torch.save(             {\"epoch\": epoch + 1, \"state_dict\": self.netd.state_dict()},             \"%s/netD.pth\" % (weight_dir),         )         torch.save(             {\"epoch\": epoch + 1, \"state_dict\": self.nets.state_dict()},             \"%s/netS.pth\" % (weight_dir),         )      def train_one_iter_er(self):         \"\"\"Train the model for one epoch.\"\"\"          self.nete.train()         self.netr.train()          # set mini-batch         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)          # train encoder &amp; decoder         self.optimize_params_er()      def train_one_iter_er_(self):         \"\"\"Train the model for one epoch.\"\"\"          self.nete.train()         self.netr.train()          # set mini-batch         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)          # train encoder &amp; decoder         self.optimize_params_er_()      def train_one_iter_s(self):         \"\"\"Train the model for one epoch.\"\"\"          # self.nete.eval()         self.nets.train()          # set mini-batch         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)          # train superviser         self.optimize_params_s()      def train_one_iter_g(self):         \"\"\"Train the model for one epoch.\"\"\"          \"\"\"self.netr.eval()     self.nets.eval()     self.netd.eval()\"\"\"         self.netg.train()          # set mini-batch         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)         self.Z = random_generator(             self.opt.batch_size, self.opt.z_dim, self.T, self.max_seq_len         )          # train superviser         self.optimize_params_g()      def train_one_iter_d(self):         \"\"\"Train the model for one epoch.\"\"\"         \"\"\"self.nete.eval()     self.netr.eval()     self.nets.eval()     self.netg.eval()\"\"\"         self.netd.train()          # set mini-batch         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.X = torch.tensor(self.X0, dtype=torch.float32).to(self.device)         self.Z = random_generator(             self.opt.batch_size, self.opt.z_dim, self.T, self.max_seq_len         )          # train superviser         self.optimize_params_d()      def train(self):         \"\"\"Train the model\"\"\"          for iter in range(self.opt.iteration):             # Train for one iter             self.train_one_iter_er()              print(\"Encoder training step: \" + str(iter) + \"/\" + str(self.opt.iteration))          for iter in range(self.opt.iteration):             # Train for one iter             self.train_one_iter_s()              print(                 \"Superviser training step: \" + str(iter) + \"/\" + str(self.opt.iteration)             )          for iter in range(self.opt.iteration):             # Train for one iter             for kk in range(2):                 self.train_one_iter_g()                 self.train_one_iter_er_()              self.train_one_iter_d()              print(                 \"Superviser training step: \" + str(iter) + \"/\" + str(self.opt.iteration)             )          self.save_weights(self.opt.iteration)         self.generated_data = self.generation(self.opt.batch_size)         print(\"Finish Synthetic Data Generation\")      #  self.evaluation()      \"\"\"def evaluation(self):     ## Performance metrics     # Output initialization     metric_results = dict()      # 1. Discriminative Score     discriminative_score = list()     for _ in range(self.opt.metric_iteration):       temp_disc = discriminative_score_metrics(self.ori_data, self.generated_data)       discriminative_score.append(temp_disc)      metric_results['discriminative'] = np.mean(discriminative_score)      # 2. Predictive score     predictive_score = list()     for tt in range(self.opt.metric_iteration):       temp_pred = predictive_score_metrics(self.ori_data, self.generated_data)       predictive_score.append(temp_pred)      metric_results['predictive'] = np.mean(predictive_score)      # 3. Visualization (PCA and tSNE)     visualization(self.ori_data, self.generated_data, 'pca')     visualization(self.ori_data, self.generated_data, 'tsne')      ## Print discriminative and predictive scores     print(metric_results) \"\"\"      def generation(self, num_samples, mean=0.0, std=1.0):         if num_samples == 0:             return None, None         ## Synthetic data generation         self.X0, self.T = batch_generator(             self.ori_data, self.ori_time, self.opt.batch_size         )         self.Z = random_generator(             num_samples, self.opt.z_dim, self.T, self.max_seq_len  # , mean, std         )         self.Z = torch.tensor(self.Z, dtype=torch.float32).to(self.device)         self.E_hat = self.netg(self.Z)  # [?, 24, 24]         self.H_hat = self.nets(self.E_hat)  # [?, 24, 24]         generated_data_curr = (             self.netr(self.H_hat).cpu().detach().numpy()         )  # [?, 24, 24]          generated_data = list()         for i in range(num_samples):             temp = generated_data_curr[i, : self.ori_time[i], :]             generated_data.append(temp)          # Renormalization         generated_data = generated_data * self.max_val         generated_data = generated_data + self.min_val         return generated_data In\u00a0[\u00a0]: Copied! <pre>class TimeGAN(BaseModel):\n    \"\"\"TimeGAN Class\"\"\"\n\n    @property\n    def name(self):\n        return \"TimeGAN\"\n\n    def __init__(self, opt, ori_data):\n        super(TimeGAN, self).__init__(opt, ori_data)\n\n        # -- Misc attributes\n        self.epoch = 0\n        self.times = []\n        self.total_steps = 0\n\n        # Create and initialize networks.\n        self.nete = Encoder(self.opt).to(self.device)\n        self.netr = Recovery(self.opt).to(self.device)\n        self.netg = Generator(self.opt).to(self.device)\n        self.netd = Discriminator(self.opt).to(self.device)\n        self.nets = Supervisor(self.opt).to(self.device)\n\n        if self.opt.resume != \"\":\n            print(\"\\nLoading pre-trained networks.\")\n            self.opt.iter = torch.load(os.path.join(self.opt.resume, \"netG.pth\"))[\n                \"epoch\"\n            ]\n            self.nete.load_state_dict(\n                torch.load(os.path.join(self.opt.resume, \"netE.pth\"))[\"state_dict\"]\n            )\n            self.netr.load_state_dict(\n                torch.load(os.path.join(self.opt.resume, \"netR.pth\"))[\"state_dict\"]\n            )\n            self.netg.load_state_dict(\n                torch.load(os.path.join(self.opt.resume, \"netG.pth\"))[\"state_dict\"]\n            )\n            self.netd.load_state_dict(\n                torch.load(os.path.join(self.opt.resume, \"netD.pth\"))[\"state_dict\"]\n            )\n            self.nets.load_state_dict(\n                torch.load(os.path.join(self.opt.resume, \"netS.pth\"))[\"state_dict\"]\n            )\n            print(\"\\tDone.\\n\")\n\n        # loss\n        self.l_mse = nn.MSELoss()\n        self.l_r = nn.L1Loss()\n        self.l_bce = nn.BCELoss()\n\n        # Setup optimizer\n        if self.opt.isTrain:\n            self.nete.train()\n            self.netr.train()\n            self.netg.train()\n            self.netd.train()\n            self.nets.train()\n            self.optimizer_e = optim.Adam(\n                self.nete.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)\n            )\n            self.optimizer_r = optim.Adam(\n                self.netr.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)\n            )\n            self.optimizer_g = optim.Adam(\n                self.netg.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)\n            )\n            self.optimizer_d = optim.Adam(\n                self.netd.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)\n            )\n            self.optimizer_s = optim.Adam(\n                self.nets.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)\n            )\n\n    def forward_e(self):\n        \"\"\"Forward propagate through netE\"\"\"\n        self.H = self.nete(self.X)\n\n    def forward_er(self):\n        \"\"\"Forward propagate through netR\"\"\"\n        self.H = self.nete(self.X)\n        self.X_tilde = self.netr(self.H)\n\n    def forward_g(self):\n        \"\"\"Forward propagate through netG\"\"\"\n        self.Z = torch.tensor(self.Z, dtype=torch.float32).to(self.device)\n        self.E_hat = self.netg(self.Z)\n\n    def forward_dg(self):\n        \"\"\"Forward propagate through netD\"\"\"\n        self.Y_fake = self.netd(self.H_hat)\n        self.Y_fake_e = self.netd(self.E_hat)\n\n    def forward_rg(self):\n        \"\"\"Forward propagate through netG\"\"\"\n        self.X_hat = self.netr(self.H_hat)\n\n    def forward_s(self):\n        \"\"\"Forward propagate through netS\"\"\"\n        self.H_supervise = self.nets(self.H)\n        # print(self.H, self.H_supervise)\n\n    def forward_sg(self):\n        \"\"\"Forward propagate through netS\"\"\"\n        self.H_hat = self.nets(self.E_hat)\n\n    def forward_d(self):\n        \"\"\"Forward propagate through netD\"\"\"\n        self.Y_real = self.netd(self.H)\n        self.Y_fake = self.netd(self.H_hat)\n        self.Y_fake_e = self.netd(self.E_hat)\n\n    def backward_er(self):\n        \"\"\"Backpropagate through netE\"\"\"\n        self.err_er = self.l_mse(self.X_tilde, self.X)\n        self.err_er.backward(retain_graph=True)\n        print(\"Loss: \", self.err_er)\n\n    def backward_er_(self):\n        \"\"\"Backpropagate through netE\"\"\"\n        self.err_er_ = self.l_mse(self.X_tilde, self.X)\n        self.err_s = self.l_mse(self.H_supervise[:, :-1, :], self.H[:, 1:, :])\n        self.err_er = 10 * torch.sqrt(self.err_er_) + 0.1 * self.err_s\n        self.err_er.backward(retain_graph=True)\n\n    #  print(\"Loss: \", self.err_er_, self.err_s)\n    def backward_g(self):\n        \"\"\"Backpropagate through netG\"\"\"\n        self.err_g_U = self.l_bce(self.Y_fake, torch.ones_like(self.Y_fake))\n\n        self.err_g_U_e = self.l_bce(self.Y_fake_e, torch.ones_like(self.Y_fake_e))\n        self.err_g_V1 = torch.mean(\n            torch.abs(\n                torch.sqrt(torch.std(self.X_hat, [0])[1] + 1e-6)\n                - torch.sqrt(torch.std(self.X, [0])[1] + 1e-6)\n            )\n        )  # |a^2 - b^2|\n        self.err_g_V2 = torch.mean(\n            torch.abs((torch.mean(self.X_hat, [0])[0]) - (torch.mean(self.X, [0])[0]))\n        )  # |a - b|\n        self.err_s = self.l_mse(self.H_supervise[:, :-1, :], self.H[:, 1:, :])\n        self.err_g = (\n            self.err_g_U\n            + self.err_g_U_e * self.opt.w_gamma\n            + self.err_g_V1 * self.opt.w_g\n            + self.err_g_V2 * self.opt.w_g\n            + torch.sqrt(self.err_s)\n        )\n        self.err_g.backward(retain_graph=True)\n        print(\"Loss G: \", self.err_g)\n\n    def backward_s(self):\n        \"\"\"Backpropagate through netS\"\"\"\n        self.err_s = self.l_mse(self.H[:, 1:, :], self.H_supervise[:, :-1, :])\n        self.err_s.backward(retain_graph=True)\n        print(\"Loss S: \", self.err_s)\n\n    #   print(torch.autograd.grad(self.err_s, self.nets.parameters()))\n\n    def backward_d(self):\n        \"\"\"Backpropagate through netD\"\"\"\n        self.err_d_real = self.l_bce(self.Y_real, torch.ones_like(self.Y_real))\n        self.err_d_fake = self.l_bce(self.Y_fake, torch.zeros_like(self.Y_fake))\n        self.err_d_fake_e = self.l_bce(self.Y_fake_e, torch.zeros_like(self.Y_fake_e))\n        self.err_d = (\n            self.err_d_real + self.err_d_fake + self.err_d_fake_e * self.opt.w_gamma\n        )\n        if self.err_d &gt; 0.15:\n            self.err_d.backward(retain_graph=True)\n\n    # print(\"Loss D: \", self.err_d)\n\n    def optimize_params_er(self):\n        \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"\n        # Forward-pass\n        self.forward_er()\n\n        # Backward-pass\n        # nete &amp; netr\n        self.optimizer_e.zero_grad()\n        self.optimizer_r.zero_grad()\n        self.backward_er()\n        self.optimizer_e.step()\n        self.optimizer_r.step()\n\n    def optimize_params_er_(self):\n        \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"\n        # Forward-pass\n        self.forward_er()\n        self.forward_s()\n        # Backward-pass\n        # nete &amp; netr\n        self.optimizer_e.zero_grad()\n        self.optimizer_r.zero_grad()\n        self.backward_er_()\n        self.optimizer_e.step()\n        self.optimizer_r.step()\n\n    def optimize_params_s(self):\n        \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"\n        # Forward-pass\n        self.forward_e()\n        self.forward_s()\n\n        # Backward-pass\n        # nets\n        self.optimizer_s.zero_grad()\n        self.backward_s()\n        self.optimizer_s.step()\n\n    def optimize_params_g(self):\n        \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"\n        # Forward-pass\n        self.forward_e()\n        self.forward_s()\n        self.forward_g()\n        self.forward_sg()\n        self.forward_rg()\n        self.forward_dg()\n\n        # Backward-pass\n        # nets\n        self.optimizer_g.zero_grad()\n        self.optimizer_s.zero_grad()\n        self.backward_g()\n        self.optimizer_g.step()\n        self.optimizer_s.step()\n\n    def optimize_params_d(self):\n        \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"\n        # Forward-pass\n        self.forward_e()\n        self.forward_g()\n        self.forward_sg()\n        self.forward_d()\n        self.forward_dg()\n\n        # Backward-pass\n        # nets\n        self.optimizer_d.zero_grad()\n        self.backward_d()\n        self.optimizer_d.step()\n</pre> class TimeGAN(BaseModel):     \"\"\"TimeGAN Class\"\"\"      @property     def name(self):         return \"TimeGAN\"      def __init__(self, opt, ori_data):         super(TimeGAN, self).__init__(opt, ori_data)          # -- Misc attributes         self.epoch = 0         self.times = []         self.total_steps = 0          # Create and initialize networks.         self.nete = Encoder(self.opt).to(self.device)         self.netr = Recovery(self.opt).to(self.device)         self.netg = Generator(self.opt).to(self.device)         self.netd = Discriminator(self.opt).to(self.device)         self.nets = Supervisor(self.opt).to(self.device)          if self.opt.resume != \"\":             print(\"\\nLoading pre-trained networks.\")             self.opt.iter = torch.load(os.path.join(self.opt.resume, \"netG.pth\"))[                 \"epoch\"             ]             self.nete.load_state_dict(                 torch.load(os.path.join(self.opt.resume, \"netE.pth\"))[\"state_dict\"]             )             self.netr.load_state_dict(                 torch.load(os.path.join(self.opt.resume, \"netR.pth\"))[\"state_dict\"]             )             self.netg.load_state_dict(                 torch.load(os.path.join(self.opt.resume, \"netG.pth\"))[\"state_dict\"]             )             self.netd.load_state_dict(                 torch.load(os.path.join(self.opt.resume, \"netD.pth\"))[\"state_dict\"]             )             self.nets.load_state_dict(                 torch.load(os.path.join(self.opt.resume, \"netS.pth\"))[\"state_dict\"]             )             print(\"\\tDone.\\n\")          # loss         self.l_mse = nn.MSELoss()         self.l_r = nn.L1Loss()         self.l_bce = nn.BCELoss()          # Setup optimizer         if self.opt.isTrain:             self.nete.train()             self.netr.train()             self.netg.train()             self.netd.train()             self.nets.train()             self.optimizer_e = optim.Adam(                 self.nete.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)             )             self.optimizer_r = optim.Adam(                 self.netr.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)             )             self.optimizer_g = optim.Adam(                 self.netg.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)             )             self.optimizer_d = optim.Adam(                 self.netd.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)             )             self.optimizer_s = optim.Adam(                 self.nets.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999)             )      def forward_e(self):         \"\"\"Forward propagate through netE\"\"\"         self.H = self.nete(self.X)      def forward_er(self):         \"\"\"Forward propagate through netR\"\"\"         self.H = self.nete(self.X)         self.X_tilde = self.netr(self.H)      def forward_g(self):         \"\"\"Forward propagate through netG\"\"\"         self.Z = torch.tensor(self.Z, dtype=torch.float32).to(self.device)         self.E_hat = self.netg(self.Z)      def forward_dg(self):         \"\"\"Forward propagate through netD\"\"\"         self.Y_fake = self.netd(self.H_hat)         self.Y_fake_e = self.netd(self.E_hat)      def forward_rg(self):         \"\"\"Forward propagate through netG\"\"\"         self.X_hat = self.netr(self.H_hat)      def forward_s(self):         \"\"\"Forward propagate through netS\"\"\"         self.H_supervise = self.nets(self.H)         # print(self.H, self.H_supervise)      def forward_sg(self):         \"\"\"Forward propagate through netS\"\"\"         self.H_hat = self.nets(self.E_hat)      def forward_d(self):         \"\"\"Forward propagate through netD\"\"\"         self.Y_real = self.netd(self.H)         self.Y_fake = self.netd(self.H_hat)         self.Y_fake_e = self.netd(self.E_hat)      def backward_er(self):         \"\"\"Backpropagate through netE\"\"\"         self.err_er = self.l_mse(self.X_tilde, self.X)         self.err_er.backward(retain_graph=True)         print(\"Loss: \", self.err_er)      def backward_er_(self):         \"\"\"Backpropagate through netE\"\"\"         self.err_er_ = self.l_mse(self.X_tilde, self.X)         self.err_s = self.l_mse(self.H_supervise[:, :-1, :], self.H[:, 1:, :])         self.err_er = 10 * torch.sqrt(self.err_er_) + 0.1 * self.err_s         self.err_er.backward(retain_graph=True)      #  print(\"Loss: \", self.err_er_, self.err_s)     def backward_g(self):         \"\"\"Backpropagate through netG\"\"\"         self.err_g_U = self.l_bce(self.Y_fake, torch.ones_like(self.Y_fake))          self.err_g_U_e = self.l_bce(self.Y_fake_e, torch.ones_like(self.Y_fake_e))         self.err_g_V1 = torch.mean(             torch.abs(                 torch.sqrt(torch.std(self.X_hat, [0])[1] + 1e-6)                 - torch.sqrt(torch.std(self.X, [0])[1] + 1e-6)             )         )  # |a^2 - b^2|         self.err_g_V2 = torch.mean(             torch.abs((torch.mean(self.X_hat, [0])[0]) - (torch.mean(self.X, [0])[0]))         )  # |a - b|         self.err_s = self.l_mse(self.H_supervise[:, :-1, :], self.H[:, 1:, :])         self.err_g = (             self.err_g_U             + self.err_g_U_e * self.opt.w_gamma             + self.err_g_V1 * self.opt.w_g             + self.err_g_V2 * self.opt.w_g             + torch.sqrt(self.err_s)         )         self.err_g.backward(retain_graph=True)         print(\"Loss G: \", self.err_g)      def backward_s(self):         \"\"\"Backpropagate through netS\"\"\"         self.err_s = self.l_mse(self.H[:, 1:, :], self.H_supervise[:, :-1, :])         self.err_s.backward(retain_graph=True)         print(\"Loss S: \", self.err_s)      #   print(torch.autograd.grad(self.err_s, self.nets.parameters()))      def backward_d(self):         \"\"\"Backpropagate through netD\"\"\"         self.err_d_real = self.l_bce(self.Y_real, torch.ones_like(self.Y_real))         self.err_d_fake = self.l_bce(self.Y_fake, torch.zeros_like(self.Y_fake))         self.err_d_fake_e = self.l_bce(self.Y_fake_e, torch.zeros_like(self.Y_fake_e))         self.err_d = (             self.err_d_real + self.err_d_fake + self.err_d_fake_e * self.opt.w_gamma         )         if self.err_d &gt; 0.15:             self.err_d.backward(retain_graph=True)      # print(\"Loss D: \", self.err_d)      def optimize_params_er(self):         \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"         # Forward-pass         self.forward_er()          # Backward-pass         # nete &amp; netr         self.optimizer_e.zero_grad()         self.optimizer_r.zero_grad()         self.backward_er()         self.optimizer_e.step()         self.optimizer_r.step()      def optimize_params_er_(self):         \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"         # Forward-pass         self.forward_er()         self.forward_s()         # Backward-pass         # nete &amp; netr         self.optimizer_e.zero_grad()         self.optimizer_r.zero_grad()         self.backward_er_()         self.optimizer_e.step()         self.optimizer_r.step()      def optimize_params_s(self):         \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"         # Forward-pass         self.forward_e()         self.forward_s()          # Backward-pass         # nets         self.optimizer_s.zero_grad()         self.backward_s()         self.optimizer_s.step()      def optimize_params_g(self):         \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"         # Forward-pass         self.forward_e()         self.forward_s()         self.forward_g()         self.forward_sg()         self.forward_rg()         self.forward_dg()          # Backward-pass         # nets         self.optimizer_g.zero_grad()         self.optimizer_s.zero_grad()         self.backward_g()         self.optimizer_g.step()         self.optimizer_s.step()      def optimize_params_d(self):         \"\"\"Forwardpass, Loss Computation and Backwardpass.\"\"\"         # Forward-pass         self.forward_e()         self.forward_g()         self.forward_sg()         self.forward_d()         self.forward_dg()          # Backward-pass         # nets         self.optimizer_d.zero_grad()         self.backward_d()         self.optimizer_d.step() In\u00a0[\u00a0]: Copied! <pre>def sine_data_generation(no, seq_len, dim):\n    \"\"\"Sine data generation.\n\n    Args:\n      - no: the number of samples\n      - seq_len: sequence length of the time-series\n      - dim: feature dimensions\n\n    Returns:\n      - data: generated data\n    \"\"\"\n    # Initialize the output\n    data = list()\n\n    # Generate sine data\n    for i in range(no):\n        # Initialize each time-series\n        temp = list()\n        # For each feature\n        for k in range(dim):\n            # Randomly drawn frequency and phase\n            freq = np.random.uniform(0, 0.1)\n            phase = np.random.uniform(0, 0.1)\n\n            # Generate sine signal based on the drawn frequency and phase\n            temp_data = [np.sin(freq * j + phase) for j in range(seq_len)]\n            temp.append(temp_data)\n\n        # Align row/column\n        temp = np.transpose(np.asarray(temp))\n        # Normalize to [0,1]\n        temp = (temp + 1) * 0.5\n        # Stack the generated data\n        data.append(temp)\n\n    return data\n</pre> def sine_data_generation(no, seq_len, dim):     \"\"\"Sine data generation.      Args:       - no: the number of samples       - seq_len: sequence length of the time-series       - dim: feature dimensions      Returns:       - data: generated data     \"\"\"     # Initialize the output     data = list()      # Generate sine data     for i in range(no):         # Initialize each time-series         temp = list()         # For each feature         for k in range(dim):             # Randomly drawn frequency and phase             freq = np.random.uniform(0, 0.1)             phase = np.random.uniform(0, 0.1)              # Generate sine signal based on the drawn frequency and phase             temp_data = [np.sin(freq * j + phase) for j in range(seq_len)]             temp.append(temp_data)          # Align row/column         temp = np.transpose(np.asarray(temp))         # Normalize to [0,1]         temp = (temp + 1) * 0.5         # Stack the generated data         data.append(temp)      return data In\u00a0[\u00a0]: Copied! <pre>def extract_time(data):\n    \"\"\"Returns Maximum sequence length and each sequence length.\n\n    Args:\n      - data: original data\n\n    Returns:\n      - time: extracted time information\n      - max_seq_len: maximum sequence length\n    \"\"\"\n    time = list()\n    max_seq_len = 0\n    for i in range(len(data)):\n        max_seq_len = max(max_seq_len, len(data[i][:, 0]))\n        time.append(len(data[i][:, 0]))\n\n    return time, max_seq_len\n</pre>   def extract_time(data):     \"\"\"Returns Maximum sequence length and each sequence length.      Args:       - data: original data      Returns:       - time: extracted time information       - max_seq_len: maximum sequence length     \"\"\"     time = list()     max_seq_len = 0     for i in range(len(data)):         max_seq_len = max(max_seq_len, len(data[i][:, 0]))         time.append(len(data[i][:, 0]))      return time, max_seq_len In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass ModelParams:\n    z_dim: int\n    seq_len: int\n    module: str\n    hidden_dim: int\n    num_layer: int\n    iteration: int\n    batch_size: int\n    metric_iteration: int\n    manualseed: int\n    outf: str\n    name: str\n    device: str\n    resume: str\n    isTrain: bool = True\n    lr: float = 0.0002\n    beta1: float = 0.9\n    w_gamma: float = 1\n    w_es: float = 0.1\n    w_e0: float = 10\n    w_g: float = 100\n\n\nmodel_params = {\n    \"z_dim\": 6,\n    \"seq_len\": 24,\n    \"module\": \"gru\",\n    \"hidden_dim\": 24,\n    \"num_layer\": 3,\n    \"iteration\": 100,\n    \"batch_size\": 128,\n    \"metric_iteration\": 10,\n    \"manualseed\": 42,\n    \"outf\": \"tmp\",\n    \"name\": \"timegan\",\n    \"device\": \"cpu\",\n    \"resume\": \"\",\n}\n</pre> @dataclasses.dataclass class ModelParams:     z_dim: int     seq_len: int     module: str     hidden_dim: int     num_layer: int     iteration: int     batch_size: int     metric_iteration: int     manualseed: int     outf: str     name: str     device: str     resume: str     isTrain: bool = True     lr: float = 0.0002     beta1: float = 0.9     w_gamma: float = 1     w_es: float = 0.1     w_e0: float = 10     w_g: float = 100   model_params = {     \"z_dim\": 6,     \"seq_len\": 24,     \"module\": \"gru\",     \"hidden_dim\": 24,     \"num_layer\": 3,     \"iteration\": 100,     \"batch_size\": 128,     \"metric_iteration\": 10,     \"manualseed\": 42,     \"outf\": \"tmp\",     \"name\": \"timegan\",     \"device\": \"cpu\",     \"resume\": \"\", } In\u00a0[\u00a0]: Copied! <pre>data = sine_data_generation(3661, 24, 1)\n</pre> data = sine_data_generation(3661, 24, 1) In\u00a0[\u00a0]: Copied! <pre>model = TimeGAN(ModelParams(**model_params), data)\n</pre>  model = TimeGAN(ModelParams(**model_params), data) In\u00a0[\u00a0]: Copied! <pre>model.train()\n</pre> model.train() In\u00a0[\u00a0]: Copied! <pre>model.generated_data.shape\n</pre> model.generated_data.shape In\u00a0[\u00a0]: Copied! <pre>model.generated_data\n</pre> model.generated_data In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>data[0]\n</pre> data[0] In\u00a0[\u00a0]: Copied! <pre>for i in range(len(data)):\n    plt.plot(data[i].shape, \".\")\n</pre> for i in range(len(data)):     plt.plot(data[i].shape, \".\")"}, {"location": "notebooks/timeseries_gan/#utilities", "title": "Utilities\u00b6", "text": ""}, {"location": "notebooks/timeseries_gan/#train", "title": "Train\u00b6", "text": ""}, {"location": "notebooks/transformer-explainer/", "title": "Transformer Explainer", "text": "In\u00a0[\u00a0]: Copied! <pre>import math\n</pre> import math In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\nimport torch\nfrom torch import nn\n</pre> import matplotlib.pyplot as plt import numpy as np import scipy as sp import torch from torch import nn In\u00a0[\u00a0]: Copied! <pre>x = np.array([1, 3])\n</pre> x = np.array([1, 3]) In\u00a0[\u00a0]: Copied! <pre>qk = np.outer(x, x)\nsp.special.expit(qk)\n</pre> qk = np.outer(x, x) sp.special.expit(qk) In\u00a0[\u00a0]: Copied! <pre>sp.special.expit(qk).dot(x)\n</pre> sp.special.expit(qk).dot(x) In\u00a0[\u00a0]: Copied! <pre>sp.special.expit(np.outer([1, 1], [1, 1]))\n</pre> sp.special.expit(np.outer([1, 1], [1, 1])) In\u00a0[\u00a0]: Copied! <pre>x = np.array([1, 10])\n</pre> x = np.array([1, 10]) In\u00a0[\u00a0]: Copied! <pre>def compare_a(a, x):\n    return a, sp.special.expit(a), sp.special.expit(a).dot(x)\n</pre> def compare_a(a, x):     return a, sp.special.expit(a), sp.special.expit(a).dot(x) In\u00a0[\u00a0]: Copied! <pre>a_1 = np.array([[1, 0], [0, 1]])\ncompare_a(a_1, x)\n</pre> a_1 = np.array([[1, 0], [0, 1]]) compare_a(a_1, x) In\u00a0[\u00a0]: Copied! <pre>a_2 = np.array([[1, -10], [-10, 1]])\ncompare_a(a_2, x)\n</pre> a_2 = np.array([[1, -10], [-10, 1]]) compare_a(a_2, x) In\u00a0[\u00a0]: Copied! <pre>a_3 = np.array([[-10, 1], [1, -10]])\ncompare_a(a_3, x)\n</pre> a_3 = np.array([[-10, 1], [1, -10]]) compare_a(a_3, x) In\u00a0[\u00a0]: Copied! <pre>atten_similarity_q = np.array([[1], [10]])\natten_similarity_k = np.array([[1], [10]])\n\natten_similarity_sim = sp.special.expit(\n    np.outer(atten_similarity_q, atten_similarity_k)\n)\n\n\nfor a in [atten_similarity_q, atten_similarity_k, atten_similarity_sim]:\n    fig, ax = plt.subplots(figsize=(7, 7))\n\n    ax.matshow(a)\n\n    for (i, j), z in np.ndenumerate(a):\n        ax.text(\n            j,\n            i,\n            \"{:0.1f}\".format(z),\n            ha=\"center\",\n            va=\"center\",\n            bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"0.3\"),\n            fontsize=\"xx-large\",\n        )\n\n    ax.set_axis_off()\n</pre> atten_similarity_q = np.array([[1], [10]]) atten_similarity_k = np.array([[1], [10]])  atten_similarity_sim = sp.special.expit(     np.outer(atten_similarity_q, atten_similarity_k) )   for a in [atten_similarity_q, atten_similarity_k, atten_similarity_sim]:     fig, ax = plt.subplots(figsize=(7, 7))      ax.matshow(a)      for (i, j), z in np.ndenumerate(a):         ax.text(             j,             i,             \"{:0.1f}\".format(z),             ha=\"center\",             va=\"center\",             bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"0.3\"),             fontsize=\"xx-large\",         )      ax.set_axis_off() In\u00a0[\u00a0]: Copied! <pre>import math\n</pre> import math In\u00a0[\u00a0]: Copied! <pre>class PositionalEncodingSimple:\n    \"\"\"Positional encoding for our transformer\n    written in numpy.\n\n    :param d_model: hidden dimension of the encoder\n    :param max_len: maximum length of our positional\n        encoder. The encoder can not encode sequence\n        length longer than max_len.\n    \"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 100):\n        position = np.expand_dims(np.arange(max_len), axis=1)\n        div_term = np.exp(np.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        self.pe = np.zeros((max_len, d_model))\n        self.pe[:, 0::2] = np.sin(position * div_term)\n        self.pe[:, 1::2] = np.cos(position * div_term)\n\n    def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        :param x: input to be encoded\n            with shape\n            `(batch_size, sequence_length, embedding_dim)`\n        \"\"\"\n        return self.pe[: x.shape[1]]\n\n\npes = PositionalEncodingSimple(d_model=50)\nx_pes_in = np.ones((1, 10, 1))\n\nx_pes_out = pes(x=x_pes_in)\n\n_, ax = plt.subplots(figsize=(10, 6.18))\n\nax.matshow(x_pes_out, cmap=\"cividis\")\nax.set_xlabel(\"Embedding\")\nax.set_ylabel(\"Temporal\")\n</pre> class PositionalEncodingSimple:     \"\"\"Positional encoding for our transformer     written in numpy.      :param d_model: hidden dimension of the encoder     :param max_len: maximum length of our positional         encoder. The encoder can not encode sequence         length longer than max_len.     \"\"\"      def __init__(self, d_model: int, max_len: int = 100):         position = np.expand_dims(np.arange(max_len), axis=1)         div_term = np.exp(np.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))         self.pe = np.zeros((max_len, d_model))         self.pe[:, 0::2] = np.sin(position * div_term)         self.pe[:, 1::2] = np.cos(position * div_term)      def __call__(self, x: np.ndarray) -&gt; np.ndarray:         \"\"\"         :param x: input to be encoded             with shape             `(batch_size, sequence_length, embedding_dim)`         \"\"\"         return self.pe[: x.shape[1]]   pes = PositionalEncodingSimple(d_model=50) x_pes_in = np.ones((1, 10, 1))  x_pes_out = pes(x=x_pes_in)  _, ax = plt.subplots(figsize=(10, 6.18))  ax.matshow(x_pes_out, cmap=\"cividis\") ax.set_xlabel(\"Embedding\") ax.set_ylabel(\"Temporal\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\nax.plot(x_pes_out[-1, :])\nax.plot(x_pes_out[0, :])\n</pre> _, ax = plt.subplots()  ax.plot(x_pes_out[-1, :]) ax.plot(x_pes_out[0, :]) <p>Positional encoding in nixtla</p> In\u00a0[\u00a0]: Copied! <pre>class PositionalEmbedding(nn.Module):\n    def __init__(self, hidden_size, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, hidden_size).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (\n            torch.arange(0, hidden_size, 2).float() * -(math.log(10000.0) / hidden_size)\n        ).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        return self.pe[:, : x.size(1)]\n</pre> class PositionalEmbedding(nn.Module):     def __init__(self, hidden_size, max_len=5000):         super(PositionalEmbedding, self).__init__()         # Compute the positional encodings once in log space.         pe = torch.zeros(max_len, hidden_size).float()         pe.require_grad = False          position = torch.arange(0, max_len).float().unsqueeze(1)         div_term = (             torch.arange(0, hidden_size, 2).float() * -(math.log(10000.0) / hidden_size)         ).exp()          pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)          pe = pe.unsqueeze(0)         self.register_buffer(\"pe\", pe)      def forward(self, x):         return self.pe[:, : x.size(1)] In\u00a0[\u00a0]: Copied! <pre>pe = PositionalEmbedding(hidden_size=192, max_len=20)\n</pre> pe = PositionalEmbedding(hidden_size=192, max_len=20) In\u00a0[\u00a0]: Copied! <pre>plt.plot((torch.arange(0, 192, 2).float() * -(math.log(10000.0) / 192)).exp().numpy())\n</pre> plt.plot((torch.arange(0, 192, 2).float() * -(math.log(10000.0) / 192)).exp().numpy()) <p>Token Embedding</p> In\u00a0[\u00a0]: Copied! <pre>class TokenEmbedding(nn.Module):\n    def __init__(self, c_in, hidden_size):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ &gt;= \"1.5.0\" else 2\n        self.tokenConv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=hidden_size,\n            kernel_size=3,\n            padding=padding,\n            padding_mode=\"circular\",\n            bias=False,\n        )\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n                )\n\n    def forward(self, x):\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n</pre> class TokenEmbedding(nn.Module):     def __init__(self, c_in, hidden_size):         super(TokenEmbedding, self).__init__()         padding = 1 if torch.__version__ &gt;= \"1.5.0\" else 2         self.tokenConv = nn.Conv1d(             in_channels=c_in,             out_channels=hidden_size,             kernel_size=3,             padding=padding,             padding_mode=\"circular\",             bias=False,         )         for m in self.modules():             if isinstance(m, nn.Conv1d):                 nn.init.kaiming_normal_(                     m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\"                 )      def forward(self, x):         x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)         return x In\u00a0[\u00a0]: Copied! <pre>x_te_in = torch.ones((1, 10, 1))\n</pre> x_te_in = torch.ones((1, 10, 1)) In\u00a0[\u00a0]: Copied! <pre>te = TokenEmbedding(c_in=1, hidden_size=4)\n\nx_te = te(x_te_in)\n\nx_te.shape\n</pre> te = TokenEmbedding(c_in=1, hidden_size=4)  x_te = te(x_te_in)  x_te.shape In\u00a0[\u00a0]: Copied! <pre>te_pe = PositionalEmbedding(hidden_size=4, max_len=20)\n\nte_pe(x)\n</pre> te_pe = PositionalEmbedding(hidden_size=4, max_len=20)  te_pe(x) In\u00a0[\u00a0]: Copied! <pre>from neuralforecast.common._modules import DataEmbedding\n</pre> from neuralforecast.common._modules import DataEmbedding In\u00a0[\u00a0]: Copied! <pre>class TriangularCausalMask:\n    def __init__(self, B, L, device=\"cpu\"):\n        mask_shape = [B, 1, L, L]\n        with torch.no_grad():\n            self._mask = torch.triu(\n                torch.ones(mask_shape, dtype=torch.bool), diagonal=1\n            ).to(device)\n\n    @property\n    def mask(self):\n        return self._mask\n\n\nclass FullAttention(nn.Module):\n    \"\"\"\n    FullAttention\n    \"\"\"\n\n    def __init__(\n        self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False\n    ):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1.0 / math.sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe-&gt;bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd-&gt;blhd\", A, values)\n\n        if self.output_attention:\n            return (V.contiguous(), A)\n        else:\n            return (V.contiguous(), None)\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, attention, hidden_size, n_head, d_keys=None, d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (hidden_size // n_head)\n        d_values = d_values or (hidden_size // n_head)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.key_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.value_projection = nn.Linear(hidden_size, d_values * n_head)\n        self.out_projection = nn.Linear(d_values * n_head, hidden_size)\n        self.n_head = n_head\n\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_head\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n        # out = out.view(B, L, -1)\n\n        # return self.out_projection(out), attn\n        return out, attn\n</pre> class TriangularCausalMask:     def __init__(self, B, L, device=\"cpu\"):         mask_shape = [B, 1, L, L]         with torch.no_grad():             self._mask = torch.triu(                 torch.ones(mask_shape, dtype=torch.bool), diagonal=1             ).to(device)      @property     def mask(self):         return self._mask   class FullAttention(nn.Module):     \"\"\"     FullAttention     \"\"\"      def __init__(         self, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=False     ):         super(FullAttention, self).__init__()         self.scale = scale         self.mask_flag = mask_flag         self.output_attention = output_attention         self.dropout = nn.Dropout(attention_dropout)      def forward(self, queries, keys, values, attn_mask):         B, L, H, E = queries.shape         _, S, _, D = values.shape         scale = self.scale or 1.0 / math.sqrt(E)          scores = torch.einsum(\"blhe,bshe-&gt;bhls\", queries, keys)          if self.mask_flag:             if attn_mask is None:                 attn_mask = TriangularCausalMask(B, L, device=queries.device)              scores.masked_fill_(attn_mask.mask, -np.inf)          A = self.dropout(torch.softmax(scale * scores, dim=-1))         V = torch.einsum(\"bhls,bshd-&gt;blhd\", A, values)          if self.output_attention:             return (V.contiguous(), A)         else:             return (V.contiguous(), None)   class AttentionLayer(nn.Module):     def __init__(self, attention, hidden_size, n_head, d_keys=None, d_values=None):         super(AttentionLayer, self).__init__()          d_keys = d_keys or (hidden_size // n_head)         d_values = d_values or (hidden_size // n_head)          self.inner_attention = attention         self.query_projection = nn.Linear(hidden_size, d_keys * n_head)         self.key_projection = nn.Linear(hidden_size, d_keys * n_head)         self.value_projection = nn.Linear(hidden_size, d_values * n_head)         self.out_projection = nn.Linear(d_values * n_head, hidden_size)         self.n_head = n_head      def forward(self, queries, keys, values, attn_mask):         B, L, _ = queries.shape         _, S, _ = keys.shape         H = self.n_head          queries = self.query_projection(queries).view(B, L, H, -1)         keys = self.key_projection(keys).view(B, S, H, -1)         values = self.value_projection(values).view(B, S, H, -1)          out, attn = self.inner_attention(queries, keys, values, attn_mask)         # out = out.view(B, L, -1)          # return self.out_projection(out), attn         return out, attn In\u00a0[\u00a0]: Copied! <pre>enc_in = 1\nhidden_size = 4\n\nenc_embedding = DataEmbedding(\n    c_in=enc_in, exog_input_size=0, hidden_size=hidden_size, pos_embedding=True\n)\n</pre> enc_in = 1 hidden_size = 4  enc_embedding = DataEmbedding(     c_in=enc_in, exog_input_size=0, hidden_size=hidden_size, pos_embedding=True ) In\u00a0[\u00a0]: Copied! <pre>x = torch.ones(size=(1, 10, 1))  # batch size: 1, history length 10, variables 1\n</pre> x = torch.ones(size=(1, 10, 1))  # batch size: 1, history length 10, variables 1 In\u00a0[\u00a0]: Copied! <pre>x_embedded = enc_embedding(x)\nx_embedded.shape\n</pre> x_embedded = enc_embedding(x) x_embedded.shape In\u00a0[\u00a0]: Copied! <pre>attention = FullAttention(mask_flag=False, output_attention=True)\nattention_layer = AttentionLayer(\n    attention,\n    hidden_size=hidden_size,\n    n_head=1,\n)\n</pre> attention = FullAttention(mask_flag=False, output_attention=True) attention_layer = AttentionLayer(     attention,     hidden_size=hidden_size,     n_head=1, ) In\u00a0[\u00a0]: Copied! <pre>attention_layer.query_projection(x_embedded).view(1, 10, 1, -1).shape\n</pre> attention_layer.query_projection(x_embedded).view(1, 10, 1, -1).shape In\u00a0[\u00a0]: Copied! <pre>al_out, al_att = attention_layer(x_embedded, x_embedded, x_embedded, attn_mask=False)\n\nal_out.shape\n</pre> al_out, al_att = attention_layer(x_embedded, x_embedded, x_embedded, attn_mask=False)  al_out.shape In\u00a0[\u00a0]: Copied! <pre>queries = attention_layer.query_projection(x_embedded).view(1, 10, 1, -1)\nkeys = attention_layer.key_projection(x_embedded).view(1, 10, 1, -1)\n\ntorch.einsum(\"blhe,bshe-&gt;bhls\", queries, keys).shape\n</pre> queries = attention_layer.query_projection(x_embedded).view(1, 10, 1, -1) keys = attention_layer.key_projection(x_embedded).view(1, 10, 1, -1)  torch.einsum(\"blhe,bshe-&gt;bhls\", queries, keys).shape In\u00a0[\u00a0]: Copied! <pre>queries.shape\n</pre> queries.shape In\u00a0[\u00a0]: Copied! <pre>class DataEmbedding_inverted(nn.Module):\n    \"\"\"\n    DataEmbedding_inverted\n    \"\"\"\n\n    def __init__(self, c_in, hidden_size, dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, hidden_size)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            # the potential to take covariates (e.g. timestamps) as tokens\n            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))\n        # x: [Batch Variate hidden_size]\n        return self.dropout(x)\n</pre> class DataEmbedding_inverted(nn.Module):     \"\"\"     DataEmbedding_inverted     \"\"\"      def __init__(self, c_in, hidden_size, dropout=0.1):         super(DataEmbedding_inverted, self).__init__()         self.value_embedding = nn.Linear(c_in, hidden_size)         self.dropout = nn.Dropout(p=dropout)      def forward(self, x, x_mark):         x = x.permute(0, 2, 1)         # x: [Batch Variate Time]         if x_mark is None:             x = self.value_embedding(x)         else:             # the potential to take covariates (e.g. timestamps) as tokens             x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))         # x: [Batch Variate hidden_size]         return self.dropout(x) In\u00a0[\u00a0]: Copied! <pre>i_de = DataEmbedding_inverted(10, hidden_size)\n\ni_enc_out = i_de(x, None)\n\ni_enc_out.shape\n</pre> i_de = DataEmbedding_inverted(10, hidden_size)  i_enc_out = i_de(x, None)  i_enc_out.shape"}, {"location": "notebooks/transformer-explainer/#transformer-explainer", "title": "Transformer Explainer\u00b6", "text": ""}, {"location": "notebooks/transformer-explainer/#examples", "title": "Examples\u00b6", "text": ""}, {"location": "notebooks/transformer-explainer/#the-a-matrix", "title": "The A Matrix\u00b6", "text": "<p>Some examples of the A matrix.</p>"}, {"location": "notebooks/transformer-explainer/#visualize", "title": "Visualize\u00b6", "text": "<p>We plot out the q,k vectors and the corresponding attention.</p>"}, {"location": "notebooks/transformer-explainer/#positional-encoding", "title": "Positional Encoding\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/", "title": "Transformer Forecaster with neuralforecast", "text": "In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"NIXTLA_ID_AS_COL\"] = \"1\"\n\nimport datetime\nfrom typing import Optional\n\nimport pandas as pd\nfrom loguru import logger\n</pre> import os  os.environ[\"NIXTLA_ID_AS_COL\"] = \"1\"  import datetime from typing import Optional  import pandas as pd from loguru import logger  In\u00a0[\u00a0]: Copied! <pre>data_source = \"https://raw.githubusercontent.com/datumorphism/dataset-m5-simplified/b486cd6a3e183b80016a91ba0fd9b19493cdc587/dataset/m5_store_sales.csv\"\n</pre> data_source = \"https://raw.githubusercontent.com/datumorphism/dataset-m5-simplified/b486cd6a3e183b80016a91ba0fd9b19493cdc587/dataset/m5_store_sales.csv\" In\u00a0[\u00a0]: Copied! <pre>df = (\n    pd.read_csv(data_source, parse_dates=[\"date\"])\n    .rename(columns={\"date\": \"ds\", \"CA\": \"y\"})[[\"ds\", \"y\"]]\n    .assign(unique_id=1)\n)\n# -\n</pre> df = (     pd.read_csv(data_source, parse_dates=[\"date\"])     .rename(columns={\"date\": \"ds\", \"CA\": \"y\"})[[\"ds\", \"y\"]]     .assign(unique_id=1) ) # - In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nsns.set_theme(context=\"paper\", style=\"ticks\", palette=\"colorblind\")\n\n\n_, ax = plt.subplots(figsize=(10, 13), nrows=3)\n\nsns.lineplot(df, x=\"ds\", y=\"y\", color=\"k\", ax=ax[0])\n\nax[0].set_title(\"Walmart Sales in CA\")\nax[0].set_ylabel(\"Sales\")\n\nsns.lineplot(\n    (df.loc[(df.ds &gt;= \"2015-01-01\") &amp; (df.ds &lt; \"2016-01-01\")]),\n    x=\"ds\",\n    y=\"y\",\n    color=\"k\",\n    ax=ax[1],\n)\n\nax[1].set_title(\"Walmart Sales in CA in 2015\")\nax[1].set_ylabel(\"Sales\")\n\nplot_acf(df.y, ax=ax[2], color=\"k\")\n</pre> import matplotlib.pyplot as plt import seaborn as sns from statsmodels.graphics.tsaplots import plot_acf  sns.set_theme(context=\"paper\", style=\"ticks\", palette=\"colorblind\")   _, ax = plt.subplots(figsize=(10, 13), nrows=3)  sns.lineplot(df, x=\"ds\", y=\"y\", color=\"k\", ax=ax[0])  ax[0].set_title(\"Walmart Sales in CA\") ax[0].set_ylabel(\"Sales\")  sns.lineplot(     (df.loc[(df.ds &gt;= \"2015-01-01\") &amp; (df.ds &lt; \"2016-01-01\")]),     x=\"ds\",     y=\"y\",     color=\"k\",     ax=ax[1], )  ax[1].set_title(\"Walmart Sales in CA in 2015\") ax[1].set_ylabel(\"Sales\")  plot_acf(df.y, ax=ax[2], color=\"k\") In\u00a0[\u00a0]: Copied! <pre>df.ds.max(), df.ds.min()\n</pre> df.ds.max(), df.ds.min() In\u00a0[\u00a0]: Copied! <pre>train_test_split_date = \"2016-01-01\"\n\ndf_train = df[df.ds &lt; train_test_split_date]\ndf_test = df[(df.ds &gt;= train_test_split_date)]\n\nhorizon = 3\nhorizon\n</pre> train_test_split_date = \"2016-01-01\"  df_train = df[df.ds &lt; train_test_split_date] df_test = df[(df.ds &gt;= train_test_split_date)]  horizon = 3 horizon In\u00a0[\u00a0]: Copied! <pre>from statsforecast import StatsForecast\nfrom statsforecast.arima import arima_string\nfrom statsforecast.models import AutoARIMA\n</pre> from statsforecast import StatsForecast from statsforecast.arima import arima_string from statsforecast.models import AutoARIMA In\u00a0[\u00a0]: Copied! <pre>sf = StatsForecast(\n    models=[\n        AutoARIMA(\n            # season_length = 365.25\n            seasonal=False\n        )\n    ],\n    freq=\"d\",\n)\n</pre> sf = StatsForecast(     models=[         AutoARIMA(             # season_length = 365.25             seasonal=False         )     ],     freq=\"d\", ) In\u00a0[\u00a0]: Copied! <pre>sf.fit(df_train)\n</pre> sf.fit(df_train) In\u00a0[\u00a0]: Copied! <pre>def forecast_test(\n    forecaster_pred: callable,\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate forecasts for tests\n\n    :param forecaster_pred: forecasting function/method\n    :param df_train: train dataframe\n    :param df_test: test dataframe\n    \"\"\"\n    dfs_pred = []\n    for i in range(len(df_test)):\n        logger.debug(f\"Prediction Step: {i}\")\n        df_pred_input_i = pd.concat([df_train, df_test[:i]]).reset_index(drop=True)\n        df_pred_output_i = forecaster_pred(df_pred_input_i)\n        df_pred_output_i[\"step\"] = i\n        dfs_pred.append(df_pred_output_i)\n    df_y_hat = pd.concat(dfs_pred)\n\n    return df_y_hat\n</pre> def forecast_test(     forecaster_pred: callable,     df_train: pd.DataFrame,     df_test: pd.DataFrame, ) -&gt; pd.DataFrame:     \"\"\"Generate forecasts for tests      :param forecaster_pred: forecasting function/method     :param df_train: train dataframe     :param df_test: test dataframe     \"\"\"     dfs_pred = []     for i in range(len(df_test)):         logger.debug(f\"Prediction Step: {i}\")         df_pred_input_i = pd.concat([df_train, df_test[:i]]).reset_index(drop=True)         df_pred_output_i = forecaster_pred(df_pred_input_i)         df_pred_output_i[\"step\"] = i         dfs_pred.append(df_pred_output_i)     df_y_hat = pd.concat(dfs_pred)      return df_y_hat In\u00a0[\u00a0]: Copied! <pre>def visualize_predictions(\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame,\n    df_pred: pd.DataFrame,\n    model_name: str,\n    n_ahead: Optional[int] = None,\n    title: Optional[str] = None,\n    ax: Optional[plt.axes] = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the forecasts\n\n    :param df_train: train dataframe\n    :param df_test: test dataframe\n    :param df_pred: prediction dataframe\n    :param model_name: which model to visualize\n    :param n_ahead: which future step to visualze\n    :param title: title of the chart\n    :param ax: matplotlib axes\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(10, 6.18))\n\n    sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)\n\n    sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)\n\n    if n_ahead is None:\n        sns.lineplot(\n            df_pred, x=\"ds\", y=model_name, hue=\"step\", linestyle=\"dashed\", ax=ax\n        )\n    else:\n        dfs_pred_n_ahead = []\n        for i in df_pred.step.unique():\n            dfs_pred_n_ahead.append(\n                df_pred.loc[df_pred.step == i].iloc[n_ahead - 1 : n_ahead]\n            )\n        df_pred_n_ahead = pd.concat(dfs_pred_n_ahead)\n        sns.lineplot(df_pred_n_ahead, x=\"ds\", y=model_name, linestyle=\"dashed\", ax=ax)\n\n    ax.set_title(\"Sales Forecast\" if title is None else title)\n    ax.set_ylabel(\"Sales\")\n    ax.set_xlabel(\"Date\")\n    # ax.legend(prop={'size': 15})\n    # ax.grid()\n</pre> def visualize_predictions(     df_train: pd.DataFrame,     df_test: pd.DataFrame,     df_pred: pd.DataFrame,     model_name: str,     n_ahead: Optional[int] = None,     title: Optional[str] = None,     ax: Optional[plt.axes] = None, ) -&gt; None:     \"\"\"     Visualizes the forecasts      :param df_train: train dataframe     :param df_test: test dataframe     :param df_pred: prediction dataframe     :param model_name: which model to visualize     :param n_ahead: which future step to visualze     :param title: title of the chart     :param ax: matplotlib axes     \"\"\"     if ax is None:         _, ax = plt.subplots(figsize=(10, 6.18))      sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)      sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)      if n_ahead is None:         sns.lineplot(             df_pred, x=\"ds\", y=model_name, hue=\"step\", linestyle=\"dashed\", ax=ax         )     else:         dfs_pred_n_ahead = []         for i in df_pred.step.unique():             dfs_pred_n_ahead.append(                 df_pred.loc[df_pred.step == i].iloc[n_ahead - 1 : n_ahead]             )         df_pred_n_ahead = pd.concat(dfs_pred_n_ahead)         sns.lineplot(df_pred_n_ahead, x=\"ds\", y=model_name, linestyle=\"dashed\", ax=ax)      ax.set_title(\"Sales Forecast\" if title is None else title)     ax.set_ylabel(\"Sales\")     ax.set_xlabel(\"Date\")     # ax.legend(prop={'size': 15})     # ax.grid() In\u00a0[\u00a0]: Copied! <pre>df_y_hat_arima = forecast_test(\n    lambda x: sf.forecast(df=x, h=horizon), df_train=df_train, df_test=df_test\n)\n\ndf_y_hat_arima\n</pre> df_y_hat_arima = forecast_test(     lambda x: sf.forecast(df=x, h=horizon), df_train=df_train, df_test=df_test )  df_y_hat_arima In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train, df_test=df_test, df_pred=df_y_hat_arima, model_name=\"AutoARIMA\"\n)\n</pre> visualize_predictions(     df_train=df_train, df_test=df_test, df_pred=df_y_hat_arima, model_name=\"AutoARIMA\" ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_arima,\n    model_name=\"AutoARIMA\",\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_arima,     model_name=\"AutoARIMA\", ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_arima,\n    model_name=\"AutoARIMA\",\n    n_ahead=1,\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_arima,     model_name=\"AutoARIMA\",     n_ahead=1, ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_arima,\n    model_name=\"AutoARIMA\",\n    n_ahead=2,\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_arima,     model_name=\"AutoARIMA\",     n_ahead=2, ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_arima,\n    model_name=\"AutoARIMA\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (ARIMA)\",\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_arima,     model_name=\"AutoARIMA\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (ARIMA)\", ) In\u00a0[\u00a0]: Copied! <pre>arima_string(sf.fitted_[0, 0].model_)\n</pre> arima_string(sf.fitted_[0, 0].model_) In\u00a0[\u00a0]: Copied! <pre>from neuralforecast import NeuralForecast\nfrom neuralforecast.models import VanillaTransformer, iTransformer\n</pre> from neuralforecast import NeuralForecast from neuralforecast.models import VanillaTransformer, iTransformer In\u00a0[\u00a0]: Copied! <pre>models = [\n    VanillaTransformer(\n        hidden_size=128,\n        n_head=4,\n        learning_rate=0.0001,\n        scaler_type=\"robust\",\n        max_steps=500,\n        batch_size=32,\n        windows_batch_size=512,\n        random_seed=16,\n        input_size=30,\n        step_size=3,\n        h=horizon,\n        # **{'hidden_size': 128,\n        #     'n_head': 4,\n        #     'learning_rate': 0.00010614524276500768,\n        #     'scaler_type': 'robust',\n        #     'max_steps': 500,\n        #     'batch_size': 32,\n        #     'windows_batch_size': 512,\n        #     'random_seed': 16,\n        #     'input_size': 30,\n        #     'step_size': 3,\n        #     \"h\": horizon,\n        # }\n    ),\n    iTransformer(\n        input_size=30,\n        h=horizon,\n        max_steps=50,\n        n_series=1,\n    ),\n]\nnf = NeuralForecast(models=models, freq=\"d\")\n</pre> models = [     VanillaTransformer(         hidden_size=128,         n_head=4,         learning_rate=0.0001,         scaler_type=\"robust\",         max_steps=500,         batch_size=32,         windows_batch_size=512,         random_seed=16,         input_size=30,         step_size=3,         h=horizon,         # **{'hidden_size': 128,         #     'n_head': 4,         #     'learning_rate': 0.00010614524276500768,         #     'scaler_type': 'robust',         #     'max_steps': 500,         #     'batch_size': 32,         #     'windows_batch_size': 512,         #     'random_seed': 16,         #     'input_size': 30,         #     'step_size': 3,         #     \"h\": horizon,         # }     ),     iTransformer(         input_size=30,         h=horizon,         max_steps=50,         n_series=1,     ), ] nf = NeuralForecast(models=models, freq=\"d\") In\u00a0[\u00a0]: Copied! <pre>nf.fit(df=df_train)\n</pre> nf.fit(df=df_train) In\u00a0[\u00a0]: Copied! <pre>df_y_hat_transformer = forecast_test(nf.predict, df_train=df_train, df_test=df_test)\n</pre> df_y_hat_transformer = forecast_test(nf.predict, df_train=df_train, df_test=df_test) In\u00a0[\u00a0]: Copied! <pre>df_y_hat_transformer\n</pre> df_y_hat_transformer In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>df_y_hat_transformer.ds.dt.freq\n</pre> df_y_hat_transformer.ds.dt.freq In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train,\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"VanillaTransformer\",\n)\n</pre> visualize_predictions(     df_train=df_train,     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"VanillaTransformer\", ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"VanillaTransformer\",\n    n_ahead=1,\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"VanillaTransformer\",     n_ahead=1, ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"VanillaTransformer\",\n    n_ahead=2,\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"VanillaTransformer\",     n_ahead=2, ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"VanillaTransformer\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (VanillaTransformer)\",\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"VanillaTransformer\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (VanillaTransformer)\", ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"iTransformer\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (iTransformer)\",\n)\n</pre> visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"iTransformer\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (iTransformer)\", ) In\u00a0[\u00a0]: Copied! <pre>from datasetsforecast.evaluation import accuracy\nfrom datasetsforecast.losses import mae, mape, mse, rmse, smape\n</pre> from datasetsforecast.evaluation import accuracy from datasetsforecast.losses import mae, mape, mse, rmse, smape In\u00a0[\u00a0]: Copied! <pre>df_y_hat_transformer.loc[df_y_hat_transformer.step == 0]\n</pre> df_y_hat_transformer.loc[df_y_hat_transformer.step == 0] In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>df_y_hat_transformer.loc[df_y_hat_transformer.step == 0]\n</pre> df_y_hat_transformer.loc[df_y_hat_transformer.step == 0] In\u00a0[\u00a0]: Copied! <pre>transformer_evals = []\n\nfor s in df_y_hat_transformer.step.unique():\n    df_transformer_eval_s = accuracy(\n        Y_hat_df=df_y_hat_transformer.loc[df_y_hat_transformer.step == s],\n        Y_test_df=df_test,\n        Y_df=df_train,\n        metrics=[mse, mae, rmse],\n        # agg_by=['unique_id']\n    )\n    df_transformer_eval_s[\"step\"] = s\n    transformer_evals.append(df_transformer_eval_s)\n\ndf_transformer_eval = pd.concat(transformer_evals)\n\ndf_transformer_eval.head()\n</pre> transformer_evals = []  for s in df_y_hat_transformer.step.unique():     df_transformer_eval_s = accuracy(         Y_hat_df=df_y_hat_transformer.loc[df_y_hat_transformer.step == s],         Y_test_df=df_test,         Y_df=df_train,         metrics=[mse, mae, rmse],         # agg_by=['unique_id']     )     df_transformer_eval_s[\"step\"] = s     transformer_evals.append(df_transformer_eval_s)  df_transformer_eval = pd.concat(transformer_evals)  df_transformer_eval.head() In\u00a0[\u00a0]: Copied! <pre>baseline_evals = []\n\nfor s in df_y_hat_arima.step.unique():\n    df_baseline_eval_s = accuracy(\n        Y_hat_df=df_y_hat_arima.loc[df_y_hat_arima.step == s],\n        Y_test_df=df_test,\n        Y_df=df_train,\n        metrics=[mse, mae, rmse],\n        # agg_by=['unique_id']\n    )\n    df_baseline_eval_s[\"step\"] = s\n    baseline_evals.append(df_baseline_eval_s)\n\n\ndf_baseline_eval = pd.concat(baseline_evals)\n\ndf_baseline_eval.head()\n</pre> baseline_evals = []  for s in df_y_hat_arima.step.unique():     df_baseline_eval_s = accuracy(         Y_hat_df=df_y_hat_arima.loc[df_y_hat_arima.step == s],         Y_test_df=df_test,         Y_df=df_train,         metrics=[mse, mae, rmse],         # agg_by=['unique_id']     )     df_baseline_eval_s[\"step\"] = s     baseline_evals.append(df_baseline_eval_s)   df_baseline_eval = pd.concat(baseline_evals)  df_baseline_eval.head() In\u00a0[\u00a0]: Copied! <pre>df_eval_metrics = pd.merge(\n    df_transformer_eval, df_baseline_eval, how=\"left\", on=[\"metric\", \"step\"]\n).melt(\n    id_vars=[\"metric\", \"step\"],\n    value_vars=[\n        \"VanillaTransformer\",\n        \"iTransformer\",\n        \"AutoARIMA\",\n    ],\n    var_name=\"model\",\n)\n\ndf_eval_metrics\n</pre> df_eval_metrics = pd.merge(     df_transformer_eval, df_baseline_eval, how=\"left\", on=[\"metric\", \"step\"] ).melt(     id_vars=[\"metric\", \"step\"],     value_vars=[         \"VanillaTransformer\",         \"iTransformer\",         \"AutoARIMA\",     ],     var_name=\"model\", )  df_eval_metrics In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(13, 5), ncols=3)\n\nfor i, m in enumerate(df_eval_metrics.metric.unique()):\n    sns.violinplot(\n        df_eval_metrics.loc[df_eval_metrics.metric == m],\n        x=\"model\",\n        y=\"value\",\n        hue=\"model\",\n        fill=False,\n        ax=ax[i],\n        label=m,\n    )\n    ax[i].set_title(f\"Metric: {m}\")\n</pre> _, ax = plt.subplots(figsize=(13, 5), ncols=3)  for i, m in enumerate(df_eval_metrics.metric.unique()):     sns.violinplot(         df_eval_metrics.loc[df_eval_metrics.metric == m],         x=\"model\",         y=\"value\",         hue=\"model\",         fill=False,         ax=ax[i],         label=m,     )     ax[i].set_title(f\"Metric: {m}\") In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 13), nrows=3)\n\nvisualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_arima,\n    model_name=\"AutoARIMA\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (ARIMA)\",\n    ax=ax[0],\n)\n\nvisualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"VanillaTransformer\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (VanillaTransformer)\",\n    ax=ax[1],\n)\n\nvisualize_predictions(\n    df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],\n    df_test=df_test,\n    df_pred=df_y_hat_transformer,\n    model_name=\"iTransformer\",\n    n_ahead=3,\n    title=\"Forecasting 3 Steps Ahead (iTransformer)\",\n    ax=ax[2],\n)\n</pre> _, ax = plt.subplots(figsize=(10, 13), nrows=3)  visualize_predictions(     df_train=df_train.loc[df_train.ds &gt; \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_arima,     model_name=\"AutoARIMA\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (ARIMA)\",     ax=ax[0], )  visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"VanillaTransformer\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (VanillaTransformer)\",     ax=ax[1], )  visualize_predictions(     df_train=df_train.loc[df_train.ds &gt;= \"2015-01-01\"],     df_test=df_test,     df_pred=df_y_hat_transformer,     model_name=\"iTransformer\",     n_ahead=3,     title=\"Forecasting 3 Steps Ahead (iTransformer)\",     ax=ax[2], )"}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#transformer-forecaster-with-neuralforecast", "title": "Transformer Forecaster with neuralforecast\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#load-data", "title": "Load Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#prepare-data", "title": "Prepare Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#baselines", "title": "Baselines\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#transformers", "title": "Transformers\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing-m5/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing/", "title": "Transformer Forecaster with neuralforecast", "text": "In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\n\nsns.set_theme(context=\"paper\", style=\"ticks\", palette=\"colorblind\")\n</pre> import pandas as pd import seaborn as sns  sns.set_theme(context=\"paper\", style=\"ticks\", palette=\"colorblind\") In\u00a0[\u00a0]: Copied! <pre>data_source = \"https://gist.githubusercontent.com/emptymalei/15e548f483e896be0afc99e49dd6fbc9/raw/8384d82cdaaed4d9b0a555888efb70d79911dd2a/sunspot_nixtla.csv\"\n\ndf = pd.read_csv(data_source, parse_dates=[\"ds\"])\n</pre> data_source = \"https://gist.githubusercontent.com/emptymalei/15e548f483e896be0afc99e49dd6fbc9/raw/8384d82cdaaed4d9b0a555888efb70d79911dd2a/sunspot_nixtla.csv\"  df = pd.read_csv(data_source, parse_dates=[\"ds\"])  In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\nsns.lineplot(df, x=\"ds\", y=\"y\", ax=ax)\n\nax.set_title(\"Sunspot Time Series\")\nax.set_ylabel(\"Sunspot Number\")\n</pre> _, ax = plt.subplots() sns.lineplot(df, x=\"ds\", y=\"y\", ax=ax)  ax.set_title(\"Sunspot Time Series\") ax.set_ylabel(\"Sunspot Number\") In\u00a0[\u00a0]: Copied! <pre>train_test_split_date = \"2000-01-01\"\n\ndf_train = df[df.ds &lt;= train_test_split_date]\ndf_test = df[df.ds &gt; train_test_split_date]\n\nhorizon = 3\nhorizon\n</pre> train_test_split_date = \"2000-01-01\"  df_train = df[df.ds &lt;= train_test_split_date] df_test = df[df.ds &gt; train_test_split_date]  horizon = 3 horizon In\u00a0[\u00a0]: Copied! <pre>from statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n</pre> from statsforecast import StatsForecast from statsforecast.models import AutoARIMA In\u00a0[\u00a0]: Copied! <pre>sf = StatsForecast(models=[AutoARIMA(season_length=12)], freq=\"YS\")\n</pre> sf = StatsForecast(models=[AutoARIMA(season_length=12)], freq=\"YS\") In\u00a0[\u00a0]: Copied! <pre>sf.fit(df_train)\n</pre> sf.fit(df_train) In\u00a0[\u00a0]: Copied! <pre>def forecast_test(forecaster_pred, df_train, df_test):\n    dfs_pred = []\n    for i in range(len(df_test)):\n        df_pred_input_i = pd.concat([df_train, df_test[:i]])\n        print(df_pred_input_i.ds.max())\n        df_pred_output_i = forecaster_pred(df_pred_input_i)\n        df_pred_output_i[\"step\"] = i\n        dfs_pred.append(df_pred_output_i)\n    df_y_hat = pd.concat(dfs_pred).reset_index(drop=False)\n    return df_y_hat\n</pre> def forecast_test(forecaster_pred, df_train, df_test):     dfs_pred = []     for i in range(len(df_test)):         df_pred_input_i = pd.concat([df_train, df_test[:i]])         print(df_pred_input_i.ds.max())         df_pred_output_i = forecaster_pred(df_pred_input_i)         df_pred_output_i[\"step\"] = i         dfs_pred.append(df_pred_output_i)     df_y_hat = pd.concat(dfs_pred).reset_index(drop=False)     return df_y_hat In\u00a0[\u00a0]: Copied! <pre>def visualize_predictions(\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame,\n    df_pred: pd.DataFrame,\n    model_name: str,\n):\n    _, ax = plt.subplots()\n\n    sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)\n\n    sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)\n\n    sns.lineplot(df_pred, x=\"ds\", y=model_name, hue=\"step\", ax=ax)\n\n    ax.set_title(\"Sunspot Forecast\", fontsize=22)\n    ax.set_ylabel(\"Sunspot Number\", fontsize=20)\n    ax.set_xlabel(\"Year\", fontsize=20)\n    ax.legend(prop={\"size\": 15})\n    ax.grid()\n</pre> def visualize_predictions(     df_train: pd.DataFrame,     df_test: pd.DataFrame,     df_pred: pd.DataFrame,     model_name: str, ):     _, ax = plt.subplots()      sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)      sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)      sns.lineplot(df_pred, x=\"ds\", y=model_name, hue=\"step\", ax=ax)      ax.set_title(\"Sunspot Forecast\", fontsize=22)     ax.set_ylabel(\"Sunspot Number\", fontsize=20)     ax.set_xlabel(\"Year\", fontsize=20)     ax.legend(prop={\"size\": 15})     ax.grid() In\u00a0[\u00a0]: Copied! <pre>df_y_hat_arima = forecast_test(\n    lambda x: sf.predict(h=horizon, level=[95]), df_train=df_train, df_test=df_test\n)\n</pre> df_y_hat_arima = forecast_test(     lambda x: sf.predict(h=horizon, level=[95]), df_train=df_train, df_test=df_test ) In\u00a0[\u00a0]: Copied! <pre>visualize_predictions(\n    df_train=df_train, df_test=df_test, df_pred=df_y_hat_arima, model_name=\"AutoARIMA\"\n)\n</pre> visualize_predictions(     df_train=df_train, df_test=df_test, df_pred=df_y_hat_arima, model_name=\"AutoARIMA\" ) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\ndf_chart = df_test.merge(df_y_hat_arima, how=\"left\", on=[\"unique_id\", \"ds\"])\ndf_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")\n\ndf_chart[\n    [\n        \"y\",\n        \"AutoARIMA\",\n        # 'NHITS'\n    ]\n].plot(ax=ax, linewidth=2)\n\nax.set_title(\"AirPassengers Forecast\", fontsize=22)\nax.set_ylabel(\"Monthly Passengers\", fontsize=20)\nax.set_xlabel(\"Timestamp [t]\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7)) df_chart = df_test.merge(df_y_hat_arima, how=\"left\", on=[\"unique_id\", \"ds\"]) df_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")  df_chart[     [         \"y\",         \"AutoARIMA\",         # 'NHITS'     ] ].plot(ax=ax, linewidth=2)  ax.set_title(\"AirPassengers Forecast\", fontsize=22) ax.set_ylabel(\"Monthly Passengers\", fontsize=20) ax.set_xlabel(\"Timestamp [t]\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid() In\u00a0[\u00a0]: Copied! <pre>from neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer\n</pre> from neuralforecast import NeuralForecast from neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer In\u00a0[\u00a0]: Copied! <pre>models = [\n    VanillaTransformer(\n        # input_size=9, h=horizon,\n        # n_head=4,\n        # windows_batch_size=512,\n        # learning_rate=0.00010614524276500768,\n        # # conv_hidden_size=2,\n        # # encoder_layers=6,\n        # max_steps=500,\n        # #early_stop_patience_steps=5,\n        **{\n            \"hidden_size\": 128,\n            \"n_head\": 4,\n            \"learning_rate\": 0.00010614524276500768,\n            \"scaler_type\": \"robust\",\n            \"max_steps\": 500,\n            \"batch_size\": 32,\n            \"windows_batch_size\": 512,\n            \"random_seed\": 16,\n            \"input_size\": 30,\n            \"step_size\": 3,\n            \"h\": horizon,\n        }\n    ),\n    # iTransformer(\n    #     input_size=history_length, h=horizon,\n    #     # n_head=1,\n    #     # conv_hidden_size=1,\n    #     # encoder_layers=6,\n    #     max_steps=50,\n    #     n_series=1,\n    #     #early_stop_patience_steps=5,\n    # ),\n    # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #        ),\n    # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #       )\n]\nnf = NeuralForecast(models=models, freq=\"YS\")\n</pre> models = [     VanillaTransformer(         # input_size=9, h=horizon,         # n_head=4,         # windows_batch_size=512,         # learning_rate=0.00010614524276500768,         # # conv_hidden_size=2,         # # encoder_layers=6,         # max_steps=500,         # #early_stop_patience_steps=5,         **{             \"hidden_size\": 128,             \"n_head\": 4,             \"learning_rate\": 0.00010614524276500768,             \"scaler_type\": \"robust\",             \"max_steps\": 500,             \"batch_size\": 32,             \"windows_batch_size\": 512,             \"random_seed\": 16,             \"input_size\": 30,             \"step_size\": 3,             \"h\": horizon,         }     ),     # iTransformer(     #     input_size=history_length, h=horizon,     #     # n_head=1,     #     # conv_hidden_size=1,     #     # encoder_layers=6,     #     max_steps=50,     #     n_series=1,     #     #early_stop_patience_steps=5,     # ),     # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #        ),     # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #       ) ] nf = NeuralForecast(models=models, freq=\"YS\") In\u00a0[\u00a0]: Copied! <pre>nf.fit(df=df_train)\n</pre> nf.fit(df=df_train) In\u00a0[\u00a0]: Copied! <pre>train_test_split_date\n</pre> train_test_split_date In\u00a0[\u00a0]: Copied! <pre>dfs_pred = []\nfor i in range(len(df_test)):\n    df_pred_input_i = pd.concat([df_train, df_test[:i]])\n    df_pred_output_i = nf.predict(df_pred_input_i)\n    df_pred_output_i[\"step\"] = i\n    dfs_pred.append(df_pred_output_i)\ndf_y_hat = pd.concat(dfs_pred).reset_index(drop=False)\n</pre> dfs_pred = [] for i in range(len(df_test)):     df_pred_input_i = pd.concat([df_train, df_test[:i]])     df_pred_output_i = nf.predict(df_pred_input_i)     df_pred_output_i[\"step\"] = i     dfs_pred.append(df_pred_output_i) df_y_hat = pd.concat(dfs_pred).reset_index(drop=False) In\u00a0[\u00a0]: Copied! <pre>df_y_hat\n</pre> df_y_hat In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>df_y_hat.ds.dt.freq\n</pre> df_y_hat.ds.dt.freq In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\n\nsns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)\n\nsns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)\n\nsns.lineplot(df_y_hat, x=\"ds\", y=\"VanillaTransformer\", hue=\"step\", ax=ax)\n\n\nax.set_title(\"Sunspot Forecast\", fontsize=22)\nax.set_ylabel(\"Avg Sunspot Area\", fontsize=20)\nax.set_xlabel(\"Year\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7))  sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)  sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)  sns.lineplot(df_y_hat, x=\"ds\", y=\"VanillaTransformer\", hue=\"step\", ax=ax)   ax.set_title(\"Sunspot Forecast\", fontsize=22) ax.set_ylabel(\"Avg Sunspot Area\", fontsize=20) ax.set_xlabel(\"Year\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid() In\u00a0[\u00a0]: Copied! <pre>from datasetsforecast.evaluation import accuracy\nfrom datasetsforecast.losses import mae, mse, rmse\n</pre> from datasetsforecast.evaluation import accuracy from datasetsforecast.losses import mae, mse, rmse In\u00a0[\u00a0]: Copied! <pre>df_y_hat.loc[df_y_hat.step == 0]\n</pre> df_y_hat.loc[df_y_hat.step == 0] In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>evaluation_df = accuracy(\n    Y_hat_df=df_y_hat.loc[df_y_hat.step == 0],\n    Y_test_df=df_test,\n    Y_df=df_train,\n    metrics=[mse, mae, rmse],\n    agg_by=[\"unique_id\"],\n)\n\nevaluation_df.head()\n</pre> evaluation_df = accuracy(     Y_hat_df=df_y_hat.loc[df_y_hat.step == 0],     Y_test_df=df_test,     Y_df=df_train,     metrics=[mse, mae, rmse],     agg_by=[\"unique_id\"], )  evaluation_df.head()"}, {"location": "notebooks/transformer-ts-nixtla-testing/#transformer-forecaster-with-neuralforecast", "title": "Transformer Forecaster with neuralforecast\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing/#load-data", "title": "Load Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing/#prepare-data", "title": "Prepare Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing/#baselines", "title": "Baselines\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla-testing/#transformers", "title": "Transformers\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/", "title": "Transformer Forecaster with neuralforecast", "text": "In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\n</pre> import pandas as pd import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>data_source = \"https://gist.githubusercontent.com/emptymalei/921a624ce44e6a60bb6ec637b195ceaf/raw/4cb52fa20dcc598e16891eae2749fd96a97750e9/sunspot.csv\"\n\ndf = (\n    pd.read_csv(data_source, parse_dates=[\"date\"])\n    .rename(columns={\"date\": \"ds\", \"avg_sunspot_area\": \"y\"})\n    .assign(unique_id=1)\n)\n</pre> data_source = \"https://gist.githubusercontent.com/emptymalei/921a624ce44e6a60bb6ec637b195ceaf/raw/4cb52fa20dcc598e16891eae2749fd96a97750e9/sunspot.csv\"  df = (     pd.read_csv(data_source, parse_dates=[\"date\"])     .rename(columns={\"date\": \"ds\", \"avg_sunspot_area\": \"y\"})     .assign(unique_id=1) )  In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() In\u00a0[\u00a0]: Copied! <pre>df.plot(x=\"ds\", y=\"y\")\n</pre> df.plot(x=\"ds\", y=\"y\") In\u00a0[\u00a0]: Copied! <pre>train_test_split_date = \"2000-01-01\"\n\ndf_train = df[df.ds &lt;= train_test_split_date]\ndf_test = df[df.ds &gt; train_test_split_date]\n\nhorizon = 3\nhorizon\n</pre> train_test_split_date = \"2000-01-01\"  df_train = df[df.ds &lt;= train_test_split_date] df_test = df[df.ds &gt; train_test_split_date]  horizon = 3 horizon In\u00a0[\u00a0]: Copied! <pre>from statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n</pre> from statsforecast import StatsForecast from statsforecast.models import AutoARIMA In\u00a0[\u00a0]: Copied! <pre>sf = StatsForecast(models=[AutoARIMA(season_length=12)], freq=\"YS\")\n</pre> sf = StatsForecast(models=[AutoARIMA(season_length=12)], freq=\"YS\") In\u00a0[\u00a0]: Copied! <pre>sf.fit(df_train)\n</pre> sf.fit(df_train) In\u00a0[\u00a0]: Copied! <pre>df_y_hat_arima = sf.predict(h=horizon, level=[95])\ndf_y_hat_arima\n</pre> df_y_hat_arima = sf.predict(h=horizon, level=[95]) df_y_hat_arima In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\ndf_chart = df_test.merge(df_y_hat_arima, how=\"left\", on=[\"unique_id\", \"ds\"])\ndf_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")\n\ndf_chart[\n    [\n        \"y\",\n        \"AutoARIMA\",\n        # 'NHITS'\n    ]\n].plot(ax=ax, linewidth=2)\n\nax.set_title(\"AirPassengers Forecast\", fontsize=22)\nax.set_ylabel(\"Monthly Passengers\", fontsize=20)\nax.set_xlabel(\"Timestamp [t]\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7)) df_chart = df_test.merge(df_y_hat_arima, how=\"left\", on=[\"unique_id\", \"ds\"]) df_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")  df_chart[     [         \"y\",         \"AutoARIMA\",         # 'NHITS'     ] ].plot(ax=ax, linewidth=2)  ax.set_title(\"AirPassengers Forecast\", fontsize=22) ax.set_ylabel(\"Monthly Passengers\", fontsize=20) ax.set_xlabel(\"Timestamp [t]\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid() In\u00a0[\u00a0]: Copied! <pre>from neuralforecast import NeuralForecast\nfrom neuralforecast.auto import AutoVanillaTransformer\nfrom neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer\n</pre> from neuralforecast import NeuralForecast from neuralforecast.auto import AutoVanillaTransformer from neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer In\u00a0[\u00a0]: Copied! <pre># vt_config = dict(\n#     max_steps=1, val_check_steps=1, input_size=12, hidden_size=8\n# )\nauto_vt_model = AutoVanillaTransformer(h=horizon, backend=\"optuna\")\n\nnf_auto = NeuralForecast(models=[auto_vt_model], freq=\"YS\")\n</pre> # vt_config = dict( #     max_steps=1, val_check_steps=1, input_size=12, hidden_size=8 # ) auto_vt_model = AutoVanillaTransformer(h=horizon, backend=\"optuna\")  nf_auto = NeuralForecast(models=[auto_vt_model], freq=\"YS\") In\u00a0[\u00a0]: Copied! <pre>nf_auto.fit(df=df_train, val_size=3)\n</pre> nf_auto.fit(df=df_train, val_size=3) In\u00a0[\u00a0]: Copied! <pre>results = nf_auto.models[0].results.trials_dataframe()\nresults.drop(columns=\"user_attrs_ALL_PARAMS\")\n</pre> results = nf_auto.models[0].results.trials_dataframe() results.drop(columns=\"user_attrs_ALL_PARAMS\") In\u00a0[\u00a0]: Copied! <pre>df_y_hat_optuna = nf_auto.predict().reset_index()\ndf_y_hat_optuna.head()\n</pre> df_y_hat_optuna = nf_auto.predict().reset_index() df_y_hat_optuna.head() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\ndf_chart = df_test.merge(df_y_hat_optuna, how=\"left\", on=[\"unique_id\", \"ds\"])\ndf_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")\n\ndf_chart[\n    [\n        \"y\",\n        \"AutoVanillaTransformer\",\n    ]\n].plot(ax=ax, linewidth=2)\n\nax.set_title(\"Forecast\", fontsize=22)\nax.set_ylabel(\"Sunspot Area\", fontsize=20)\nax.set_xlabel(\"Timestamp [t]\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7)) df_chart = df_test.merge(df_y_hat_optuna, how=\"left\", on=[\"unique_id\", \"ds\"]) df_chart = pd.concat([df_train, df_chart]).set_index(\"ds\")  df_chart[     [         \"y\",         \"AutoVanillaTransformer\",     ] ].plot(ax=ax, linewidth=2)  ax.set_title(\"Forecast\", fontsize=22) ax.set_ylabel(\"Sunspot Area\", fontsize=20) ax.set_xlabel(\"Timestamp [t]\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid() In\u00a0[\u00a0]: Copied! <pre>nf_auto.models[0].results.best_trial.params\n</pre> nf_auto.models[0].results.best_trial.params In\u00a0[\u00a0]: Copied! <pre># nf_auto.save(\"lightning_logs/nf_auto_vanilla_transformer_sunspot\")\n</pre> # nf_auto.save(\"lightning_logs/nf_auto_vanilla_transformer_sunspot\") In\u00a0[\u00a0]: Copied! <pre>models = [\n    VanillaTransformer(\n        # input_size=9, h=horizon,\n        # n_head=4,\n        # windows_batch_size=512,\n        # learning_rate=0.00010614524276500768,\n        # # conv_hidden_size=2,\n        # # encoder_layers=6,\n        # max_steps=500,\n        # #early_stop_patience_steps=5,\n        **{\n            \"hidden_size\": 128,\n            \"n_head\": 4,\n            \"learning_rate\": 0.00010614524276500768,\n            \"scaler_type\": \"robust\",\n            \"max_steps\": 500,\n            \"batch_size\": 32,\n            \"windows_batch_size\": 512,\n            \"random_seed\": 16,\n            \"input_size\": 30,\n            \"step_size\": 3,\n            \"h\": horizon,\n        }\n    ),\n    # iTransformer(\n    #     input_size=history_length, h=horizon,\n    #     # n_head=1,\n    #     # conv_hidden_size=1,\n    #     # encoder_layers=6,\n    #     max_steps=50,\n    #     n_series=1,\n    #     #early_stop_patience_steps=5,\n    # ),\n    # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #        ),\n    # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #       )\n]\nnf = NeuralForecast(models=models, freq=\"YS\")\n</pre> models = [     VanillaTransformer(         # input_size=9, h=horizon,         # n_head=4,         # windows_batch_size=512,         # learning_rate=0.00010614524276500768,         # # conv_hidden_size=2,         # # encoder_layers=6,         # max_steps=500,         # #early_stop_patience_steps=5,         **{             \"hidden_size\": 128,             \"n_head\": 4,             \"learning_rate\": 0.00010614524276500768,             \"scaler_type\": \"robust\",             \"max_steps\": 500,             \"batch_size\": 32,             \"windows_batch_size\": 512,             \"random_seed\": 16,             \"input_size\": 30,             \"step_size\": 3,             \"h\": horizon,         }     ),     # iTransformer(     #     input_size=history_length, h=horizon,     #     # n_head=1,     #     # conv_hidden_size=1,     #     # encoder_layers=6,     #     max_steps=50,     #     n_series=1,     #     #early_stop_patience_steps=5,     # ),     # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #        ),     # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #       ) ] nf = NeuralForecast(models=models, freq=\"YS\") In\u00a0[\u00a0]: Copied! <pre>nf.fit(df=df_train)\n</pre> nf.fit(df=df_train) In\u00a0[\u00a0]: Copied! <pre>train_test_split_date\n</pre> train_test_split_date In\u00a0[\u00a0]: Copied! <pre>dfs_pred = []\nfor i in range(len(df_test)):\n    df_pred_input_i = pd.concat([df_train, df_test[:i]])\n    df_pred_output_i = nf.predict(df_pred_input_i)\n    df_pred_output_i[\"step\"] = i\n    dfs_pred.append(df_pred_output_i)\ndf_y_hat = pd.concat(dfs_pred).reset_index(drop=False)\n</pre> dfs_pred = [] for i in range(len(df_test)):     df_pred_input_i = pd.concat([df_train, df_test[:i]])     df_pred_output_i = nf.predict(df_pred_input_i)     df_pred_output_i[\"step\"] = i     dfs_pred.append(df_pred_output_i) df_y_hat = pd.concat(dfs_pred).reset_index(drop=False) In\u00a0[\u00a0]: Copied! <pre>df_y_hat\n</pre> df_y_hat In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>df_y_hat.ds.dt.freq\n</pre> df_y_hat.ds.dt.freq In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\n\nsns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)\n\nsns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)\n\nsns.lineplot(df_y_hat, x=\"ds\", y=\"VanillaTransformer\", hue=\"step\", ax=ax)\n\n\nax.set_title(\"Sunspot Forecast\", fontsize=22)\nax.set_ylabel(\"Avg Sunspot Area\", fontsize=20)\nax.set_xlabel(\"Year\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7))  sns.lineplot(df_train, x=\"ds\", y=\"y\", ax=ax)  sns.lineplot(df_test, x=\"ds\", y=\"y\", color=\"k\", ax=ax)  sns.lineplot(df_y_hat, x=\"ds\", y=\"VanillaTransformer\", hue=\"step\", ax=ax)   ax.set_title(\"Sunspot Forecast\", fontsize=22) ax.set_ylabel(\"Avg Sunspot Area\", fontsize=20) ax.set_xlabel(\"Year\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid() In\u00a0[\u00a0]: Copied! <pre>from datasetsforecast.evaluation import accuracy\nfrom datasetsforecast.losses import mae, mse, rmse\n</pre> from datasetsforecast.evaluation import accuracy from datasetsforecast.losses import mae, mse, rmse In\u00a0[\u00a0]: Copied! <pre>df_y_hat.loc[df_y_hat.step == 0]\n</pre> df_y_hat.loc[df_y_hat.step == 0] In\u00a0[\u00a0]: Copied! <pre>df_test\n</pre> df_test In\u00a0[\u00a0]: Copied! <pre>evaluation_df = accuracy(\n    Y_hat_df=df_y_hat.loc[df_y_hat.step == 0],\n    Y_test_df=df_test,\n    Y_df=df_train,\n    metrics=[mse, mae, rmse],\n    agg_by=[\"unique_id\"],\n)\n\nevaluation_df.head()\n</pre> evaluation_df = accuracy(     Y_hat_df=df_y_hat.loc[df_y_hat.step == 0],     Y_test_df=df_test,     Y_df=df_train,     metrics=[mse, mae, rmse],     agg_by=[\"unique_id\"], )  evaluation_df.head()"}, {"location": "notebooks/transformer-ts-nixtla/#transformer-forecaster-with-neuralforecast", "title": "Transformer Forecaster with neuralforecast\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/#load-data", "title": "Load Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/#prepare-data", "title": "Prepare Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/#baselines", "title": "Baselines\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/#transformers", "title": "Transformers\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla/#transformers", "title": "Transformers\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla_naive_data/", "title": "Transformer Forecaster with neuralforecast", "text": "In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom ts_dl_utils.datasets.pendulum import Pendulum\n</pre> import pandas as pd from ts_dl_utils.datasets.pendulum import Pendulum In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=100)\ndf_pen = (\n    pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.00001))\n    .reset_index()\n    .rename(columns={\"index\": \"ds\", \"theta\": \"y\"})[[\"ds\", \"y\"]]\n)\ndf_pen[\"unique_id\"] = 1\n\ndf_pen.head()\n</pre> pen = Pendulum(length=100) df_pen = (     pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.00001))     .reset_index()     .rename(columns={\"index\": \"ds\", \"theta\": \"y\"})[[\"ds\", \"y\"]] ) df_pen[\"unique_id\"] = 1  df_pen.head() In\u00a0[\u00a0]: Copied! <pre>df_pen.plot(x=\"ds\", y=\"y\")\n</pre> df_pen.plot(x=\"ds\", y=\"y\") In\u00a0[\u00a0]: Copied! <pre>df_pen_train = df_pen[:-3]\ndf_pen_test = df_pen[-3:]\n\nhorizon_pen = len(df_pen_test)\n</pre> df_pen_train = df_pen[:-3] df_pen_test = df_pen[-3:]  horizon_pen = len(df_pen_test) In\u00a0[\u00a0]: Copied! <pre>from statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n</pre> from statsforecast import StatsForecast from statsforecast.models import AutoARIMA In\u00a0[\u00a0]: Copied! <pre>sf_pen = StatsForecast(models=[AutoARIMA(season_length=12)], freq=1)\n</pre> sf_pen = StatsForecast(models=[AutoARIMA(season_length=12)], freq=1) In\u00a0[\u00a0]: Copied! <pre>sf_pen.fit(df_pen_train)\n</pre> sf_pen.fit(df_pen_train) In\u00a0[\u00a0]: Copied! <pre>df_pen_y_hat_arima = sf_pen.predict(h=horizon_pen, level=[95])\ndf_pen_y_hat_arima\n</pre> df_pen_y_hat_arima = sf_pen.predict(h=horizon_pen, level=[95]) df_pen_y_hat_arima In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\n\ndf_pen_test.plot(x=\"ds\", y=\"y\", ax=ax)\ndf_pen_y_hat_arima.plot(x=\"ds\", y=\"AutoARIMA\", ax=ax)\n</pre> _, ax = plt.subplots()  df_pen_test.plot(x=\"ds\", y=\"y\", ax=ax) df_pen_y_hat_arima.plot(x=\"ds\", y=\"AutoARIMA\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>from neuralforecast import NeuralForecast\nfrom neuralforecast.auto import AutoVanillaTransformer\nfrom neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer\n\nhistory_length_pen = 10\n</pre> from neuralforecast import NeuralForecast from neuralforecast.auto import AutoVanillaTransformer from neuralforecast.models import NBEATS, NHITS, VanillaTransformer, iTransformer  history_length_pen = 10  In\u00a0[\u00a0]: Copied! <pre>models_pen = [\n    VanillaTransformer(\n        input_size=history_length_pen,\n        h=horizon_pen,\n        n_head=1,\n        conv_hidden_size=1,\n        encoder_layers=6,\n        max_steps=100,\n        # early_stop_patience_steps=5,\n    ),\n    # iTransformer(\n    #     input_size=history_length, h=horizon,\n    #     # n_head=1,\n    #     # conv_hidden_size=1,\n    #     # encoder_layers=6,\n    #     max_steps=50,\n    #     n_series=1,\n    #     #early_stop_patience_steps=5,\n    # ),\n    # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #        ),\n    # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5\n    #       )\n]\nnf_pen = NeuralForecast(models=models_pen, freq=1)\n</pre> models_pen = [     VanillaTransformer(         input_size=history_length_pen,         h=horizon_pen,         n_head=1,         conv_hidden_size=1,         encoder_layers=6,         max_steps=100,         # early_stop_patience_steps=5,     ),     # iTransformer(     #     input_size=history_length, h=horizon,     #     # n_head=1,     #     # conv_hidden_size=1,     #     # encoder_layers=6,     #     max_steps=50,     #     n_series=1,     #     #early_stop_patience_steps=5,     # ),     # NBEATS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #        ),     # NHITS(input_size=history_length, h=horizon, max_steps=100, #early_stop_patience_steps=5     #       ) ] nf_pen = NeuralForecast(models=models_pen, freq=1) In\u00a0[\u00a0]: Copied! <pre>nf_pen.fit(df=df_pen_train)\n</pre> nf_pen.fit(df=df_pen_train) In\u00a0[\u00a0]: Copied! <pre>df_pen_y_hat = nf_pen.predict().reset_index()\n</pre> df_pen_y_hat = nf_pen.predict().reset_index() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 7))\ndf_chart = df_pen_test.merge(df_pen_y_hat, how=\"left\", on=[\"unique_id\", \"ds\"])\ndf_chart = pd.concat([df_pen_train, df_chart]).set_index(\"ds\")\n\ndf_chart[\n    [\n        \"y\",\n        # 'NBEATS',\n        \"VanillaTransformer\",\n        # \"iTransformer\"\n        # 'NHITS'\n    ]\n].plot(ax=ax, linewidth=2)\n\nax.set_title(\"AirPassengers Forecast\", fontsize=22)\nax.set_ylabel(\"Monthly Passengers\", fontsize=20)\nax.set_xlabel(\"Timestamp [t]\", fontsize=20)\nax.legend(prop={\"size\": 15})\nax.grid()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(20, 7)) df_chart = df_pen_test.merge(df_pen_y_hat, how=\"left\", on=[\"unique_id\", \"ds\"]) df_chart = pd.concat([df_pen_train, df_chart]).set_index(\"ds\")  df_chart[     [         \"y\",         # 'NBEATS',         \"VanillaTransformer\",         # \"iTransformer\"         # 'NHITS'     ] ].plot(ax=ax, linewidth=2)  ax.set_title(\"AirPassengers Forecast\", fontsize=22) ax.set_ylabel(\"Monthly Passengers\", fontsize=20) ax.set_xlabel(\"Timestamp [t]\", fontsize=20) ax.legend(prop={\"size\": 15}) ax.grid()"}, {"location": "notebooks/transformer-ts-nixtla_naive_data/#transformer-forecaster-with-neuralforecast", "title": "Transformer Forecaster with neuralforecast\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla_naive_data/#load-data", "title": "Load Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla_naive_data/#prepare-data", "title": "Prepare Data\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla_naive_data/#baselines", "title": "Baselines\u00b6", "text": ""}, {"location": "notebooks/transformer-ts-nixtla_naive_data/#transformers", "title": "Transformers\u00b6", "text": ""}, {"location": "notebooks/transformer_history/", "title": "History of Transformer for Forecasting", "text": "<p>This is a temporary notebook to produce figures for my book.</p> In\u00a0[\u00a0]: Copied! <pre>import datetime\nimport json\n</pre> import datetime import json In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\n\ndata_path = Path(\"data/transformer-and-forecasting-papers.json\")\n</pre> from pathlib import Path  import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd  data_path = Path(\"data/transformer-and-forecasting-papers.json\") In\u00a0[\u00a0]: Copied! <pre>with open(data_path, \"r\") as fp:\n    data = json.load(fp)\n</pre> with open(data_path, \"r\") as fp:     data = json.load(fp) In\u00a0[\u00a0]: Copied! <pre>tags = (\n    pd.read_csv(\"data/transformer-and-forecasting-papers.csv\")\n    .set_index(\"LitmapsId\")\n    .Tags.to_dict()\n)\n</pre> tags = (     pd.read_csv(\"data/transformer-and-forecasting-papers.csv\")     .set_index(\"LitmapsId\")     .Tags.to_dict() ) In\u00a0[\u00a0]: Copied! <pre># labels = {\n#     i[\"id\"]: {\n#         \"first_author_last\": i.get(\"authorString\", \"\").split(\", \")[0].split(\" \")[-1],\n#         \"arxiv_id\": i.get(\"arxivIds\", \"\"),\n#         \"year\": datetime.datetime.strptime(i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\").year\n#     }\n#     for i in\n#     data[\"items\"]\n# }\n</pre> # labels = { #     i[\"id\"]: { #         \"first_author_last\": i.get(\"authorString\", \"\").split(\", \")[0].split(\" \")[-1], #         \"arxiv_id\": i.get(\"arxivIds\", \"\"), #         \"year\": datetime.datetime.strptime(i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\").year #     } #     for i in #     data[\"items\"] # } In\u00a0[\u00a0]: Copied! <pre>data[\"items\"][0]\n</pre> data[\"items\"][0] In\u00a0[\u00a0]: Copied! <pre>min_date = min(\n    [\n        datetime.datetime.strptime(\n            i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"\n        ).toordinal()\n        for i in data[\"items\"]\n    ]\n)\n</pre> min_date = min(     [         datetime.datetime.strptime(             i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"         ).toordinal()         for i in data[\"items\"]     ] ) In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(42)\n\ncoordinate_calculator = lambda i: np.array(\n    [\n        np.log(\n            datetime.datetime.strptime(\n                i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"\n            ).toordinal()\n        ),\n        (\n            np.log(1 + i.get(\"forwardEdgeCount\", 0)) * (1 + rng.random() * 0.2)\n            if i.get(\"forwardEdgeCount\", 0) &gt; 50\n            else np.log(1 + i.get(\"forwardEdgeCount\", 0)) + rng.random() * 3\n        ),\n    ]\n)\n\ncoordinates = {i[\"id\"]: coordinate_calculator(i) for i in data[\"items\"]}\n\ncolor_calculator = lambda i: \"black\" if \"Foundation Model\" in tags[i[\"id\"]] else \"red\"\n\n\nmarker_calculator = lambda i: \"s\" if \"Foundation Model\" in tags[i[\"id\"]] else \"o\"\n\nnode_size_calculator = lambda x: 10 + 30 * np.log(1 + x)\n</pre> rng = np.random.default_rng(42)  coordinate_calculator = lambda i: np.array(     [         np.log(             datetime.datetime.strptime(                 i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"             ).toordinal()         ),         (             np.log(1 + i.get(\"forwardEdgeCount\", 0)) * (1 + rng.random() * 0.2)             if i.get(\"forwardEdgeCount\", 0) &gt; 50             else np.log(1 + i.get(\"forwardEdgeCount\", 0)) + rng.random() * 3         ),     ] )  coordinates = {i[\"id\"]: coordinate_calculator(i) for i in data[\"items\"]}  color_calculator = lambda i: \"black\" if \"Foundation Model\" in tags[i[\"id\"]] else \"red\"   marker_calculator = lambda i: \"s\" if \"Foundation Model\" in tags[i[\"id\"]] else \"o\"  node_size_calculator = lambda x: 10 + 30 * np.log(1 + x) In\u00a0[\u00a0]: Copied! <pre>G = nx.Graph()\n\nfor i in data[\"items\"]:\n    G.add_node(\n        i[\"id\"],\n        forwardEdgeCount=i.get(\"forwardEdgeCount\", 0),\n        coordinate=coordinates[i[\"id\"]],\n        color=color_calculator(i),\n        marker=marker_calculator(i),\n        size=node_size_calculator(i.get(\"forwardEdgeCount\", 0)),\n        first_author_last=i.get(\"authorString\", \"\").split(\", \")[0].split(\" \")[-1],\n        arxiv_id=i.get(\"arxivIds\", \"\"),\n        year=datetime.datetime.strptime(\n            i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"\n        ).year,\n        title=i[\"title\"],\n    )\n    for b in i.get(\"backwardEdges\", \"\").split(\",\"):\n        try:\n            if int(b) in data[\"ids\"]:\n                G.add_edge(i[\"id\"], int(b))\n        except ValueError as e:\n            print(f\"{b}: {e}\")\n</pre> G = nx.Graph()  for i in data[\"items\"]:     G.add_node(         i[\"id\"],         forwardEdgeCount=i.get(\"forwardEdgeCount\", 0),         coordinate=coordinates[i[\"id\"]],         color=color_calculator(i),         marker=marker_calculator(i),         size=node_size_calculator(i.get(\"forwardEdgeCount\", 0)),         first_author_last=i.get(\"authorString\", \"\").split(\", \")[0].split(\" \")[-1],         arxiv_id=i.get(\"arxivIds\", \"\"),         year=datetime.datetime.strptime(             i[\"publicationDate\"], \"%Y-%m-%dT%H:%M:%SZ\"         ).year,         title=i[\"title\"],     )     for b in i.get(\"backwardEdges\", \"\").split(\",\"):         try:             if int(b) in data[\"ids\"]:                 G.add_edge(i[\"id\"], int(b))         except ValueError as e:             print(f\"{b}: {e}\") In\u00a0[\u00a0]: Copied! <pre>transformer_origin_id = 261105339\n\n# title_labels = {\n#     i[\"id\"]: i[\"title\"]\n#     for i in\n#     data[\"items\"]\n#     if i.get(\"forwardEdgeCount\",0) &gt;= 1000\n# }\n</pre> transformer_origin_id = 261105339  # title_labels = { #     i[\"id\"]: i[\"title\"] #     for i in #     data[\"items\"] #     if i.get(\"forwardEdgeCount\",0) &gt;= 1000 # } In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(25, 25))\n\n\nnode_size_calculator = lambda x: 10 + 30 * np.log(1 + x)\n\nnx.draw_networkx_edges(G, coordinates, edge_color=\"lightgray\")\n\n# Draw the nodes with different markers\neffective_nodes = []\nfor node in G.nodes:\n    try:\n        nx.draw_networkx_nodes(\n            G,\n            pos=coordinates,\n            nodelist=[node],\n            node_size=G.nodes.get(node).get(\"size\"),\n            node_shape=G.nodes.get(node).get(\"marker\"),\n            node_color=G.nodes.get(node).get(\"color\"),\n        )\n        effective_nodes.append(node)\n    except IndexError as e:\n        print(f\"{node}: {e}\")\n\nfor node in effective_nodes:\n    x, y = G.nodes.get(node).get(\"coordinate\")\n    first_author_last = G.nodes.get(node).get(\"first_author_last\")\n    year = G.nodes.get(node).get(\"year\")\n    plt.text(x, y + 0.1, f\"{first_author_last} {year}\", horizontalalignment=\"center\")\n\nfor node in effective_nodes:\n    node_citations = G.nodes.get(node).get(\"forwardEdgeCount\")\n    if node_citations &gt;= 1000:\n        node_title = G.nodes.get(node).get(\"title\")\n        plt.text(\n            coordinates[node][0],\n            coordinates[node][1] - 0.3,\n            node_title,\n            horizontalalignment=\"center\",\n        )\n\nplt.axis(\"off\")\nfig.savefig(\n    \"results/transformer_history/transformer_ts_papers.png\",\n    bbox_inches=\"tight\",\n    pad_inches=-0.5,\n    # transparent=True\n)\n</pre> fig, ax = plt.subplots(figsize=(25, 25))   node_size_calculator = lambda x: 10 + 30 * np.log(1 + x)  nx.draw_networkx_edges(G, coordinates, edge_color=\"lightgray\")  # Draw the nodes with different markers effective_nodes = [] for node in G.nodes:     try:         nx.draw_networkx_nodes(             G,             pos=coordinates,             nodelist=[node],             node_size=G.nodes.get(node).get(\"size\"),             node_shape=G.nodes.get(node).get(\"marker\"),             node_color=G.nodes.get(node).get(\"color\"),         )         effective_nodes.append(node)     except IndexError as e:         print(f\"{node}: {e}\")  for node in effective_nodes:     x, y = G.nodes.get(node).get(\"coordinate\")     first_author_last = G.nodes.get(node).get(\"first_author_last\")     year = G.nodes.get(node).get(\"year\")     plt.text(x, y + 0.1, f\"{first_author_last} {year}\", horizontalalignment=\"center\")  for node in effective_nodes:     node_citations = G.nodes.get(node).get(\"forwardEdgeCount\")     if node_citations &gt;= 1000:         node_title = G.nodes.get(node).get(\"title\")         plt.text(             coordinates[node][0],             coordinates[node][1] - 0.3,             node_title,             horizontalalignment=\"center\",         )  plt.axis(\"off\") fig.savefig(     \"results/transformer_history/transformer_ts_papers.png\",     bbox_inches=\"tight\",     pad_inches=-0.5,     # transparent=True ) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(data[\"items\"]).rename(\n    columns={\n        \"forwardEdgeCount\": \"citations\",\n        \"authorString\": \"authors\",\n    }\n)\n\ndf[\"year\"] = df.publicationDate.apply(\n    lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").year\n)\ndf.fillna({\"citations\": 0}, inplace=True)\ndf = df.astype(dtype={\"citations\": int})[[\"title\", \"authors\", \"citations\", \"year\"]]\n\ndf.sort_values(by=\"year\", inplace=True)\n</pre> df = pd.DataFrame(data[\"items\"]).rename(     columns={         \"forwardEdgeCount\": \"citations\",         \"authorString\": \"authors\",     } )  df[\"year\"] = df.publicationDate.apply(     lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").year ) df.fillna({\"citations\": 0}, inplace=True) df = df.astype(dtype={\"citations\": int})[[\"title\", \"authors\", \"citations\", \"year\"]]  df.sort_values(by=\"year\", inplace=True) In\u00a0[\u00a0]: Copied! <pre>print(df.to_markdown())\n</pre> print(df.to_markdown())"}, {"location": "notebooks/transformer_history/#history-of-transformer-for-forecasting", "title": "History of Transformer for Forecasting\u00b6", "text": ""}, {"location": "notebooks/transformer_history/#table", "title": "Table\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/", "title": "Transformer for Univariate Time Series Forecasting", "text": "In\u00a0[\u00a0]: Copied! <pre>import dataclasses\n</pre> import dataclasses In\u00a0[\u00a0]: Copied! <pre>import math\n\nimport lightning as L\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom torch import nn\nfrom ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule\nfrom ts_dl_utils.evaluation.evaluator import Evaluator\nfrom ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster\n</pre> import math  import lightning as L import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from lightning.pytorch.callbacks.early_stopping import EarlyStopping from torch import nn from ts_dl_utils.datasets.pendulum import Pendulum, PendulumDataModule from ts_dl_utils.evaluation.evaluator import Evaluator from ts_dl_utils.naive_forecasters.last_observation import LastObservationForecaster  In\u00a0[\u00a0]: Copied! <pre>pen = Pendulum(length=10000)\n</pre> pen = Pendulum(length=10000) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(pen(100, 400, initial_angle=1, beta=0.000001))\n</pre> df = pd.DataFrame(pen(100, 400, initial_angle=1, beta=0.000001)) <p>Since the damping constant is very small, the data generated is mostly a sin wave.</p> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(figsize=(10, 6.18))\n\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</pre> _, ax = plt.subplots(figsize=(10, 6.18))  df.plot(x=\"t\", y=\"theta\", ax=ax) <p>Since we do not deal with future covariates, we do not need a decoder. In this example, we build a simple transformer that only contains attention in encoder.</p> In\u00a0[\u00a0]: Copied! <pre>@dataclasses.dataclass\nclass TSTransformerParams:\n    \"\"\"A dataclass that contains all\n    the parameters for the transformer model.\n    \"\"\"\n\n    d_model: int = 512\n    nhead: int = 8\n    num_encoder_layers: int = 6\n    dropout: int = 0.1\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding to be added to\n    input embedding.\n\n    :param d_model: hidden dimension of the encoder\n    :param dropout: rate of dropout\n    :param max_len: maximum length of our positional\n        encoder. The encoder can not encode sequence\n        length longer than max_len.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        dropout: float = 0.1,\n        max_len: int = 5000,\n    ):\n        super().__init__()\n        self.max_len = max_len\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        :param x: input embedded time series,\n            shape `[batch_size, seq_len, embedding_dim]`\n        \"\"\"\n        history_length = x.size(1)\n        x = x + self.pe[:history_length]\n\n        return self.dropout(x)\n\n\nclass TSTransformer(nn.Module):\n    \"\"\"Transformer for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param transformer_params: all the parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        history_length: int,\n        horizon: int,\n        transformer_params: TSTransformerParams,\n    ):\n        super().__init__()\n        self.transformer_params = transformer_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.embedding = nn.Linear(1, self.transformer_params.d_model)\n\n        self.positional_encoding = PositionalEncoding(\n            d_model=self.transformer_params.d_model\n        )\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.transformer_params.d_model,\n            nhead=self.transformer_params.nhead,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(\n            encoder_layer, num_layers=self.transformer_params.num_encoder_layers\n        )\n\n        self.reverse_embedding = nn.Linear(self.transformer_params.d_model, 1)\n\n        self.decoder = nn.Linear(self.history_length, self.horizon)\n\n    @property\n    def transformer_config(self) -&gt; dict:\n        \"\"\"all the param in dict format\"\"\"\n        return dataclasses.asdict(self.transformer_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        :param x: input historical time series,\n            shape `[batch_size, seq_len, n_var]`\n        \"\"\"\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n\n        encoder_state = self.encoder(x)\n\n        decoder_in = self.reverse_embedding(encoder_state).squeeze(-1)\n\n        return self.decoder(decoder_in)\n</pre> @dataclasses.dataclass class TSTransformerParams:     \"\"\"A dataclass that contains all     the parameters for the transformer model.     \"\"\"      d_model: int = 512     nhead: int = 8     num_encoder_layers: int = 6     dropout: int = 0.1   class PositionalEncoding(nn.Module):     \"\"\"Positional encoding to be added to     input embedding.      :param d_model: hidden dimension of the encoder     :param dropout: rate of dropout     :param max_len: maximum length of our positional         encoder. The encoder can not encode sequence         length longer than max_len.     \"\"\"      def __init__(         self,         d_model: int,         dropout: float = 0.1,         max_len: int = 5000,     ):         super().__init__()         self.max_len = max_len         self.dropout = nn.Dropout(p=dropout)          position = torch.arange(max_len).unsqueeze(1)         div_term = torch.exp(             torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)         )         pe = torch.zeros(max_len, d_model)         pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)         self.register_buffer(\"pe\", pe)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         :param x: input embedded time series,             shape `[batch_size, seq_len, embedding_dim]`         \"\"\"         history_length = x.size(1)         x = x + self.pe[:history_length]          return self.dropout(x)   class TSTransformer(nn.Module):     \"\"\"Transformer for univaraite time series modeling.      :param history_length: the length of the input history.     :param horizon: the number of steps to be forecasted.     :param transformer_params: all the parameters.     \"\"\"      def __init__(         self,         history_length: int,         horizon: int,         transformer_params: TSTransformerParams,     ):         super().__init__()         self.transformer_params = transformer_params         self.history_length = history_length         self.horizon = horizon          self.embedding = nn.Linear(1, self.transformer_params.d_model)          self.positional_encoding = PositionalEncoding(             d_model=self.transformer_params.d_model         )          encoder_layer = nn.TransformerEncoderLayer(             d_model=self.transformer_params.d_model,             nhead=self.transformer_params.nhead,             batch_first=True,         )         self.encoder = nn.TransformerEncoder(             encoder_layer, num_layers=self.transformer_params.num_encoder_layers         )          self.reverse_embedding = nn.Linear(self.transformer_params.d_model, 1)          self.decoder = nn.Linear(self.history_length, self.horizon)      @property     def transformer_config(self) -&gt; dict:         \"\"\"all the param in dict format\"\"\"         return dataclasses.asdict(self.transformer_params)      def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         \"\"\"         :param x: input historical time series,             shape `[batch_size, seq_len, n_var]`         \"\"\"         x = self.embedding(x)         x = self.positional_encoding(x)          encoder_state = self.encoder(x)          decoder_in = self.reverse_embedding(encoder_state).squeeze(-1)          return self.decoder(decoder_in) <p>We use lightning to train our model.</p> In\u00a0[\u00a0]: Copied! <pre>history_length_1_step = 100\nhorizon_1_step = 1\n\ngap = 0\n</pre> history_length_1_step = 100 horizon_1_step = 1  gap = 0 <p>We will build a few utilities</p> <ol> <li>To be able to feed the data into our model, we build a class (<code>DataFrameDataset</code>) that converts the pandas dataframe into a Dataset for pytorch.</li> <li>To make the lightning training code simpler, we will build a LightningDataModule (<code>PendulumDataModule</code>) and a LightningModule (<code>TransformerForecaster</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>class TransformerForecaster(L.LightningModule):\n    \"\"\"Transformer forecasting training, validation,\n    and prediction all collected in one class.\n\n    :param transformer: pre-defined transformer model\n    \"\"\"\n\n    def __init__(self, transformer: nn.Module):\n        super().__init__()\n        self.transformer = transformer\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n\n        return optimizer\n\n    def training_step(self, batch: tuple[torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        x, y = batch\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.transformer(x)\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"train_loss\": loss}, prog_bar=True)\n        return loss\n\n    def validation_step(\n        self, batch: tuple[torch.Tensor], batch_idx: int\n    ) -&gt; torch.Tensor:\n        x, y = batch\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.transformer(x)\n\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log_dict({\"val_loss\": loss}, prog_bar=True)\n\n        return loss\n\n    def predict_step(\n        self, batch: list[torch.Tensor], batch_idx: int\n    ) -&gt; tuple[torch.Tensor]:\n        x, y = batch\n        y = y.squeeze(-1).type(self.dtype)\n\n        y_hat = self.transformer(x)\n\n        return x, y_hat\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor]:\n        return x, self.transformer(x)\n</pre> class TransformerForecaster(L.LightningModule):     \"\"\"Transformer forecasting training, validation,     and prediction all collected in one class.      :param transformer: pre-defined transformer model     \"\"\"      def __init__(self, transformer: nn.Module):         super().__init__()         self.transformer = transformer      def configure_optimizers(self) -&gt; torch.optim.Optimizer:         optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)          return optimizer      def training_step(self, batch: tuple[torch.Tensor], batch_idx: int) -&gt; torch.Tensor:         x, y = batch         y = y.squeeze(-1).type(self.dtype)          y_hat = self.transformer(x)          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"train_loss\": loss}, prog_bar=True)         return loss      def validation_step(         self, batch: tuple[torch.Tensor], batch_idx: int     ) -&gt; torch.Tensor:         x, y = batch         y = y.squeeze(-1).type(self.dtype)          y_hat = self.transformer(x)          loss = nn.functional.mse_loss(y_hat, y)         self.log_dict({\"val_loss\": loss}, prog_bar=True)          return loss      def predict_step(         self, batch: list[torch.Tensor], batch_idx: int     ) -&gt; tuple[torch.Tensor]:         x, y = batch         y = y.squeeze(-1).type(self.dtype)          y_hat = self.transformer(x)          return x, y_hat      def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor]:         return x, self.transformer(x) In\u00a0[\u00a0]: Copied! <pre>pdm_1_step = PendulumDataModule(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_1_step = PendulumDataModule(     history_length=history_length_1_step,     horizon=horizon_1_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_transformer_params_1_step = TSTransformerParams(\n    d_model=192, nhead=6, num_encoder_layers=1\n)\n\nts_transformer_1_step = TSTransformer(\n    history_length=history_length_1_step,\n    horizon=horizon_1_step,\n    transformer_params=ts_transformer_params_1_step,\n)\n\nts_transformer_1_step\n</pre> ts_transformer_params_1_step = TSTransformerParams(     d_model=192, nhead=6, num_encoder_layers=1 )  ts_transformer_1_step = TSTransformer(     history_length=history_length_1_step,     horizon=horizon_1_step,     transformer_params=ts_transformer_params_1_step, )  ts_transformer_1_step In\u00a0[\u00a0]: Copied! <pre>transformer_forecaster_1_step = TransformerForecaster(transformer=ts_transformer_1_step)\n\ntransformer_forecaster_1_step\n</pre> transformer_forecaster_1_step = TransformerForecaster(transformer=ts_transformer_1_step)  transformer_forecaster_1_step In\u00a0[\u00a0]: Copied! <pre>logger_1_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"transformer_ts_1_step\"\n)\n\n\ntrainer_1_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)\n    ],\n    logger=logger_1_step,\n)\n</pre> logger_1_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"transformer_ts_1_step\" )   trainer_1_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)     ],     logger=logger_1_step, ) In\u00a0[\u00a0]: Copied! <pre>demo_x = list(pdm_1_step.train_dataloader())[0][0].type(\n    transformer_forecaster_1_step.dtype\n)\ndemo_x.shape\n</pre> demo_x = list(pdm_1_step.train_dataloader())[0][0].type(     transformer_forecaster_1_step.dtype ) demo_x.shape In\u00a0[\u00a0]: Copied! <pre>nn.Linear(\n    1,\n    ts_transformer_1_step.transformer_params.d_model,\n    dtype=transformer_forecaster_1_step.dtype,\n)(demo_x).shape\n</pre> nn.Linear(     1,     ts_transformer_1_step.transformer_params.d_model,     dtype=transformer_forecaster_1_step.dtype, )(demo_x).shape In\u00a0[\u00a0]: Copied! <pre>ts_transformer_1_step.encoder(ts_transformer_1_step.embedding(demo_x)).shape\n</pre> ts_transformer_1_step.encoder(ts_transformer_1_step.embedding(demo_x)).shape In\u00a0[\u00a0]: Copied! <pre>trainer_1_step.fit(model=transformer_forecaster_1_step, datamodule=pdm_1_step)\n</pre> trainer_1_step.fit(model=transformer_forecaster_1_step, datamodule=pdm_1_step) In\u00a0[\u00a0]: Copied! <pre>predictions_1_step = trainer_1_step.predict(\n    model=transformer_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> predictions_1_step = trainer_1_step.predict(     model=transformer_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_1_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step)\nlobs_1_step_predictions = trainer_naive_1_step.predict(\n    model=lobs_forecaster_1_step, datamodule=pdm_1_step\n)\n</pre> trainer_naive_1_step = L.Trainer(precision=\"64\")  lobs_forecaster_1_step = LastObservationForecaster(horizon=horizon_1_step) lobs_1_step_predictions = trainer_naive_1_step.predict(     model=lobs_forecaster_1_step, datamodule=pdm_1_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_1_step = Evaluator(step=0)\n</pre> evaluator_1_step = Evaluator(step=0) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(50, 6.18))\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(50, 6.18))  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_1_step.y(predictions_1_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_1_step.y(lobs_1_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\ninspection_slice_length = 200\n\nax.plot(\n    evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader())[\n        :inspection_slice_length\n    ],\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(\n    evaluator_1_step.y(predictions_1_step)[:inspection_slice_length],\n    \"r--\",\n    label=\"predictions\",\n)\n\nax.plot(\n    evaluator_1_step.y(lobs_1_step_predictions)[:inspection_slice_length],\n    \"b-.\",\n    label=\"naive predictions\",\n)\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  inspection_slice_length = 200  ax.plot(     evaluator_1_step.y_true(dataloader=pdm_1_step.predict_dataloader())[         :inspection_slice_length     ],     \"g-\",     label=\"truth\", )  ax.plot(     evaluator_1_step.y(predictions_1_step)[:inspection_slice_length],     \"r--\",     label=\"predictions\", )  ax.plot(     evaluator_1_step.y(lobs_1_step_predictions)[:inspection_slice_length],     \"b-.\",     label=\"naive predictions\", )  plt.legend() <p>To quantify the results, we compute a few metrics.</p> In\u00a0[\u00a0]: Copied! <pre>pd.merge(\n    evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()),\n    evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()),\n    how=\"left\",\n    left_index=True,\n    right_index=True,\n    suffixes=[\"_transformer\", \"_naive\"],\n)\n</pre> pd.merge(     evaluator_1_step.metrics(predictions_1_step, pdm_1_step.predict_dataloader()),     evaluator_1_step.metrics(lobs_1_step_predictions, pdm_1_step.predict_dataloader()),     how=\"left\",     left_index=True,     right_index=True,     suffixes=[\"_transformer\", \"_naive\"], ) <p>Here SMAPE is better because of better forecasts for larger values</p> In\u00a0[\u00a0]: Copied! <pre>history_length_m_step = 100\nhorizon_m_step = 3\n</pre> history_length_m_step = 100 horizon_m_step = 3 In\u00a0[\u00a0]: Copied! <pre>pdm_m_step = PendulumDataModule(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    dataframe=df[[\"theta\"]],\n    gap=gap,\n)\n</pre> pdm_m_step = PendulumDataModule(     history_length=history_length_m_step,     horizon=horizon_m_step,     dataframe=df[[\"theta\"]],     gap=gap, ) In\u00a0[\u00a0]: Copied! <pre>ts_transformer_params_m_step = TSTransformerParams(\n    d_model=192, nhead=6, num_encoder_layers=1\n)\n\nts_transformer_m_step = TSTransformer(\n    history_length=history_length_m_step,\n    horizon=horizon_m_step,\n    transformer_params=ts_transformer_params_m_step,\n)\n\nts_transformer_m_step\n</pre> ts_transformer_params_m_step = TSTransformerParams(     d_model=192, nhead=6, num_encoder_layers=1 )  ts_transformer_m_step = TSTransformer(     history_length=history_length_m_step,     horizon=horizon_m_step,     transformer_params=ts_transformer_params_m_step, )  ts_transformer_m_step In\u00a0[\u00a0]: Copied! <pre>transformer_forecaster_m_step = TransformerForecaster(transformer=ts_transformer_m_step)\n</pre> transformer_forecaster_m_step = TransformerForecaster(transformer=ts_transformer_m_step) In\u00a0[\u00a0]: Copied! <pre>logger_m_step = L.pytorch.loggers.TensorBoardLogger(\n    save_dir=\"lightning_logs\", name=\"transformer_ts_m_step\"\n)\n\n\ntrainer_m_step = L.Trainer(\n    precision=\"64\",\n    max_epochs=100,\n    min_epochs=5,\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)\n    ],\n    logger=logger_m_step,\n)\n</pre> logger_m_step = L.pytorch.loggers.TensorBoardLogger(     save_dir=\"lightning_logs\", name=\"transformer_ts_m_step\" )   trainer_m_step = L.Trainer(     precision=\"64\",     max_epochs=100,     min_epochs=5,     callbacks=[         EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=1e-7, patience=3)     ],     logger=logger_m_step, ) In\u00a0[\u00a0]: Copied! <pre>trainer_m_step.fit(model=transformer_forecaster_m_step, datamodule=pdm_m_step)\n</pre> trainer_m_step.fit(model=transformer_forecaster_m_step, datamodule=pdm_m_step) In\u00a0[\u00a0]: Copied! <pre>predictions_m_step = trainer_m_step.predict(\n    model=transformer_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> predictions_m_step = trainer_m_step.predict(     model=transformer_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>trainer_naive_m_step = L.Trainer(precision=\"64\")\n\nlobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step)\nlobs_m_step_predictions = trainer_naive_m_step.predict(\n    model=lobs_forecaster_m_step, datamodule=pdm_m_step\n)\n</pre> trainer_naive_m_step = L.Trainer(precision=\"64\")  lobs_forecaster_m_step = LastObservationForecaster(horizon=horizon_m_step) lobs_m_step_predictions = trainer_naive_m_step.predict(     model=lobs_forecaster_m_step, datamodule=pdm_m_step ) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step = Evaluator(step=2, gap=gap)\n</pre> evaluator_m_step = Evaluator(step=2, gap=gap) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nax.plot(\n    evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),\n    \"g-\",\n    label=\"truth\",\n)\n\nax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")\n\nax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  ax.plot(     evaluator_m_step.y_true(dataloader=pdm_m_step.predict_dataloader()),     \"g-\",     label=\"truth\", )  ax.plot(evaluator_m_step.y(predictions_m_step), \"r--\", label=\"predictions\")  ax.plot(evaluator_m_step.y(lobs_m_step_predictions), \"b-.\", label=\"naive predictions\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\n\nfor i in np.arange(0, 1000, 120):\n    evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i)\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))   for i in np.arange(0, 1000, 120):     evaluator_m_step.plot_one_sample(ax=ax, predictions=predictions_m_step, idx=i) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(predictions_m_step, pdm_m_step.predict_dataloader()) In\u00a0[\u00a0]: Copied! <pre>evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())\n</pre> evaluator_m_step.metrics(lobs_m_step_predictions, pdm_m_step.predict_dataloader())"}, {"location": "notebooks/transformer_timeseries_univariate/#transformer-for-univariate-time-series-forecasting", "title": "Transformer for Univariate Time Series Forecasting\u00b6", "text": "<p>In this notebook, we build a transformer using pytorch to forecast $\\sin$ function as a time series.</p>"}, {"location": "notebooks/transformer_timeseries_univariate/#data", "title": "Data\u00b6", "text": "<p>We create a dataset that models a damped pendulum. The pendulum is modelled as a damped harmonic oscillator, i.e.,</p> $$ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), $$<p>where $\\theta(t)$ is the angle of the pendulum at time $t$. The period $p$ is calculated using</p> $$ p = 2 \\pi \\sqrt(L / g), $$<p>with $L$ being the length of the pendulum and $g$ being the surface gravity.</p>"}, {"location": "notebooks/transformer_timeseries_univariate/#model", "title": "Model\u00b6", "text": "<p>In this section, we create the transformer model.</p>"}, {"location": "notebooks/transformer_timeseries_univariate/#training", "title": "Training\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#training-utilities", "title": "Training Utilities\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#data-model-and-training", "title": "Data, Model and Training\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#datamodule", "title": "DataModule\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#lightningmodule", "title": "LightningModule\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#trainer", "title": "Trainer\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#fitting", "title": "Fitting\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#retrieving-predictions", "title": "Retrieving Predictions\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#forecasting-horizon3", "title": "Forecasting (horizon=3)\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#train-a-model", "title": "Train a Model\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#naive-forecaster", "title": "Naive Forecaster\u00b6", "text": ""}, {"location": "notebooks/transformer_timeseries_univariate/#evaluations", "title": "Evaluations\u00b6", "text": ""}, {"location": "notebooks/tree_basics/", "title": "Tree Basics", "text": "In\u00a0[\u00a0]: Copied! <pre>import json\nfrom typing import List, Literal, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import tree\n\nsns.set()\n</pre> import json from typing import List, Literal, Union  import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import tree  sns.set() In\u00a0[\u00a0]: Copied! <pre>class WFHData:\n    \"\"\"\n    Generate a dataset about wheter to go to the office.\n\n    Go to the office, if and only if\n    - I am healthy,\n    - the weather is good,\n    - not holiday.\n\n    Represented in the feature values, this condition is `[1,1,0]`.\n    However, we also randomize the target value based on `randomize_prob`:\n\n    - `randomize_prob=0`: keep perfect data, no randomization\n    - `randomize_prob=1`: use the wrong target value. The rules are inverted.\n\n\n    ```python\n    wfh = WFHData(length=10)\n    ```\n\n    :param length: the number of data points to generate.\n    :param randomize_prob: the probability of randomizing the target values.\n        `0` indicates that we keep the perfect target value based on rules.\n    :param seed: random generator seed.\n    \"\"\"\n\n    def __init__(self, length: int, randomize_prob: int = 0, seed: int = 42):\n        self.randomize_prob = randomize_prob\n        self.length = length\n        self.rng = np.random.default_rng(seed)\n        self.x = self._generate_feature_values()\n        self.y = self._generate_target_values()\n\n    @property\n    def feature_names(self) -&gt; List[str]:\n        return [\"health\", \"weather\", \"holiday\"]\n\n    @property\n    def target_names(self) -&gt; List[str]:\n        return [\"go_to_office\"]\n\n    @property\n    def feature_dataframe(self) -&gt; pd.DataFrame:\n        return pd.DataFrame(self.x, columns=self.feature_names)\n\n    @property\n    def target_dataframe(self) -&gt; pd.DataFrame:\n        return pd.DataFrame(self.y, columns=self.target_names)\n\n    def _generate_feature_values(self) -&gt; List[List[Literal[0, 1]]]:\n        \"\"\"Generate the values for the three features\n\n        The values can only be either `0` or `1`.\n        \"\"\"\n\n        return self.rng.choice([0, 1], (self.length, len(self.feature_names))).tolist()\n\n    def _perfect_target(self) -&gt; List[Literal[0, 1]]:\n        \"\"\"Create target value based on rules:\n\n        Go to the office, if and only if\n        - I am healthy,\n        - the weather is good,\n        - not holiday.\n\n        Represented in the feature values, this condition is `[1,1,0]`.\n        \"\"\"\n        target = []\n        for i in self.x:\n            if i == [1, 1, 0]:\n                target.append(1)\n            else:\n                target.append(0)\n\n        return target\n\n    @staticmethod\n    def _randomize_target(y, rng, probability: float) -&gt; Literal[0, 1]:\n        \"\"\"Randomly choose from the current value `y` and its alternative.\n        For example, if current value of `y=0`, its alternative is `1`.\n        We will randomly choose from `0` and `1` based on the specified probability.\n\n        If `probability=0`, we return the current value, i.e., `0`.\n        If `probability=0`, we return the alternative value, i.e., `1`.\n        Otherwise, it is randomly selected based on the probability.\n        \"\"\"\n        alternative_y = 1 if y == 0 else 0\n\n        return rng.choice(\n            [y, alternative_y], 1, p=(1 - probability, probability)\n        ).item()\n\n    def _generate_target_values(self) -&gt; List[Literal[0, 1]]:\n        \"\"\"Generate the target values\"\"\"\n        y = self._perfect_target()\n        y = [self._randomize_target(i, self.rng, self.randomize_prob) for i in y]\n\n        return y\n</pre> class WFHData:     \"\"\"     Generate a dataset about wheter to go to the office.      Go to the office, if and only if     - I am healthy,     - the weather is good,     - not holiday.      Represented in the feature values, this condition is `[1,1,0]`.     However, we also randomize the target value based on `randomize_prob`:      - `randomize_prob=0`: keep perfect data, no randomization     - `randomize_prob=1`: use the wrong target value. The rules are inverted.       ```python     wfh = WFHData(length=10)     ```      :param length: the number of data points to generate.     :param randomize_prob: the probability of randomizing the target values.         `0` indicates that we keep the perfect target value based on rules.     :param seed: random generator seed.     \"\"\"      def __init__(self, length: int, randomize_prob: int = 0, seed: int = 42):         self.randomize_prob = randomize_prob         self.length = length         self.rng = np.random.default_rng(seed)         self.x = self._generate_feature_values()         self.y = self._generate_target_values()      @property     def feature_names(self) -&gt; List[str]:         return [\"health\", \"weather\", \"holiday\"]      @property     def target_names(self) -&gt; List[str]:         return [\"go_to_office\"]      @property     def feature_dataframe(self) -&gt; pd.DataFrame:         return pd.DataFrame(self.x, columns=self.feature_names)      @property     def target_dataframe(self) -&gt; pd.DataFrame:         return pd.DataFrame(self.y, columns=self.target_names)      def _generate_feature_values(self) -&gt; List[List[Literal[0, 1]]]:         \"\"\"Generate the values for the three features          The values can only be either `0` or `1`.         \"\"\"          return self.rng.choice([0, 1], (self.length, len(self.feature_names))).tolist()      def _perfect_target(self) -&gt; List[Literal[0, 1]]:         \"\"\"Create target value based on rules:          Go to the office, if and only if         - I am healthy,         - the weather is good,         - not holiday.          Represented in the feature values, this condition is `[1,1,0]`.         \"\"\"         target = []         for i in self.x:             if i == [1, 1, 0]:                 target.append(1)             else:                 target.append(0)          return target      @staticmethod     def _randomize_target(y, rng, probability: float) -&gt; Literal[0, 1]:         \"\"\"Randomly choose from the current value `y` and its alternative.         For example, if current value of `y=0`, its alternative is `1`.         We will randomly choose from `0` and `1` based on the specified probability.          If `probability=0`, we return the current value, i.e., `0`.         If `probability=0`, we return the alternative value, i.e., `1`.         Otherwise, it is randomly selected based on the probability.         \"\"\"         alternative_y = 1 if y == 0 else 0          return rng.choice(             [y, alternative_y], 1, p=(1 - probability, probability)         ).item()      def _generate_target_values(self) -&gt; List[Literal[0, 1]]:         \"\"\"Generate the target values\"\"\"         y = self._perfect_target()         y = [self._randomize_target(i, self.rng, self.randomize_prob) for i in y]          return y In\u00a0[\u00a0]: Copied! <pre>wfh_demo = WFHData(length=10)\n</pre> wfh_demo = WFHData(length=10) In\u00a0[\u00a0]: Copied! <pre>wfh_demo.feature_dataframe\n</pre> wfh_demo.feature_dataframe In\u00a0[\u00a0]: Copied! <pre>wfh_demo.target_dataframe\n</pre> wfh_demo.target_dataframe In\u00a0[\u00a0]: Copied! <pre>wfh_pure = WFHData(length=100, randomize_prob=0)\n</pre> wfh_pure = WFHData(length=100, randomize_prob=0) In\u00a0[\u00a0]: Copied! <pre>clf_pure = tree.DecisionTreeClassifier()\nclf_pure.fit(wfh_pure.feature_dataframe, wfh_pure.target_dataframe)\n</pre> clf_pure = tree.DecisionTreeClassifier() clf_pure.fit(wfh_pure.feature_dataframe, wfh_pure.target_dataframe) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(15, 15))\ntree.plot_tree(clf_pure, feature_names=wfh_pure.feature_names, ax=ax)\nax.set_title(\"Tree Trained on Perfect Data\")\n</pre> fig, ax = plt.subplots(figsize=(15, 15)) tree.plot_tree(clf_pure, feature_names=wfh_pure.feature_names, ax=ax) ax.set_title(\"Tree Trained on Perfect Data\") In\u00a0[\u00a0]: Copied! <pre>wfh_impure = WFHData(length=100, randomize_prob=0.1)\n</pre> wfh_impure = WFHData(length=100, randomize_prob=0.1) In\u00a0[\u00a0]: Copied! <pre>clf_impure = tree.DecisionTreeClassifier(\n    max_depth=20, min_samples_leaf=1, min_samples_split=0.0001\n)\nclf_impure.fit(wfh_impure.feature_dataframe, wfh_impure.target_dataframe)\n</pre> clf_impure = tree.DecisionTreeClassifier(     max_depth=20, min_samples_leaf=1, min_samples_split=0.0001 ) clf_impure.fit(wfh_impure.feature_dataframe, wfh_impure.target_dataframe) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(15, 10))\ntree.plot_tree(clf_impure, feature_names=wfh_impure.feature_names, ax=ax)\nax.set_title(\"Tree Trained on Imperfect Data\")\n</pre> fig, ax = plt.subplots(figsize=(15, 10)) tree.plot_tree(clf_impure, feature_names=wfh_impure.feature_names, ax=ax) ax.set_title(\"Tree Trained on Imperfect Data\") In\u00a0[\u00a0]: Copied! <pre>def gini_2(p1: float, p2: float) -&gt; Union[None, float]:\n    \"\"\"Compute the Gini impurity for the two input values.\"\"\"\n    if p1 + p2 &lt;= 1:\n        return p1 * (1 - p1) + p2 * (1 - p2)\n    else:\n        return None\n</pre> def gini_2(p1: float, p2: float) -&gt; Union[None, float]:     \"\"\"Compute the Gini impurity for the two input values.\"\"\"     if p1 + p2 &lt;= 1:         return p1 * (1 - p1) + p2 * (1 - p2)     else:         return None In\u00a0[\u00a0]: Copied! <pre>gini_2_test_p1 = np.linspace(0, 1, 1001)\ngini_2_test_p2 = np.linspace(0, 1, 1001)\n</pre> gini_2_test_p1 = np.linspace(0, 1, 1001) gini_2_test_p2 = np.linspace(0, 1, 1001) In\u00a0[\u00a0]: Copied! <pre>gini_2_test_impurity = [\n    [gini_2(p1, p2) for p1 in gini_2_test_p1] for p2 in gini_2_test_p2\n]\n</pre> gini_2_test_impurity = [     [gini_2(p1, p2) for p1 in gini_2_test_p1] for p2 in gini_2_test_p2 ] In\u00a0[\u00a0]: Copied! <pre>df_gini_2_test = pd.DataFrame(\n    gini_2_test_impurity,\n    index=[f\"{i:0.2f}\" for i in gini_2_test_p2],\n    columns=[f\"{i:0.2f}\" for i in gini_2_test_p1],\n)\n</pre> df_gini_2_test = pd.DataFrame(     gini_2_test_impurity,     index=[f\"{i:0.2f}\" for i in gini_2_test_p2],     columns=[f\"{i:0.2f}\" for i in gini_2_test_p1], ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(df_gini_2_test.loc[::-1,], ax=ax)\nax.set_xlabel(\"$p_1$\")\nax.set_ylabel(\"$p_2$\")\nax.set_title(\"Gini Impurity for Data with 2 Possible Values\")\n</pre> fig, ax = plt.subplots(figsize=(12, 10)) sns.heatmap(df_gini_2_test.loc[::-1,], ax=ax) ax.set_xlabel(\"$p_1$\") ax.set_ylabel(\"$p_2$\") ax.set_title(\"Gini Impurity for Data with 2 Possible Values\") In\u00a0[\u00a0]: Copied! <pre>def gini_3(p1: float, p2: float) -&gt; Union[None, float]:\n    \"\"\"Computes the gini impurity for three potential classes\"\"\"\n    if p1 + p2 &lt;= 1:\n        return p1 * (1 - p1) + p2 * (1 - p2) + (1 - p1 - p2) * (p1 + p2)\n    else:\n        return None\n</pre> def gini_3(p1: float, p2: float) -&gt; Union[None, float]:     \"\"\"Computes the gini impurity for three potential classes\"\"\"     if p1 + p2 &lt;= 1:         return p1 * (1 - p1) + p2 * (1 - p2) + (1 - p1 - p2) * (p1 + p2)     else:         return None In\u00a0[\u00a0]: Copied! <pre>gini_3_test_p1 = np.linspace(0, 1, 1001)\ngini_3_test_p2 = np.linspace(0, 1, 1001)\ngini_3_test_impurity = [\n    [gini_3(p1, p2) for p1 in gini_3_test_p1] for p2 in gini_3_test_p2\n]\n\ndf_gini_3_test = pd.DataFrame(\n    gini_3_test_impurity,\n    index=[f\"{i:0.2f}\" for i in gini_3_test_p2],\n    columns=[f\"{i:0.2f}\" for i in gini_3_test_p1],\n)\n</pre> gini_3_test_p1 = np.linspace(0, 1, 1001) gini_3_test_p2 = np.linspace(0, 1, 1001) gini_3_test_impurity = [     [gini_3(p1, p2) for p1 in gini_3_test_p1] for p2 in gini_3_test_p2 ]  df_gini_3_test = pd.DataFrame(     gini_3_test_impurity,     index=[f\"{i:0.2f}\" for i in gini_3_test_p2],     columns=[f\"{i:0.2f}\" for i in gini_3_test_p1], ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(df_gini_3_test.loc[::-1,], ax=ax)\nax.set_xlabel(\"$p_1$\")\nax.set_ylabel(\"$p_2$\")\nax.set_title(\"Gini Impurity for Data with 3 Possible Values\")\n</pre> fig, ax = plt.subplots(figsize=(12, 10)) sns.heatmap(df_gini_3_test.loc[::-1,], ax=ax) ax.set_xlabel(\"$p_1$\") ax.set_ylabel(\"$p_2$\") ax.set_title(\"Gini Impurity for Data with 3 Possible Values\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/tree_basics/#tree-basics", "title": "Tree Basics\u00b6", "text": ""}, {"location": "notebooks/tree_basics/#generate-data", "title": "Generate Data\u00b6", "text": "<p>We generate some artificial dataset about whether to go to the office or work from home.</p> <p>We will use three features, <code>[\"health\", \"weather\", \"holiday\"]</code>. And people are only going to the office, iff</p> <ul> <li>health=1: not sick,</li> <li>weather=1: good weather,</li> <li>holiday=0: not holiday.</li> </ul> <p>We use <code>1</code> to indicate that we go to the office.</p>"}, {"location": "notebooks/tree_basics/#decision-tree-on-perfect-data", "title": "Decision Tree on Perfect Data\u00b6", "text": ""}, {"location": "notebooks/tree_basics/#impure-data", "title": "Impure Data\u00b6", "text": ""}, {"location": "notebooks/tree_basics/#understand-gini-impurity", "title": "Understand Gini Impurity\u00b6", "text": ""}, {"location": "notebooks/tree_basics/#gini-impurity-for-2-possible-classes", "title": "Gini Impurity for 2 possible classes\u00b6", "text": ""}, {"location": "notebooks/tree_basics/#gini-impurity-for-3-possible-classes", "title": "Gini Impurity for 3 possible classes\u00b6", "text": ""}, {"location": "notebooks/tree_darts_boosted_tree/", "title": "Forecasting with Boosted Trees Using Darts", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import Callable, Dict, List\n\nimport darts.utils as du\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom darts import TimeSeries, metrics\nfrom darts.dataprocessing.transformers import BoxCox\nfrom darts.datasets import AirPassengersDataset\nfrom darts.models import LightGBMModel, NaiveDrift\nfrom sklearn.linear_model import LinearRegression\n</pre> from typing import Callable, Dict, List  import darts.utils as du import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from darts import TimeSeries, metrics from darts.dataprocessing.transformers import BoxCox from darts.datasets import AirPassengersDataset from darts.models import LightGBMModel, NaiveDrift from sklearn.linear_model import LinearRegression In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_series = AirPassengersDataset().load()\ndarts_air_passenger_series.plot()\n</pre> darts_air_passenger_series = AirPassengersDataset().load() darts_air_passenger_series.plot() In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_series\n</pre> darts_air_passenger_series <p>From the outputs, we see that the time series dataset contains montly data for 144 months.</p> In\u00a0[\u00a0]: Copied! <pre>train_series_length = 120\ntest_series_length = len(darts_air_passenger_series) - train_series_length\ntrain_series_length, test_series_length\n</pre> train_series_length = 120 test_series_length = len(darts_air_passenger_series) - train_series_length train_series_length, test_series_length In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_train,\n    darts_air_passenger_test,\n) = darts_air_passenger_series.split_before(train_series_length)\n\ndarts_air_passenger_train.plot(label=\"Training Data\")\ndarts_air_passenger_test.plot(label=\"Test Data\")\n</pre> (     darts_air_passenger_train,     darts_air_passenger_test, ) = darts_air_passenger_series.split_before(train_series_length)  darts_air_passenger_train.plot(label=\"Training Data\") darts_air_passenger_test.plot(label=\"Test Data\") In\u00a0[\u00a0]: Copied! <pre>ap_horizon = len(darts_air_passenger_test)\nap_gbdt_params = dict(lags=52, output_chunk_length=ap_horizon)\n</pre> ap_horizon = len(darts_air_passenger_test) ap_gbdt_params = dict(lags=52, output_chunk_length=ap_horizon) In\u00a0[\u00a0]: Copied! <pre>gbdt_ap = LightGBMModel(**ap_gbdt_params)\n</pre> gbdt_ap = LightGBMModel(**ap_gbdt_params) In\u00a0[\u00a0]: Copied! <pre>gbdt_ap.fit(darts_air_passenger_train)\n</pre> gbdt_ap.fit(darts_air_passenger_train) <p>Insample predictions: We plot out the predictions for the last 24 days in the training data.</p> In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.drop_after(\n    darts_air_passenger_train.time_index[-ap_horizon]\n).plot(label=\"Prediction Input\")\ndarts_air_passenger_train.drop_before(\n    darts_air_passenger_train.time_index[-ap_horizon]\n).plot(label=\"True Values\")\ngbdt_ap.predict(\n    n=ap_horizon,\n    series=darts_air_passenger_train.drop_after(\n        darts_air_passenger_train.time_index[-ap_horizon]\n    ),\n).plot(label=\"Predictions (In-sample)\", linestyle=\"--\")\n</pre> darts_air_passenger_train.drop_after(     darts_air_passenger_train.time_index[-ap_horizon] ).plot(label=\"Prediction Input\") darts_air_passenger_train.drop_before(     darts_air_passenger_train.time_index[-ap_horizon] ).plot(label=\"True Values\") gbdt_ap.predict(     n=ap_horizon,     series=darts_air_passenger_train.drop_after(         darts_air_passenger_train.time_index[-ap_horizon]     ), ).plot(label=\"Predictions (In-sample)\", linestyle=\"--\") <p>To observe the actual performance, we plot out the predictions of the test dates.</p> In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot(label=\"Train\")\ndarts_air_passenger_test.plot(label=\"Test\")\npred_gbdt_ap = gbdt_ap.predict(n=ap_horizon)\npred_gbdt_ap.plot(label=\"Prediction\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot(label=\"Train\") darts_air_passenger_test.plot(label=\"Test\") pred_gbdt_ap = gbdt_ap.predict(n=ap_horizon) pred_gbdt_ap.plot(label=\"Prediction\", linestyle=\"--\") <p>We train the same model but with the detrended dataset, and reconstruct the predictions using the trend.</p> In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_trend,\n    darts_air_passenger_seasonal,\n) = du.statistics.extract_trend_and_seasonality(\n    darts_air_passenger_series,\n    #     model=du.utils.ModelMode.ADDITIVE,\n    #     method=\"STL\"\n)\n\ndarts_air_passenger_series.plot()\ndarts_air_passenger_trend.plot()\n(darts_air_passenger_trend * darts_air_passenger_seasonal).plot()\n</pre> (     darts_air_passenger_trend,     darts_air_passenger_seasonal, ) = du.statistics.extract_trend_and_seasonality(     darts_air_passenger_series,     #     model=du.utils.ModelMode.ADDITIVE,     #     method=\"STL\" )  darts_air_passenger_series.plot() darts_air_passenger_trend.plot() (darts_air_passenger_trend * darts_air_passenger_seasonal).plot() In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_seasonal_train,\n    darts_air_passenger_seasonal_test,\n) = darts_air_passenger_seasonal.split_before(120)\ndarts_air_passenger_seasonal_train.plot(label=\"Seasonal Component Train\")\ndarts_air_passenger_seasonal_test.plot(label=\"Seasonal Component Test\")\n</pre> (     darts_air_passenger_seasonal_train,     darts_air_passenger_seasonal_test, ) = darts_air_passenger_seasonal.split_before(120) darts_air_passenger_seasonal_train.plot(label=\"Seasonal Component Train\") darts_air_passenger_seasonal_test.plot(label=\"Seasonal Component Test\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.histplot(\n    darts_air_passenger_seasonal_train.pd_dataframe(),\n    x=\"0\",\n    kde=True,\n    binwidth=0.1,\n    binrange=(0.7, 1.3),\n    label=\"Training Distribution\",\n    stat=\"probability\",\n    #     fill=False,\n    ax=ax,\n)\nsns.histplot(\n    darts_air_passenger_seasonal_test.pd_dataframe(),\n    x=\"0\",\n    kde=True,\n    binwidth=0.1,\n    binrange=(0.7, 1.3),\n    label=\"Test Distribution\",\n    stat=\"probability\",\n    color=\"r\",\n    #     fill=False,\n    ax=ax,\n)\n\nax.set_xlabel(\"# Passengers\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.histplot(     darts_air_passenger_seasonal_train.pd_dataframe(),     x=\"0\",     kde=True,     binwidth=0.1,     binrange=(0.7, 1.3),     label=\"Training Distribution\",     stat=\"probability\",     #     fill=False,     ax=ax, ) sns.histplot(     darts_air_passenger_seasonal_test.pd_dataframe(),     x=\"0\",     kde=True,     binwidth=0.1,     binrange=(0.7, 1.3),     label=\"Test Distribution\",     stat=\"probability\",     color=\"r\",     #     fill=False,     ax=ax, )  ax.set_xlabel(\"# Passengers\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>gbdt_ap_seasonal = LightGBMModel(**ap_gbdt_params)\n</pre> gbdt_ap_seasonal = LightGBMModel(**ap_gbdt_params) In\u00a0[\u00a0]: Copied! <pre>gbdt_ap_seasonal.fit(darts_air_passenger_seasonal_train)\n</pre> gbdt_ap_seasonal.fit(darts_air_passenger_seasonal_train) In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot(label=\"Train\")\ndarts_air_passenger_test.plot(label=\"Test\")\npred_rf_ap_seasonal = gbdt_ap_seasonal.predict(\n    n=ap_horizon\n) * darts_air_passenger_trend.drop_before(119)\npred_rf_ap_seasonal.plot(label=\"Trend * Predicted Seasonal Component\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot(label=\"Train\") darts_air_passenger_test.plot(label=\"Test\") pred_rf_ap_seasonal = gbdt_ap_seasonal.predict(     n=ap_horizon ) * darts_air_passenger_trend.drop_before(119) pred_rf_ap_seasonal.plot(label=\"Trend * Predicted Seasonal Component\", linestyle=\"--\") <p>This indiates that the performance of trees on out of sample predictions if we only predict on the cycle part of the series. In a real world case, however, we have to predict the trend accurately for this to work. To better reconstruct the trend, there are also tricks like Box-Cox transformations.</p> <p>It is not easy to determine a best model simply looking at the charts. We need some metrics.</p> In\u00a0[\u00a0]: Copied! <pre>air_passenger_boxcox = BoxCox()\ndarts_air_passenger_train_boxcox = air_passenger_boxcox.fit_transform(\n    darts_air_passenger_train\n)\ndarts_air_passenger_test_boxcox = air_passenger_boxcox.transform(\n    darts_air_passenger_test\n)\ndarts_air_passenger_train_boxcox.plot(label=\"Train (Box-Cox Transformed)\")\ndarts_air_passenger_test_boxcox.plot(label=\"Test (Box-Cox Transformed)\")\n</pre> air_passenger_boxcox = BoxCox() darts_air_passenger_train_boxcox = air_passenger_boxcox.fit_transform(     darts_air_passenger_train ) darts_air_passenger_test_boxcox = air_passenger_boxcox.transform(     darts_air_passenger_test ) darts_air_passenger_train_boxcox.plot(label=\"Train (Box-Cox Transformed)\") darts_air_passenger_test_boxcox.plot(label=\"Test (Box-Cox Transformed)\") In\u00a0[\u00a0]: Copied! <pre>def linear_trend_model(series: TimeSeries) -&gt; LinearRegression:\n    \"\"\"Fit a linear trend of the series. This can be used to find the linear\n    model using training data.\n\n    :param series: training timeseries\n    \"\"\"\n    positional_index_start = 0\n    series_trend, _ = du.statistics.extract_trend_and_seasonality(series)\n    model = LinearRegression()\n    length = len(series_trend)\n    model.fit(\n        np.arange(positional_index_start, positional_index_start + length).reshape(\n            length, 1\n        ),\n        series_trend.values(),\n    )\n\n    return model\n\n\ndef find_linear_trend(\n    series: TimeSeries, model, positional_index_start: int = 0\n) -&gt; TimeSeries:\n    \"\"\"Using the fitted linear model to find or extrapolate the linear trend.\n\n    :param series: train or test timeseries\n    :param model: LinearRegression model that has `predict` method\n    :param positional_index_start: the position of the first value in the original timeseries.\n    \"\"\"\n    length = len(series)\n    linear_preds = model.predict(\n        np.arange(positional_index_start, positional_index_start + length).reshape(\n            length, 1\n        )\n    ).squeeze()\n\n    dataframe = pd.DataFrame(\n        {\"date\": series.time_index, \"# Passengers\": linear_preds}\n    ).set_index(\"date\")\n\n    return TimeSeries.from_dataframe(dataframe)\n</pre> def linear_trend_model(series: TimeSeries) -&gt; LinearRegression:     \"\"\"Fit a linear trend of the series. This can be used to find the linear     model using training data.      :param series: training timeseries     \"\"\"     positional_index_start = 0     series_trend, _ = du.statistics.extract_trend_and_seasonality(series)     model = LinearRegression()     length = len(series_trend)     model.fit(         np.arange(positional_index_start, positional_index_start + length).reshape(             length, 1         ),         series_trend.values(),     )      return model   def find_linear_trend(     series: TimeSeries, model, positional_index_start: int = 0 ) -&gt; TimeSeries:     \"\"\"Using the fitted linear model to find or extrapolate the linear trend.      :param series: train or test timeseries     :param model: LinearRegression model that has `predict` method     :param positional_index_start: the position of the first value in the original timeseries.     \"\"\"     length = len(series)     linear_preds = model.predict(         np.arange(positional_index_start, positional_index_start + length).reshape(             length, 1         )     ).squeeze()      dataframe = pd.DataFrame(         {\"date\": series.time_index, \"# Passengers\": linear_preds}     ).set_index(\"date\")      return TimeSeries.from_dataframe(dataframe) In\u00a0[\u00a0]: Copied! <pre>ap_trend_lm = linear_trend_model(darts_air_passenger_train_boxcox)\nap_trend_lm\n</pre> ap_trend_lm = linear_trend_model(darts_air_passenger_train_boxcox) ap_trend_lm In\u00a0[\u00a0]: Copied! <pre>ap_trend_linear_train = find_linear_trend(\n    model=ap_trend_lm, series=darts_air_passenger_train_boxcox\n)\nap_trend_linear_test = find_linear_trend(\n    model=ap_trend_lm,\n    series=darts_air_passenger_test_boxcox,\n    positional_index_start=train_series_length,\n)\n\ndarts_air_passenger_train_boxcox.plot(label=\"Train\")\nap_trend_linear_train.plot(label=\"Linear Trend (Train)\")\ndarts_air_passenger_test_boxcox.plot(label=\"Test\")\nap_trend_linear_test.plot(label=\"Linear Trend (Test)\")\n</pre> ap_trend_linear_train = find_linear_trend(     model=ap_trend_lm, series=darts_air_passenger_train_boxcox ) ap_trend_linear_test = find_linear_trend(     model=ap_trend_lm,     series=darts_air_passenger_test_boxcox,     positional_index_start=train_series_length, )  darts_air_passenger_train_boxcox.plot(label=\"Train\") ap_trend_linear_train.plot(label=\"Linear Trend (Train)\") darts_air_passenger_test_boxcox.plot(label=\"Test\") ap_trend_linear_test.plot(label=\"Linear Trend (Test)\") In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train_transformed = (\n    darts_air_passenger_train_boxcox - ap_trend_linear_train\n)\ndarts_air_passenger_train_transformed.plot()\n</pre> darts_air_passenger_train_transformed = (     darts_air_passenger_train_boxcox - ap_trend_linear_train ) darts_air_passenger_train_transformed.plot() In\u00a0[\u00a0]: Copied! <pre>gbdt_bc_lt = LightGBMModel(**ap_gbdt_params)\ngbdt_bc_lt.fit(darts_air_passenger_train_transformed)\n</pre> gbdt_bc_lt = LightGBMModel(**ap_gbdt_params) gbdt_bc_lt.fit(darts_air_passenger_train_transformed) In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot()\ndarts_air_passenger_test.plot()\npred_gbdt_bc_lt = air_passenger_boxcox.inverse_transform(\n    gbdt_bc_lt.predict(n=ap_horizon) + ap_trend_linear_test\n)\npred_gbdt_bc_lt.plot(label=\"Box-Cox + Linear Detrend Predictions\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot() darts_air_passenger_test.plot() pred_gbdt_bc_lt = air_passenger_boxcox.inverse_transform(     gbdt_bc_lt.predict(n=ap_horizon) + ap_trend_linear_test ) pred_gbdt_bc_lt.plot(label=\"Box-Cox + Linear Detrend Predictions\", linestyle=\"--\") <p>Detrending is not the only possibility. LightGBM implements a linear tree version of the base learners.</p> In\u00a0[\u00a0]: Copied! <pre>ap_gbdt_linear_tree_params = dict(\n    lags=52, output_chunk_length=ap_horizon, linear_tree=True\n)\n</pre> ap_gbdt_linear_tree_params = dict(     lags=52, output_chunk_length=ap_horizon, linear_tree=True ) In\u00a0[\u00a0]: Copied! <pre>gbdt_linear_tree_ap = LightGBMModel(**ap_gbdt_linear_tree_params)\n</pre> gbdt_linear_tree_ap = LightGBMModel(**ap_gbdt_linear_tree_params) In\u00a0[\u00a0]: Copied! <pre>gbdt_linear_tree_ap.fit(darts_air_passenger_train)\n</pre> gbdt_linear_tree_ap.fit(darts_air_passenger_train) In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot(label=\"Train\")\ndarts_air_passenger_test.plot(label=\"Test\")\npred_gbdt_linear_tree_ap = gbdt_linear_tree_ap.predict(n=ap_horizon)\npred_gbdt_linear_tree_ap.plot(label=\"Linear Tree Prediction\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot(label=\"Train\") darts_air_passenger_test.plot(label=\"Test\") pred_gbdt_linear_tree_ap = gbdt_linear_tree_ap.predict(n=ap_horizon) pred_gbdt_linear_tree_ap.plot(label=\"Linear Tree Prediction\", linestyle=\"--\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_test.plot(label=\"Test\")\npred_gbdt_ap.plot(label=\"Simple GBDT\", linestyle=\"--\")\npred_rf_ap_seasonal.plot(\n    label=\"GBDT on Global Detrended Data (Cheating)\", linestyle=\"--\"\n)\npred_gbdt_bc_lt.plot(label=\"GBDT on Box-Cox + Linear Detrend Data\", linestyle=\"--\")\npred_gbdt_linear_tree_ap.plot(label=\"Linear Tree|\", linestyle=\"--\", color=\"r\")\n</pre> darts_air_passenger_test.plot(label=\"Test\") pred_gbdt_ap.plot(label=\"Simple GBDT\", linestyle=\"--\") pred_rf_ap_seasonal.plot(     label=\"GBDT on Global Detrended Data (Cheating)\", linestyle=\"--\" ) pred_gbdt_bc_lt.plot(label=\"GBDT on Box-Cox + Linear Detrend Data\", linestyle=\"--\") pred_gbdt_linear_tree_ap.plot(label=\"Linear Tree|\", linestyle=\"--\", color=\"r\") In\u00a0[\u00a0]: Copied! <pre>benchmark_metrics = [\n    metrics.mae,\n    metrics.mape,\n    metrics.mse,\n    metrics.rmse,\n    metrics.smape,\n    metrics.r2_score,\n]\n</pre> benchmark_metrics = [     metrics.mae,     metrics.mape,     metrics.mse,     metrics.rmse,     metrics.smape,     metrics.r2_score, ] In\u00a0[\u00a0]: Copied! <pre>def benchmark_predictions(\n    series_true: TimeSeries,\n    series_prediction: TimeSeries,\n    metrics: List[Callable],\n    experiment_id: str,\n) -&gt; Dict:\n    results = []\n    for m in benchmark_metrics:\n        results.append(\n            {\n                \"metric\": f\"{m.__name__}\",\n                \"value\": m(series_true, series_prediction),\n                \"experiment\": experiment_id,\n            }\n        )\n\n    return results\n</pre> def benchmark_predictions(     series_true: TimeSeries,     series_prediction: TimeSeries,     metrics: List[Callable],     experiment_id: str, ) -&gt; Dict:     results = []     for m in benchmark_metrics:         results.append(             {                 \"metric\": f\"{m.__name__}\",                 \"value\": m(series_true, series_prediction),                 \"experiment\": experiment_id,             }         )      return results In\u00a0[\u00a0]: Copied! <pre>benchmark_results = []\n\nfor i, pred in zip(\n    [\"simple_gbdt\", \"detrended_cheating\", \"boxcox_linear_trend\", \"linear_tree\"],\n    [pred_gbdt_ap, pred_rf_ap_seasonal, pred_gbdt_bc_lt, pred_gbdt_linear_tree_ap],\n):\n    benchmark_results += benchmark_predictions(\n        series_true=darts_air_passenger_test,\n        series_prediction=pred,\n        metrics=benchmark_metrics,\n        experiment_id=i,\n    )\n\ndf_benchmark_metrics = pd.DataFrame(benchmark_results)\ndf_benchmark_metrics\n</pre> benchmark_results = []  for i, pred in zip(     [\"simple_gbdt\", \"detrended_cheating\", \"boxcox_linear_trend\", \"linear_tree\"],     [pred_gbdt_ap, pred_rf_ap_seasonal, pred_gbdt_bc_lt, pred_gbdt_linear_tree_ap], ):     benchmark_results += benchmark_predictions(         series_true=darts_air_passenger_test,         series_prediction=pred,         metrics=benchmark_metrics,         experiment_id=i,     )  df_benchmark_metrics = pd.DataFrame(benchmark_results) df_benchmark_metrics In\u00a0[\u00a0]: Copied! <pre>metric_chart_grid = sns.FacetGrid(\n    df_benchmark_metrics,\n    col=\"metric\",\n    hue=\"metric\",\n    col_wrap=2,\n    height=4,\n    aspect=1 / 0.618,\n    sharey=False,\n)\n\nmetric_chart_grid.map(\n    sns.barplot, \"experiment\", \"value\", order=df_benchmark_metrics.experiment.unique()\n)\n# for axes in metric_chart_grid.axes.flat:\n#     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n# metric_chart_grid.fig.tight_layout(w_pad=1)\n</pre> metric_chart_grid = sns.FacetGrid(     df_benchmark_metrics,     col=\"metric\",     hue=\"metric\",     col_wrap=2,     height=4,     aspect=1 / 0.618,     sharey=False, )  metric_chart_grid.map(     sns.barplot, \"experiment\", \"value\", order=df_benchmark_metrics.experiment.unique() ) # for axes in metric_chart_grid.axes.flat: #     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90) # metric_chart_grid.fig.tight_layout(w_pad=1) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/tree_darts_boosted_tree/#forecasting-with-boosted-trees-using-darts", "title": "Forecasting with Boosted Trees Using Darts\u00b6", "text": "<p>In this notebook, we explore some basic ideas of how to forecast using trees with the help of the package called Darts.</p>"}, {"location": "notebooks/tree_darts_boosted_tree/#following-the-darts-official-tutorial", "title": "Following the Darts Official Tutorial\u00b6", "text": "<p>Darts provides a tutorial here to help the users get started. Here we replicate some of them to provide a minimal working example for tree-based models.</p>"}, {"location": "notebooks/tree_darts_boosted_tree/#first-random-forest-model", "title": "First Random Forest Model\u00b6", "text": ""}, {"location": "notebooks/tree_darts_boosted_tree/#detrending-helps", "title": "Detrending Helps\u00b6", "text": ""}, {"location": "notebooks/tree_darts_boosted_tree/#train-test-and-metrics", "title": "Train, Test, and Metrics\u00b6", "text": ""}, {"location": "notebooks/tree_darts_boosted_tree/#linear-tree-horizon", "title": "Linear Tree Horizon\u00b6", "text": ""}, {"location": "notebooks/tree_darts_boosted_tree/#metrics", "title": "Metrics\u00b6", "text": ""}, {"location": "notebooks/tree_darts_random_forest/", "title": "Forecasting with Trees Using Darts", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import Callable, Dict, List\n</pre> from typing import Callable, Dict, List In\u00a0[\u00a0]: Copied! <pre>import darts.utils as du\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom darts import TimeSeries, metrics\nfrom darts.dataprocessing.transformers import BoxCox\nfrom darts.datasets import AirPassengersDataset\nfrom darts.models import LightGBMModel, NaiveDrift, RandomForest\nfrom sklearn.linear_model import LinearRegression\n</pre> import darts.utils as du import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from darts import TimeSeries, metrics from darts.dataprocessing.transformers import BoxCox from darts.datasets import AirPassengersDataset from darts.models import LightGBMModel, NaiveDrift, RandomForest from sklearn.linear_model import LinearRegression In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_series = AirPassengersDataset().load()\ndarts_air_passenger_series.plot()\n</pre> darts_air_passenger_series = AirPassengersDataset().load() darts_air_passenger_series.plot() In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_series\n</pre> darts_air_passenger_series <p>From the outputs, we see that the time series dataset contains montly data for 144 months.</p> In\u00a0[\u00a0]: Copied! <pre>train_series_length = 120\ntest_series_length = len(darts_air_passenger_series) - train_series_length\ntrain_series_length, test_series_length\n</pre> train_series_length = 120 test_series_length = len(darts_air_passenger_series) - train_series_length train_series_length, test_series_length In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_train,\n    darts_air_passenger_test,\n) = darts_air_passenger_series.split_before(train_series_length)\n\ndarts_air_passenger_train.plot(label=\"Training Data\")\ndarts_air_passenger_test.plot(label=\"Test Data\")\n</pre> (     darts_air_passenger_train,     darts_air_passenger_test, ) = darts_air_passenger_series.split_before(train_series_length)  darts_air_passenger_train.plot(label=\"Training Data\") darts_air_passenger_test.plot(label=\"Test Data\") In\u00a0[\u00a0]: Copied! <pre>ap_horizon = len(darts_air_passenger_test)\nap_rf_params = dict(lags=52, output_chunk_length=ap_horizon)\n</pre> ap_horizon = len(darts_air_passenger_test) ap_rf_params = dict(lags=52, output_chunk_length=ap_horizon) In\u00a0[\u00a0]: Copied! <pre>rf_ap = RandomForest(**ap_rf_params)\n</pre> rf_ap = RandomForest(**ap_rf_params) In\u00a0[\u00a0]: Copied! <pre>rf_ap.fit(darts_air_passenger_train)\n</pre> rf_ap.fit(darts_air_passenger_train) <p>To observe how the model performs on the training data, we predict a time range that has already seen by the model during training.</p> In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.drop_after(\n    darts_air_passenger_train.time_index[-ap_horizon]\n).plot(label=\"Prediction Input\")\ndarts_air_passenger_train.drop_before(\n    darts_air_passenger_train.time_index[-ap_horizon]\n).plot(label=\"True Values\")\nrf_ap.predict(\n    n=ap_horizon,\n    series=darts_air_passenger_train.drop_after(\n        darts_air_passenger_train.time_index[-ap_horizon]\n    ),\n).plot(label=\"Predictions (In-sample)\", linestyle=\"--\")\n</pre> darts_air_passenger_train.drop_after(     darts_air_passenger_train.time_index[-ap_horizon] ).plot(label=\"Prediction Input\") darts_air_passenger_train.drop_before(     darts_air_passenger_train.time_index[-ap_horizon] ).plot(label=\"True Values\") rf_ap.predict(     n=ap_horizon,     series=darts_air_passenger_train.drop_after(         darts_air_passenger_train.time_index[-ap_horizon]     ), ).plot(label=\"Predictions (In-sample)\", linestyle=\"--\") <p>The predictions looks amazing. However, we all know that tree-based models are not good at out of sample extrapolations. In our case, the trend of the time series may cause some problems. To test this idea, we plot out the predictions for the test date range.</p> In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot(label=\"Train\")\ndarts_air_passenger_test.plot(label=\"Test\")\npred_rf_ap = rf_ap.predict(n=ap_horizon)\npred_rf_ap.plot(label=\"Prediction\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot(label=\"Train\") darts_air_passenger_test.plot(label=\"Test\") pred_rf_ap = rf_ap.predict(n=ap_horizon) pred_rf_ap.plot(label=\"Prediction\", linestyle=\"--\") <p>We train the same model but with the detrended dataset, and reconstruct the predictions using the trend. This method demonstrate that detrended data is easier for random forest.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.histplot(\n    darts_air_passenger_train.pd_dataframe(),\n    x=\"#Passengers\",\n    kde=True,\n    binwidth=50,\n    binrange=(0, 700),\n    label=\"Training Distribution\",\n    stat=\"probability\",\n    ax=ax,\n)\nsns.histplot(\n    darts_air_passenger_test.pd_dataframe(),\n    x=\"#Passengers\",\n    kde=True,\n    binwidth=50,\n    binrange=(0, 700),\n    label=\"Test Distribution\",\n    stat=\"probability\",\n    color=\"r\",\n    ax=ax,\n)\n\nax.set_xlabel(\"# Passengers\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.histplot(     darts_air_passenger_train.pd_dataframe(),     x=\"#Passengers\",     kde=True,     binwidth=50,     binrange=(0, 700),     label=\"Training Distribution\",     stat=\"probability\",     ax=ax, ) sns.histplot(     darts_air_passenger_test.pd_dataframe(),     x=\"#Passengers\",     kde=True,     binwidth=50,     binrange=(0, 700),     label=\"Test Distribution\",     stat=\"probability\",     color=\"r\",     ax=ax, )  ax.set_xlabel(\"# Passengers\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_trend,\n    darts_air_passenger_seasonal,\n) = du.statistics.extract_trend_and_seasonality(\n    darts_air_passenger_series,\n    #     model=du.utils.ModelMode.ADDITIVE,\n    #     method=\"STL\"\n)\n\ndarts_air_passenger_series.plot()\ndarts_air_passenger_trend.plot()\n(darts_air_passenger_trend * darts_air_passenger_seasonal).plot()\n</pre> (     darts_air_passenger_trend,     darts_air_passenger_seasonal, ) = du.statistics.extract_trend_and_seasonality(     darts_air_passenger_series,     #     model=du.utils.ModelMode.ADDITIVE,     #     method=\"STL\" )  darts_air_passenger_series.plot() darts_air_passenger_trend.plot() (darts_air_passenger_trend * darts_air_passenger_seasonal).plot() In\u00a0[\u00a0]: Copied! <pre>(\n    darts_air_passenger_seasonal_train,\n    darts_air_passenger_seasonal_test,\n) = darts_air_passenger_seasonal.split_before(120)\ndarts_air_passenger_seasonal_train.plot(label=\"Seasonal Component Train\")\ndarts_air_passenger_seasonal_test.plot(label=\"Seasonal Component Test\")\n</pre> (     darts_air_passenger_seasonal_train,     darts_air_passenger_seasonal_test, ) = darts_air_passenger_seasonal.split_before(120) darts_air_passenger_seasonal_train.plot(label=\"Seasonal Component Train\") darts_air_passenger_seasonal_test.plot(label=\"Seasonal Component Test\") In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_seasonal_test.pd_dataframe()\n</pre> darts_air_passenger_seasonal_test.pd_dataframe() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.histplot(\n    darts_air_passenger_seasonal_train.pd_dataframe(),\n    x=\"0\",\n    kde=True,\n    binwidth=0.1,\n    binrange=(0.7, 1.3),\n    label=\"Training Distribution\",\n    stat=\"probability\",\n    #     fill=False,\n    ax=ax,\n)\nsns.histplot(\n    darts_air_passenger_seasonal_test.pd_dataframe(),\n    x=\"0\",\n    kde=True,\n    binwidth=0.1,\n    binrange=(0.7, 1.3),\n    label=\"Test Distribution\",\n    stat=\"probability\",\n    color=\"r\",\n    #     fill=False,\n    ax=ax,\n)\n\nax.set_xlabel(\"# Passengers\")\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.histplot(     darts_air_passenger_seasonal_train.pd_dataframe(),     x=\"0\",     kde=True,     binwidth=0.1,     binrange=(0.7, 1.3),     label=\"Training Distribution\",     stat=\"probability\",     #     fill=False,     ax=ax, ) sns.histplot(     darts_air_passenger_seasonal_test.pd_dataframe(),     x=\"0\",     kde=True,     binwidth=0.1,     binrange=(0.7, 1.3),     label=\"Test Distribution\",     stat=\"probability\",     color=\"r\",     #     fill=False,     ax=ax, )  ax.set_xlabel(\"# Passengers\")  plt.legend() In\u00a0[\u00a0]: Copied! <pre>rf_ap_seasonal = RandomForest(**ap_rf_params)\n</pre> rf_ap_seasonal = RandomForest(**ap_rf_params) In\u00a0[\u00a0]: Copied! <pre>rf_ap_seasonal.fit(darts_air_passenger_seasonal_train)\n</pre> rf_ap_seasonal.fit(darts_air_passenger_seasonal_train) In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot(label=\"Train\")\ndarts_air_passenger_test.plot(label=\"Test\")\npred_rf_ap_seasonal = rf_ap_seasonal.predict(\n    n=ap_horizon\n) * darts_air_passenger_trend.drop_before(119)\npred_rf_ap_seasonal.plot(label=\"Trend * Predicted Seasonal Component\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot(label=\"Train\") darts_air_passenger_test.plot(label=\"Test\") pred_rf_ap_seasonal = rf_ap_seasonal.predict(     n=ap_horizon ) * darts_air_passenger_trend.drop_before(119) pred_rf_ap_seasonal.plot(label=\"Trend * Predicted Seasonal Component\", linestyle=\"--\") <p>This indiates that the performance of trees on out of sample predictions if we only predict on the cycle part of the series. In a real world case, however, we have to predict the trend accurately for this to work. To better reconstruct the trend, there are also tricks like Box-Cox transformations.</p> <p>It is not easy to determine a best model simply looking at the charts. We need some metrics.</p> In\u00a0[\u00a0]: Copied! <pre>air_passenger_boxcox = BoxCox()\ndarts_air_passenger_train_boxcox = air_passenger_boxcox.fit_transform(\n    darts_air_passenger_train\n)\ndarts_air_passenger_test_boxcox = air_passenger_boxcox.transform(\n    darts_air_passenger_test\n)\ndarts_air_passenger_train_boxcox.plot(label=\"Train (Box-Cox Transformed)\")\ndarts_air_passenger_test_boxcox.plot(label=\"Test (Box-Cox Transformed)\")\n</pre> air_passenger_boxcox = BoxCox() darts_air_passenger_train_boxcox = air_passenger_boxcox.fit_transform(     darts_air_passenger_train ) darts_air_passenger_test_boxcox = air_passenger_boxcox.transform(     darts_air_passenger_test ) darts_air_passenger_train_boxcox.plot(label=\"Train (Box-Cox Transformed)\") darts_air_passenger_test_boxcox.plot(label=\"Test (Box-Cox Transformed)\") In\u00a0[\u00a0]: Copied! <pre>def linear_trend_model(series: TimeSeries) -&gt; LinearRegression:\n    \"\"\"Fit a linear trend of the series. This can be used to find the linear\n    model using training data.\n\n    :param series: training timeseries\n    \"\"\"\n    positional_index_start = 0\n    series_trend, _ = du.statistics.extract_trend_and_seasonality(series)\n    model = LinearRegression()\n    length = len(series_trend)\n    model.fit(\n        np.arange(positional_index_start, positional_index_start + length).reshape(\n            length, 1\n        ),\n        series_trend.values(),\n    )\n\n    return model\n\n\ndef find_linear_trend(\n    series: TimeSeries, model, positional_index_start: int = 0\n) -&gt; TimeSeries:\n    \"\"\"Using the fitted linear model to find or extrapolate the linear trend.\n\n    :param series: train or test timeseries\n    :param model: LinearRegression model that has `predict` method\n    :param positional_index_start: the position of the first value in the original timeseries.\n    \"\"\"\n    length = len(series)\n    linear_preds = model.predict(\n        np.arange(positional_index_start, positional_index_start + length).reshape(\n            length, 1\n        )\n    ).squeeze()\n\n    dataframe = pd.DataFrame(\n        {\"date\": series.time_index, \"# Passengers\": linear_preds}\n    ).set_index(\"date\")\n\n    return TimeSeries.from_dataframe(dataframe)\n</pre> def linear_trend_model(series: TimeSeries) -&gt; LinearRegression:     \"\"\"Fit a linear trend of the series. This can be used to find the linear     model using training data.      :param series: training timeseries     \"\"\"     positional_index_start = 0     series_trend, _ = du.statistics.extract_trend_and_seasonality(series)     model = LinearRegression()     length = len(series_trend)     model.fit(         np.arange(positional_index_start, positional_index_start + length).reshape(             length, 1         ),         series_trend.values(),     )      return model   def find_linear_trend(     series: TimeSeries, model, positional_index_start: int = 0 ) -&gt; TimeSeries:     \"\"\"Using the fitted linear model to find or extrapolate the linear trend.      :param series: train or test timeseries     :param model: LinearRegression model that has `predict` method     :param positional_index_start: the position of the first value in the original timeseries.     \"\"\"     length = len(series)     linear_preds = model.predict(         np.arange(positional_index_start, positional_index_start + length).reshape(             length, 1         )     ).squeeze()      dataframe = pd.DataFrame(         {\"date\": series.time_index, \"# Passengers\": linear_preds}     ).set_index(\"date\")      return TimeSeries.from_dataframe(dataframe) In\u00a0[\u00a0]: Copied! <pre>ap_trend_lm = linear_trend_model(darts_air_passenger_train_boxcox)\nap_trend_lm\n</pre> ap_trend_lm = linear_trend_model(darts_air_passenger_train_boxcox) ap_trend_lm In\u00a0[\u00a0]: Copied! <pre>ap_trend_linear_train = find_linear_trend(\n    model=ap_trend_lm, series=darts_air_passenger_train_boxcox\n)\nap_trend_linear_test = find_linear_trend(\n    model=ap_trend_lm,\n    series=darts_air_passenger_test_boxcox,\n    positional_index_start=train_history_length,\n)\n\ndarts_air_passenger_train_boxcox.plot(label=\"Train\")\nap_trend_linear_train.plot(label=\"Linear Trend (Train)\")\ndarts_air_passenger_test_boxcox.plot(label=\"Test\")\nap_trend_linear_test.plot(label=\"Linear Trend (Test)\")\n</pre> ap_trend_linear_train = find_linear_trend(     model=ap_trend_lm, series=darts_air_passenger_train_boxcox ) ap_trend_linear_test = find_linear_trend(     model=ap_trend_lm,     series=darts_air_passenger_test_boxcox,     positional_index_start=train_history_length, )  darts_air_passenger_train_boxcox.plot(label=\"Train\") ap_trend_linear_train.plot(label=\"Linear Trend (Train)\") darts_air_passenger_test_boxcox.plot(label=\"Test\") ap_trend_linear_test.plot(label=\"Linear Trend (Test)\") In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train_transformed = (\n    darts_air_passenger_train_boxcox - ap_trend_linear_train\n)\ndarts_air_passenger_train_transformed.plot()\n</pre> darts_air_passenger_train_transformed = (     darts_air_passenger_train_boxcox - ap_trend_linear_train ) darts_air_passenger_train_transformed.plot() In\u00a0[\u00a0]: Copied! <pre>rf_bc_lt = RandomForest(**ap_rf_params)\nrf_bc_lt.fit(darts_air_passenger_train_transformed)\n</pre> rf_bc_lt = RandomForest(**ap_rf_params) rf_bc_lt.fit(darts_air_passenger_train_transformed) In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_train.plot()\ndarts_air_passenger_test.plot()\npred_rf_bc_lt = boxcox.inverse_transform(\n    rf_bc_lt.predict(n=ap_horizon) + ap_trend_linear_test\n)\npred_rf_bc_lt.plot(label=\"Box-Cox + Linear Detrend Predictions\", linestyle=\"--\")\n</pre> darts_air_passenger_train.plot() darts_air_passenger_test.plot() pred_rf_bc_lt = boxcox.inverse_transform(     rf_bc_lt.predict(n=ap_horizon) + ap_trend_linear_test ) pred_rf_bc_lt.plot(label=\"Box-Cox + Linear Detrend Predictions\", linestyle=\"--\") In\u00a0[\u00a0]: Copied! <pre>darts_air_passenger_test.plot(label=\"Test\")\npred_rf_ap.plot(label=\"Simple RF\", linestyle=\"--\")\npred_rf_ap_seasonal.plot(label=\"RF on Global Detrended Data (Cheating)\", linestyle=\"--\")\npred_rf_bc_lt.plot(label=\"Box-Cox + Linear Detrend\", linestyle=\"--\")\n</pre> darts_air_passenger_test.plot(label=\"Test\") pred_rf_ap.plot(label=\"Simple RF\", linestyle=\"--\") pred_rf_ap_seasonal.plot(label=\"RF on Global Detrended Data (Cheating)\", linestyle=\"--\") pred_rf_bc_lt.plot(label=\"Box-Cox + Linear Detrend\", linestyle=\"--\") In\u00a0[\u00a0]: Copied! <pre>benchmark_metrics = [\n    metrics.mae,\n    metrics.mape,\n    metrics.mse,\n    metrics.rmse,\n    metrics.smape,\n    metrics.r2_score,\n]\n</pre> benchmark_metrics = [     metrics.mae,     metrics.mape,     metrics.mse,     metrics.rmse,     metrics.smape,     metrics.r2_score, ] In\u00a0[\u00a0]: Copied! <pre>def benchmark_predictions(\n    series_true: TimeSeries,\n    series_prediction: TimeSeries,\n    metrics: List[Callable],\n    experiment_id: str,\n) -&gt; Dict:\n    results = []\n    for m in benchmark_metrics:\n        results.append(\n            {\n                \"metric\": f\"{m.__name__}\",\n                \"value\": m(series_true, series_prediction),\n                \"experiment\": experiment_id,\n            }\n        )\n\n    return results\n</pre> def benchmark_predictions(     series_true: TimeSeries,     series_prediction: TimeSeries,     metrics: List[Callable],     experiment_id: str, ) -&gt; Dict:     results = []     for m in benchmark_metrics:         results.append(             {                 \"metric\": f\"{m.__name__}\",                 \"value\": m(series_true, series_prediction),                 \"experiment\": experiment_id,             }         )      return results In\u00a0[\u00a0]: Copied! <pre>benchmark_results = []\n\nfor i, pred in zip(\n    [\"simple_rf\", \"detrended_cheating\", \"boxcox_linear_trend\"],\n    [pred_rf_ap, pred_rf_ap_seasonal, pred_rf_bc_lt],\n):\n    benchmark_results += benchmark_predictions(\n        series_true=darts_air_passenger_test,\n        series_prediction=pred,\n        metrics=benchmark_metrics,\n        experiment_id=i,\n    )\n\ndf_benchmark_metrics = pd.DataFrame(benchmark_results)\ndf_benchmark_metrics\n</pre> benchmark_results = []  for i, pred in zip(     [\"simple_rf\", \"detrended_cheating\", \"boxcox_linear_trend\"],     [pred_rf_ap, pred_rf_ap_seasonal, pred_rf_bc_lt], ):     benchmark_results += benchmark_predictions(         series_true=darts_air_passenger_test,         series_prediction=pred,         metrics=benchmark_metrics,         experiment_id=i,     )  df_benchmark_metrics = pd.DataFrame(benchmark_results) df_benchmark_metrics In\u00a0[\u00a0]: Copied! <pre>metric_chart_grid = sns.FacetGrid(\n    df_benchmark_metrics,\n    col=\"metric\",\n    hue=\"metric\",\n    col_wrap=2,\n    height=4,\n    aspect=1 / 0.618,\n    sharey=False,\n)\n\nmetric_chart_grid.map(\n    sns.barplot, \"experiment\", \"value\", order=df_benchmark_metrics.experiment.unique()\n)\n# for axes in metric_chart_grid.axes.flat:\n#     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n# metric_chart_grid.fig.tight_layout(w_pad=1)\n</pre> metric_chart_grid = sns.FacetGrid(     df_benchmark_metrics,     col=\"metric\",     hue=\"metric\",     col_wrap=2,     height=4,     aspect=1 / 0.618,     sharey=False, )  metric_chart_grid.map(     sns.barplot, \"experiment\", \"value\", order=df_benchmark_metrics.experiment.unique() ) # for axes in metric_chart_grid.axes.flat: #     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90) # metric_chart_grid.fig.tight_layout(w_pad=1)"}, {"location": "notebooks/tree_darts_random_forest/#forecasting-with-trees-using-darts", "title": "Forecasting with Trees Using Darts\u00b6", "text": "<p>In this notebook, we explore some basic ideas of how to forecast using trees with the help of the package called Darts.</p>"}, {"location": "notebooks/tree_darts_random_forest/#following-the-darts-official-tutorial", "title": "Following the Darts Official Tutorial\u00b6", "text": "<p>Darts provides a tutorial here to help the users get started. Here we replicate some of them to provide a minimal working example for tree-based models.</p>"}, {"location": "notebooks/tree_darts_random_forest/#first-random-forest-model", "title": "First Random Forest Model\u00b6", "text": ""}, {"location": "notebooks/tree_darts_random_forest/#detrending-helps", "title": "Detrending Helps\u00b6", "text": ""}, {"location": "notebooks/tree_darts_random_forest/#train-test-and-metrics", "title": "Train, Test, and Metrics\u00b6", "text": ""}, {"location": "notebooks/tree_darts_random_forest/#metrics", "title": "Metrics\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/", "title": "Random Forest Playground", "text": "<p>Outline</p> <ol> <li>Generate data of specific functions</li> <li>Fit the functions using ensemble methods</li> <li>Analyze the trees</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.tree as _tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import (\n    SelectFromModel,\n    SelectKBest,\n    chi2,\n    mutual_info_regression,\n)\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_val_score,\n    learning_curve,\n    train_test_split,\n    validation_curve,\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import sklearn.tree as _tree from sklearn.ensemble import RandomForestRegressor from sklearn.feature_selection import (     SelectFromModel,     SelectKBest,     chi2,     mutual_info_regression, ) from sklearn.model_selection import (     GridSearchCV,     RandomizedSearchCV,     cross_val_score,     learning_curve,     train_test_split,     validation_curve, ) from sklearn.pipeline import Pipeline from sklearn.tree import DecisionTreeRegressor In\u00a0[\u00a0]: Copied! <pre>mpl.rcParams[\"axes.unicode_minus\"] = False\nfrom random import random\n</pre> mpl.rcParams[\"axes.unicode_minus\"] = False from random import random In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport seaborn as sns\nfrom joblib import dump, load\n</pre> import numpy as np import seaborn as sns from joblib import dump, load In\u00a0[\u00a0]: Copied! <pre># Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(1000, 4000, 11)]\n# Number of features to consider at every split\nmax_features = [\"auto\", \"sqrt\"]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(10, 30, 2)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [0.001, 0.01, 0.02, 0.05, 0.1, 0.2]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [10, 20, 30, 40, 50]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\nrf_random_grid = {\n    \"rf__n_estimators\": n_estimators,\n    #     \"rf__max_features\": max_features,\n    \"rf__max_depth\": max_depth,\n    \"rf__min_samples_split\": min_samples_split,\n    \"rf__min_samples_leaf\": min_samples_leaf,\n    \"rf__bootstrap\": bootstrap,\n}\n\nrf = RandomForestRegressor(random_state=42, oob_score=True)\n\n##########\n\npipeline_steps = [\n    (\"rf\", rf),\n]\n\npipeline = Pipeline(pipeline_steps)\n</pre> # Number of trees in random forest n_estimators = [int(x) for x in np.linspace(1000, 4000, 11)] # Number of features to consider at every split max_features = [\"auto\", \"sqrt\"] # Maximum number of levels in tree max_depth = [int(x) for x in range(10, 30, 2)] max_depth.append(None) # Minimum number of samples required to split a node min_samples_split = [0.001, 0.01, 0.02, 0.05, 0.1, 0.2] # Minimum number of samples required at each leaf node min_samples_leaf = [10, 20, 30, 40, 50] # Method of selecting samples for training each tree bootstrap = [True, False]   rf_random_grid = {     \"rf__n_estimators\": n_estimators,     #     \"rf__max_features\": max_features,     \"rf__max_depth\": max_depth,     \"rf__min_samples_split\": min_samples_split,     \"rf__min_samples_leaf\": min_samples_leaf,     \"rf__bootstrap\": bootstrap, }  rf = RandomForestRegressor(random_state=42, oob_score=True)  ##########  pipeline_steps = [     (\"rf\", rf), ]  pipeline = Pipeline(pipeline_steps) In\u00a0[\u00a0]: Copied! <pre>def pred_true_comparison_plot(dataframe, ax, pred_sample=100):\n    sns.scatterplot(\n        dataframe, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=5\n    )\n\n    sns.scatterplot(\n        dataframe.sample(100),\n        x=\"x\",\n        y=\"y_pred\",\n        ax=ax,\n        label=\"y_pred\",\n        marker=\"+\",\n        s=100,\n        linewidth=2,\n    )\n\n    return ax\n\n\ndef predictions_each_estimators(x, rf_model):\n    preds = []\n    for i in x:\n        i_preds = []\n        for est in rf_model.best_estimator_[\"rf\"].estimators_:\n            i_preds.append(est.predict([[i]]).tolist())\n        i_preds = sum(i_preds, [])\n        preds.append(i_preds)\n\n    return {\"x\": pd.DataFrame(x, columns=[\"x\"]), \"preds\": pd.DataFrame(preds)}\n</pre> def pred_true_comparison_plot(dataframe, ax, pred_sample=100):     sns.scatterplot(         dataframe, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=5     )      sns.scatterplot(         dataframe.sample(100),         x=\"x\",         y=\"y_pred\",         ax=ax,         label=\"y_pred\",         marker=\"+\",         s=100,         linewidth=2,     )      return ax   def predictions_each_estimators(x, rf_model):     preds = []     for i in x:         i_preds = []         for est in rf_model.best_estimator_[\"rf\"].estimators_:             i_preds.append(est.predict([[i]]).tolist())         i_preds = sum(i_preds, [])         preds.append(i_preds)      return {\"x\": pd.DataFrame(x, columns=[\"x\"]), \"preds\": pd.DataFrame(preds)} In\u00a0[\u00a0]: Copied! <pre>X_sin = [6 * random() for i in range(10000)]\ny_sin = np.sin(X_sin)\n\nX_sin_test = [6 * random() for i in range(10000)]\ny_sin_test = np.sin(X_sin_test)\n</pre> X_sin = [6 * random() for i in range(10000)] y_sin = np.sin(X_sin)  X_sin_test = [6 * random() for i in range(10000)] y_sin_test = np.sin(X_sin_test) In\u00a0[\u00a0]: Copied! <pre>df_sin = pd.DataFrame(\n    {\n        \"x\": X_sin,\n        \"y\": y_sin,\n    }\n)\n</pre> df_sin = pd.DataFrame(     {         \"x\": X_sin,         \"y\": y_sin,     } ) In\u00a0[\u00a0]: Copied! <pre>model = RandomizedSearchCV(\n    pipeline, cv=10, param_distributions=rf_random_grid, verbose=3, n_jobs=-1\n)\n</pre> model = RandomizedSearchCV(     pipeline, cv=10, param_distributions=rf_random_grid, verbose=3, n_jobs=-1 ) In\u00a0[\u00a0]: Copied! <pre>model.fit(df_sin[[\"x\"]], df_sin[\"y\"].values.ravel())\n</pre> model.fit(df_sin[[\"x\"]], df_sin[\"y\"].values.ravel()) In\u00a0[\u00a0]: Copied! <pre>sin_score = model.score(df_sin[[\"x\"]], df_sin[\"y\"].values.ravel())\n</pre> sin_score = model.score(df_sin[[\"x\"]], df_sin[\"y\"].values.ravel()) In\u00a0[\u00a0]: Copied! <pre>model.best_params_\n</pre> model.best_params_ In\u00a0[\u00a0]: Copied! <pre># dump(model, \"reports/rf_sin.joblib\")\n</pre> # dump(model, \"reports/rf_sin.joblib\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10 * 10, 4 * 10))\n_tree.plot_tree(model.best_estimator_[\"rf\"].estimators_[0], fontsize=7)\n</pre> fig, ax = plt.subplots(figsize=(10 * 10, 4 * 10)) _tree.plot_tree(model.best_estimator_[\"rf\"].estimators_[0], fontsize=7) <p>Plot out the result</p> In\u00a0[\u00a0]: Copied! <pre>df_sin[\"y_pred\"] = model.predict(df_sin[[\"x\"]])\n</pre> df_sin[\"y_pred\"] = model.predict(df_sin[[\"x\"]]) In\u00a0[\u00a0]: Copied! <pre>df_sin\n</pre> df_sin In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\npred_true_comparison_plot(df_sin, ax)\nax.set_title(f\"Random Forest on Sin Data; $R^2$ Score: {sin_score:0.2f}\")\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  pred_true_comparison_plot(df_sin, ax) ax.set_title(f\"Random Forest on Sin Data; $R^2$ Score: {sin_score:0.2f}\") plt.legend() <p>Plot out the boxplots of each data point</p> In\u00a0[\u00a0]: Copied! <pre>est_sample_skip = 100\n</pre> est_sample_skip = 100 In\u00a0[\u00a0]: Copied! <pre>sin_est_pred = predictions_each_estimators(sorted(X_sin_test)[::est_sample_skip], model)\n</pre> sin_est_pred = predictions_each_estimators(sorted(X_sin_test)[::est_sample_skip], model) In\u00a0[\u00a0]: Copied! <pre>df_sin_est_quantiles = pd.merge(\n    sin_est_pred[\"x\"],\n    sin_est_pred[\"preds\"].quantile(q=[0.75, 0.25], axis=1).T,\n    how=\"left\",\n    left_index=True,\n    right_index=True,\n)\n\ndf_sin_est_quantiles[\"boxsize\"] = (\n    df_sin_est_quantiles[0.75] - df_sin_est_quantiles[0.25]\n)\n</pre> df_sin_est_quantiles = pd.merge(     sin_est_pred[\"x\"],     sin_est_pred[\"preds\"].quantile(q=[0.75, 0.25], axis=1).T,     how=\"left\",     left_index=True,     right_index=True, )  df_sin_est_quantiles[\"boxsize\"] = (     df_sin_est_quantiles[0.75] - df_sin_est_quantiles[0.25] ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 1.5 * 6.18))\nfig_skip = 5\n\nax.violinplot(\n    sin_est_pred[\"preds\"].values.tolist()[::fig_skip],\n    positions=sin_est_pred[\"x\"].x.tolist()[::fig_skip],\n)\n\nsns.lineplot(df_sin, x=\"x\", y=\"y\", ax=ax, label=\"y\")\n\nplt.xticks([])\n# ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(range(10)))\n# ax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x])\n\nax.set_title(\n    \"Violin Plot for All Predictions of Each Tree in a Random Forest on Some Sin Data Points\"\n)\n</pre> fig, ax = plt.subplots(figsize=(10, 1.5 * 6.18)) fig_skip = 5  ax.violinplot(     sin_est_pred[\"preds\"].values.tolist()[::fig_skip],     positions=sin_est_pred[\"x\"].x.tolist()[::fig_skip], )  sns.lineplot(df_sin, x=\"x\", y=\"y\", ax=ax, label=\"y\")  plt.xticks([]) # ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(range(10))) # ax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x])  ax.set_title(     \"Violin Plot for All Predictions of Each Tree in a Random Forest on Some Sin Data Points\" ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 2 * 6.18))\nfig_skip = 5\n\nax.boxplot(\n    sin_est_pred[\"preds\"].values.tolist()[::fig_skip],\n    positions=sin_est_pred[\"x\"].x.tolist()[::fig_skip],\n)\nsns.lineplot(df_sin, x=\"x\", y=\"y\", ax=ax, label=\"y\")\n\nax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x[::fig_skip]])\n\nax.set_title(\"Box Plot for Tree Predictions on Random Forest on Sin Data\")\n</pre> fig, ax = plt.subplots(figsize=(10, 2 * 6.18)) fig_skip = 5  ax.boxplot(     sin_est_pred[\"preds\"].values.tolist()[::fig_skip],     positions=sin_est_pred[\"x\"].x.tolist()[::fig_skip], ) sns.lineplot(df_sin, x=\"x\", y=\"y\", ax=ax, label=\"y\")  ax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x[::fig_skip]])  ax.set_title(\"Box Plot for Tree Predictions on Random Forest on Sin Data\") In\u00a0[\u00a0]: Copied! <pre>df_sin_est_quantiles\n</pre> df_sin_est_quantiles In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.barplot(df_sin_est_quantiles, x=\"x\", y=\"boxsize\")\n\nax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x])\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.barplot(df_sin_est_quantiles, x=\"x\", y=\"boxsize\")  ax.set_xticklabels([f\"{i:0.2f}\" for i in sin_est_pred[\"x\"].x]) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.histplot(df_sin_est_quantiles.boxsize, ax=ax)\nax.set_yscale(\"log\")\nax.set_xscale(\"log\")\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.histplot(df_sin_est_quantiles.boxsize, ax=ax) ax.set_yscale(\"log\") ax.set_xscale(\"log\") In\u00a0[\u00a0]: Copied! <pre>X_sin_noise = np.array([6 * random() for i in range(10000)])\ny_sin_noise = np.array([i + 0.1 * (random() - 0.5) for i in np.sin(X_sin_noise)])\n\n\ndf_sin_noise = pd.DataFrame({\"x\": X_sin_noise, \"y\": y_sin_noise})\n</pre> X_sin_noise = np.array([6 * random() for i in range(10000)]) y_sin_noise = np.array([i + 0.1 * (random() - 0.5) for i in np.sin(X_sin_noise)])   df_sin_noise = pd.DataFrame({\"x\": X_sin_noise, \"y\": y_sin_noise}) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.lineplot(df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\")\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.lineplot(df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\")  In\u00a0[\u00a0]: Copied! <pre>model_noise = RandomizedSearchCV(\n    pipeline, cv=10, param_distributions=rf_random_grid, verbose=3, n_jobs=-1\n)\n</pre> model_noise = RandomizedSearchCV(     pipeline, cv=10, param_distributions=rf_random_grid, verbose=3, n_jobs=-1 ) In\u00a0[\u00a0]: Copied! <pre>model_noise.fit(df_sin_noise[[\"x\"]], df_sin_noise[\"y\"].values.ravel())\n</pre> model_noise.fit(df_sin_noise[[\"x\"]], df_sin_noise[\"y\"].values.ravel()) In\u00a0[\u00a0]: Copied! <pre>sin_noise_score = model_noise.score(df_sin_noise[[\"x\"]], df_sin_noise[\"y\"])\n</pre> sin_noise_score = model_noise.score(df_sin_noise[[\"x\"]], df_sin_noise[\"y\"]) In\u00a0[\u00a0]: Copied! <pre>model_noise.best_params_\n</pre> model_noise.best_params_ In\u00a0[\u00a0]: Copied! <pre># dump(model_noise, \"reports/rf_sin_noise.joblib\")\n</pre> # dump(model_noise, \"reports/rf_sin_noise.joblib\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(9 * 10, 4 * 10))\n_tree.plot_tree(model_noise.best_estimator_[\"rf\"].estimators_[0], fontsize=7)\n</pre> fig, ax = plt.subplots(figsize=(9 * 10, 4 * 10)) _tree.plot_tree(model_noise.best_estimator_[\"rf\"].estimators_[0], fontsize=7) In\u00a0[\u00a0]: Copied! <pre>df_sin_noise[\"y_pred\"] = model_noise.predict(df_sin_noise[[\"x\"]])\n</pre> df_sin_noise[\"y_pred\"] = model_noise.predict(df_sin_noise[[\"x\"]]) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\npred_true_comparison_plot(df_sin_noise, ax)\n\nax.set_title(\n    f\"Random Forest on Sin Data with Noise; Test $R^2$ Score: {sin_noise_score:0.2f}\"\n)\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  pred_true_comparison_plot(df_sin_noise, ax)  ax.set_title(     f\"Random Forest on Sin Data with Noise; Test $R^2$ Score: {sin_noise_score:0.2f}\" )  plt.legend() In\u00a0[\u00a0]: Copied! <pre>sin_noise_est_pred = predictions_each_estimators(\n    sorted(X_sin_noise)[::100], model_noise\n)\n</pre> sin_noise_est_pred = predictions_each_estimators(     sorted(X_sin_noise)[::100], model_noise ) In\u00a0[\u00a0]: Copied! <pre>df_sin_noise_est_quantiles = pd.merge(\n    sin_noise_est_pred[\"x\"],\n    sin_noise_est_pred[\"preds\"].quantile(q=[0.75, 0.25], axis=1).T,\n    how=\"left\",\n    left_index=True,\n    right_index=True,\n)\n\ndf_sin_noise_est_quantiles[\"boxsize\"] = (\n    df_sin_noise_est_quantiles[0.75] - df_sin_noise_est_quantiles[0.25]\n)\n</pre> df_sin_noise_est_quantiles = pd.merge(     sin_noise_est_pred[\"x\"],     sin_noise_est_pred[\"preds\"].quantile(q=[0.75, 0.25], axis=1).T,     how=\"left\",     left_index=True,     right_index=True, )  df_sin_noise_est_quantiles[\"boxsize\"] = (     df_sin_noise_est_quantiles[0.75] - df_sin_noise_est_quantiles[0.25] ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 1.5 * 6.18))\nfig_skip = 5\n\nax.violinplot(\n    sin_noise_est_pred[\"preds\"].values.tolist()[::fig_skip],\n    positions=sin_noise_est_pred[\"x\"].x.tolist()[::fig_skip],\n)\n\nsns.scatterplot(\n    df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=1\n)\n\nplt.xticks([])\n\nax.set_title(\n    \"Violin Plot for All Predictions of Each Tree in a Random Forest on Some Sin Data Points\"\n)\n</pre> fig, ax = plt.subplots(figsize=(10, 1.5 * 6.18)) fig_skip = 5  ax.violinplot(     sin_noise_est_pred[\"preds\"].values.tolist()[::fig_skip],     positions=sin_noise_est_pred[\"x\"].x.tolist()[::fig_skip], )  sns.scatterplot(     df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=1 )  plt.xticks([])  ax.set_title(     \"Violin Plot for All Predictions of Each Tree in a Random Forest on Some Sin Data Points\" ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 2 * 6.18))\nfig_skip = 5\n\nax.boxplot(\n    sin_noise_est_pred[\"preds\"].values.tolist()[::fig_skip],\n    positions=sin_noise_est_pred[\"x\"].x.tolist()[::fig_skip],\n)\nsns.scatterplot(\n    df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=1\n)\n\n\nax.set_xticklabels([f\"{i:0.2f}\" for i in sin_noise_est_pred[\"x\"].x[::fig_skip]])\n\nax.set_title(\"Box Plot for Tree Predictions on Random Forest on Sin Data\")\n</pre> fig, ax = plt.subplots(figsize=(10, 2 * 6.18)) fig_skip = 5  ax.boxplot(     sin_noise_est_pred[\"preds\"].values.tolist()[::fig_skip],     positions=sin_noise_est_pred[\"x\"].x.tolist()[::fig_skip], ) sns.scatterplot(     df_sin_noise, x=\"x\", y=\"y\", ax=ax, label=\"y\", marker=\".\", ec=\"face\", s=1 )   ax.set_xticklabels([f\"{i:0.2f}\" for i in sin_noise_est_pred[\"x\"].x[::fig_skip]])  ax.set_title(\"Box Plot for Tree Predictions on Random Forest on Sin Data\") In\u00a0[\u00a0]: Copied! <pre>df_sin_noise_est_quantiles[\"model\"] = \"with_noise\"\ndf_sin_est_quantiles[\"model\"] = \"no_noise\"\n\ndf_quantiles = pd.concat(\n    [\n        df_sin_est_quantiles[[\"model\", \"boxsize\"]],\n        df_sin_noise_est_quantiles[[\"model\", \"boxsize\"]],\n    ]\n)\n</pre> df_sin_noise_est_quantiles[\"model\"] = \"with_noise\" df_sin_est_quantiles[\"model\"] = \"no_noise\"  df_quantiles = pd.concat(     [         df_sin_est_quantiles[[\"model\", \"boxsize\"]],         df_sin_noise_est_quantiles[[\"model\", \"boxsize\"]],     ] ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.boxplot(df_quantiles, x=\"boxsize\", y=\"model\", ax=ax)\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.boxplot(df_quantiles, x=\"boxsize\", y=\"model\", ax=ax) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.histplot(\n    df_sin_noise_est_quantiles.boxsize,\n    #     bins=20,\n    ax=ax,\n    kde=True,\n    label=\"with Noise\",\n    stat=\"probability\",\n    binwidth=0.002,\n    binrange=(0, 0.07),\n)\n\nsns.histplot(\n    df_sin_est_quantiles.boxsize,\n    #     bins=20,\n    ax=ax,\n    kde=True,\n    label=\"without Noise\",\n    stat=\"probability\",\n    binwidth=0.002,\n    binrange=(0, 0.07),\n)\n\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.histplot(     df_sin_noise_est_quantiles.boxsize,     #     bins=20,     ax=ax,     kde=True,     label=\"with Noise\",     stat=\"probability\",     binwidth=0.002,     binrange=(0, 0.07), )  sns.histplot(     df_sin_est_quantiles.boxsize,     #     bins=20,     ax=ax,     kde=True,     label=\"without Noise\",     stat=\"probability\",     binwidth=0.002,     binrange=(0, 0.07), )  plt.legend() In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.kdeplot(\n    df_sin_noise_est_quantiles.boxsize,\n    #     bins=20,\n    ax=ax,\n    #     hist=False,\n    label=\"with Noise\",\n)\n\nsns.kdeplot(\n    df_sin_est_quantiles.boxsize,\n    #     bins=20,\n    ax=ax,\n    #     hist=True,\n    label=\"without Noise\",\n)\nplt.legend()\n</pre> fig, ax = plt.subplots(figsize=(10, 6.18))  sns.kdeplot(     df_sin_noise_est_quantiles.boxsize,     #     bins=20,     ax=ax,     #     hist=False,     label=\"with Noise\", )  sns.kdeplot(     df_sin_est_quantiles.boxsize,     #     bins=20,     ax=ax,     #     hist=True,     label=\"without Noise\", ) plt.legend()  $$ P_{err} \\leq \\frac{\\bar \\rho (1-s^2) }{s^2}. $$  In\u00a0[\u00a0]: Copied! <pre>def generalization_error(rhob, s):\n    res = rhob * (1 - s**2) / s**2\n\n    if res &gt; 1:\n        res = 1\n\n    return res\n</pre> def generalization_error(rhob, s):     res = rhob * (1 - s**2) / s**2      if res &gt; 1:         res = 1      return res In\u00a0[\u00a0]: Copied! <pre>generalization_error(0.2, 0.8)\n</pre> generalization_error(0.2, 0.8) In\u00a0[\u00a0]: Copied! <pre>pe_data = [\n    [generalization_error(rhob, s) for s in np.linspace(0.01, 0.1, 10)]\n    for rhob in np.linspace(0.01, 0.1, 10)\n]\n\npe_data_s_label = [s for s in np.linspace(0.01, 0.1, 10)]\n\npe_data_rhob_label = [rhob for rhob in np.linspace(0.01, 0.1, 10)]\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nsns.heatmap(pe_data, center=0, ax=ax)\n\nax.set_xlabel(\"s\")\nax.set_ylabel(\"correlation\")\nax.set_xticklabels([f\"{i:0.2f}\" for i in pe_data_s_label])\nax.set_yticklabels([f\"{i:0.2f}\" for i in pe_data_rhob_label])\n</pre> pe_data = [     [generalization_error(rhob, s) for s in np.linspace(0.01, 0.1, 10)]     for rhob in np.linspace(0.01, 0.1, 10) ]  pe_data_s_label = [s for s in np.linspace(0.01, 0.1, 10)]  pe_data_rhob_label = [rhob for rhob in np.linspace(0.01, 0.1, 10)]  fig, ax = plt.subplots(figsize=(10, 10))  sns.heatmap(pe_data, center=0, ax=ax)  ax.set_xlabel(\"s\") ax.set_ylabel(\"correlation\") ax.set_xticklabels([f\"{i:0.2f}\" for i in pe_data_s_label]) ax.set_yticklabels([f\"{i:0.2f}\" for i in pe_data_rhob_label]) In\u00a0[\u00a0]: Copied! <pre>temp_space = np.linspace(0.1, 1, 91)\n\npe_data = [[generalization_error(rhob, s) for s in temp_space] for rhob in temp_space]\n\npe_data_s_label = [s for s in temp_space]\n\npe_data_rhob_label = [rhob for rhob in temp_space]\n\nfig, ax = plt.subplots(figsize=(12, 10))\n\nsns.heatmap(pe_data, center=0, ax=ax)\n\nax.set_xlabel(\"s\")\nax.set_ylabel(\"correlation\")\n\nax.set_xticklabels([f\"{i:0.2f}\" for i in (ax.get_xticks() + 0.1) / 100])\nax.set_yticklabels([f\"{i:0.2f}\" for i in (ax.get_yticks() + 0.1) / 100])\n\nfor label in ax.xaxis.get_ticklabels()[::2]:\n    label.set_visible(False)\n\nfor label in ax.yaxis.get_ticklabels()[::2]:\n    label.set_visible(False)\n\n\nax.set_title(\"Upper Limit of Generalization Error of Random Forest\")\n</pre> temp_space = np.linspace(0.1, 1, 91)  pe_data = [[generalization_error(rhob, s) for s in temp_space] for rhob in temp_space]  pe_data_s_label = [s for s in temp_space]  pe_data_rhob_label = [rhob for rhob in temp_space]  fig, ax = plt.subplots(figsize=(12, 10))  sns.heatmap(pe_data, center=0, ax=ax)  ax.set_xlabel(\"s\") ax.set_ylabel(\"correlation\")  ax.set_xticklabels([f\"{i:0.2f}\" for i in (ax.get_xticks() + 0.1) / 100]) ax.set_yticklabels([f\"{i:0.2f}\" for i in (ax.get_yticks() + 0.1) / 100])  for label in ax.xaxis.get_ticklabels()[::2]:     label.set_visible(False)  for label in ax.yaxis.get_ticklabels()[::2]:     label.set_visible(False)   ax.set_title(\"Upper Limit of Generalization Error of Random Forest\")"}, {"location": "notebooks/tree_random_forest/#random-forest-playground", "title": "Random Forest Playground\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/#model", "title": "Model\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/#components", "title": "Components\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/#data-without-noise", "title": "Data without Noise\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/#data-with-noise", "title": "Data with Noise\u00b6", "text": ""}, {"location": "notebooks/tree_random_forest/#generalization-error", "title": "Generalization Error\u00b6", "text": ""}, {"location": "notebooks/ts_dl_utils/__init__/", "title": "init", "text": ""}, {"location": "notebooks/ts_dl_utils/version/", "title": "Version", "text": "In\u00a0[\u00a0]: Copied! <pre>__version__ = \"0.0.1\"\n</pre> __version__ = \"0.0.1\""}, {"location": "notebooks/ts_dl_utils/datasets/__init__/", "title": "init", "text": ""}, {"location": "notebooks/ts_dl_utils/datasets/dataset/", "title": "Dataset", "text": "In\u00a0[\u00a0]: Copied! <pre>from functools import cached_property\nfrom typing import Tuple\n</pre> from functools import cached_property from typing import Tuple In\u00a0[\u00a0]: Copied! <pre>import lightning as L\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom loguru import logger\nfrom torch.utils.data import DataLoader, Dataset\n</pre> import lightning as L import numpy as np import pandas as pd import torch from loguru import logger from torch.utils.data import DataLoader, Dataset In\u00a0[\u00a0]: Copied! <pre>class DataFrameDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param history_length: length of input X in time dimension\n        in the final Dataset class.\n    :param horizon: number of steps to be forecasted.\n    :param gap: gap between input history and prediction\n    \"\"\"\n\n    def __init__(\n        self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.history_length = history_length\n        self.horzion = horizon\n        self.gap = gap\n        self.dataframe_rows = len(self.dataframe)\n        self.length = (\n            self.dataframe_rows - self.history_length - self.horzion - self.gap + 1\n        )\n\n    def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        x, y = (\n            self.dataframe[idx : self.history_length + idx].values,\n            self.dataframe[\n                self.history_length\n                + idx\n                + gap : self.history_length\n                + self.horzion\n                + idx\n                + gap\n            ].values,\n        )\n        return x, y\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [\n                self.moving_slicing(i, self.gap)\n                for i in range(idx.start, idx.stop, step)\n            ]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx, self.gap)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</pre> class DataFrameDataset(Dataset):     \"\"\"A dataset from a pandas dataframe.      For a given pandas dataframe, this generates a pytorch     compatible dataset by sliding in time dimension.      ```python     ds = DataFrameDataset(         dataframe=df, history_length=10, horizon=2     )     ```      :param dataframe: input dataframe with a DatetimeIndex.     :param history_length: length of input X in time dimension         in the final Dataset class.     :param horizon: number of steps to be forecasted.     :param gap: gap between input history and prediction     \"\"\"      def __init__(         self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0     ):         super().__init__()         self.dataframe = dataframe         self.history_length = history_length         self.horzion = horizon         self.gap = gap         self.dataframe_rows = len(self.dataframe)         self.length = (             self.dataframe_rows - self.history_length - self.horzion - self.gap + 1         )      def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:         x, y = (             self.dataframe[idx : self.history_length + idx].values,             self.dataframe[                 self.history_length                 + idx                 + gap : self.history_length                 + self.horzion                 + idx                 + gap             ].values,         )         return x, y      def _validate_dataframe(self) -&gt; None:         \"\"\"Validate the input dataframe.          - We require the dataframe index to be DatetimeIndex.         - This dataset is null aversion.         - Dataframe index should be sorted.         \"\"\"          if not isinstance(             self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex         ):             raise TypeError(                 \"Type of the dataframe index is not DatetimeIndex\"                 f\": {type(self.dataframe.index)}\"             )          has_na = self.dataframe.isnull().values.any()          if has_na:             logger.warning(\"Dataframe has null\")          has_index_sorted = self.dataframe.index.equals(             self.dataframe.index.sort_values()         )          if not has_index_sorted:             logger.warning(\"Dataframe index is not sorted\")      def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:         if isinstance(idx, slice):             if (idx.start &lt; 0) or (idx.stop &gt;= self.length):                 raise IndexError(f\"Slice out of range: {idx}\")             step = idx.step if idx.step is not None else 1             return [                 self.moving_slicing(i, self.gap)                 for i in range(idx.start, idx.stop, step)             ]         else:             if idx &gt;= self.length:                 raise IndexError(\"End of dataset\")             return self.moving_slicing(idx, self.gap)      def __len__(self) -&gt; int:         return self.length In\u00a0[\u00a0]: Copied! <pre>class DataFrameDataModule(L.LightningDataModule):\n    def __init__(\n        self,\n        history_length: int,\n        horizon: int,\n        dataframe: pd.DataFrame,\n        gap: int = 0,\n        test_fraction: float = 0.3,\n        val_fraction: float = 0.1,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.history_length = history_length\n        self.horizon = horizon\n        self.batch_size = batch_size\n        self.dataframe = dataframe\n        self.gap = gap\n        self.test_fraction = test_fraction\n        self.val_fraction = val_fraction\n        self.num_workers = num_workers\n\n        self.train_dataset, self.val_dataset = self.split_train_val(\n            self.train_val_dataset\n        )\n\n    @cached_property\n    def df_length(self):\n        return len(self.dataframe)\n\n    @cached_property\n    def df_test_length(self):\n        return int(self.df_length * self.test_fraction)\n\n    @cached_property\n    def df_train_val_length(self):\n        return self.df_length - self.df_test_length\n\n    @cached_property\n    def train_val_dataframe(self):\n        return self.dataframe.iloc[: self.df_train_val_length]\n\n    @cached_property\n    def test_dataframe(self):\n        return self.dataframe.iloc[self.df_train_val_length :]\n\n    @cached_property\n    def train_val_dataset(self):\n        return DataFrameDataset(\n            dataframe=self.train_val_dataframe,\n            history_length=self.history_length,\n            horizon=self.horizon,\n            gap=self.gap,\n        )\n\n    @cached_property\n    def test_dataset(self):\n        return DataFrameDataset(\n            dataframe=self.test_dataframe,\n            history_length=self.history_length,\n            horizon=self.horizon,\n            gap=self.gap,\n        )\n\n    def split_train_val(self, dataset: Dataset):\n        return torch.utils.data.random_split(\n            dataset, [1 - self.val_fraction, self.val_fraction]\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            persistent_workers=True if self.num_workers &gt; 0 else False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            persistent_workers=True if self.num_workers &gt; 0 else False,\n        )\n\n    def predict_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False\n        )\n</pre> class DataFrameDataModule(L.LightningDataModule):     def __init__(         self,         history_length: int,         horizon: int,         dataframe: pd.DataFrame,         gap: int = 0,         test_fraction: float = 0.3,         val_fraction: float = 0.1,         batch_size: int = 32,         num_workers: int = 0,     ):         super().__init__()         self.history_length = history_length         self.horizon = horizon         self.batch_size = batch_size         self.dataframe = dataframe         self.gap = gap         self.test_fraction = test_fraction         self.val_fraction = val_fraction         self.num_workers = num_workers          self.train_dataset, self.val_dataset = self.split_train_val(             self.train_val_dataset         )      @cached_property     def df_length(self):         return len(self.dataframe)      @cached_property     def df_test_length(self):         return int(self.df_length * self.test_fraction)      @cached_property     def df_train_val_length(self):         return self.df_length - self.df_test_length      @cached_property     def train_val_dataframe(self):         return self.dataframe.iloc[: self.df_train_val_length]      @cached_property     def test_dataframe(self):         return self.dataframe.iloc[self.df_train_val_length :]      @cached_property     def train_val_dataset(self):         return DataFrameDataset(             dataframe=self.train_val_dataframe,             history_length=self.history_length,             horizon=self.horizon,             gap=self.gap,         )      @cached_property     def test_dataset(self):         return DataFrameDataset(             dataframe=self.test_dataframe,             history_length=self.history_length,             horizon=self.horizon,             gap=self.gap,         )      def split_train_val(self, dataset: Dataset):         return torch.utils.data.random_split(             dataset, [1 - self.val_fraction, self.val_fraction]         )      def train_dataloader(self):         return DataLoader(             dataset=self.train_dataset,             batch_size=self.batch_size,             shuffle=True,             num_workers=self.num_workers,             persistent_workers=True if self.num_workers &gt; 0 else False,         )      def test_dataloader(self):         return DataLoader(             dataset=self.test_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,         )      def val_dataloader(self):         return DataLoader(             dataset=self.val_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,             persistent_workers=True if self.num_workers &gt; 0 else False,         )      def predict_dataloader(self):         return DataLoader(             dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False         )"}, {"location": "notebooks/ts_dl_utils/datasets/pendulum/", "title": "Pendulum", "text": "In\u00a0[\u00a0]: Copied! <pre>import math\nfrom functools import cached_property\nfrom typing import Dict, List\n</pre> import math from functools import cached_property from typing import Dict, List In\u00a0[\u00a0]: Copied! <pre>import lightning as L\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom ts_dl_utils.datasets.dataset import DataFrameDataset\n</pre> import lightning as L import pandas as pd import torch from torch.utils.data import DataLoader, Dataset from ts_dl_utils.datasets.dataset import DataFrameDataset In\u00a0[\u00a0]: Copied! <pre>class Pendulum:\n    r\"\"\"Class for generating time series data for a pendulum.\n\n    The pendulum is modelled as a damped harmonic oscillator, i.e.,\n\n    $$\n    \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t),\n    $$\n\n    where $\\theta(t)$ is the angle of the pendulum at time $t$.\n    The period $p$ is calculated using\n\n    $$\n    p = 2 \\pi \\sqrt(L / g),\n    $$\n\n    with $L$ being the length of the pendulum\n    and $g$ being the surface gravity.\n\n    :param length: Length of the pendulum.\n    :param gravity: Acceleration due to gravity.\n    \"\"\"\n\n    def __init__(self, length: float, gravity: float = 9.81) -&gt; None:\n        self.length = length\n        self.gravity = gravity\n\n    @cached_property\n    def period(self) -&gt; float:\n        \"\"\"Calculate the period of the pendulum.\"\"\"\n        return 2 * math.pi * math.sqrt(self.length / self.gravity)\n\n    def __call__(\n        self,\n        num_periods: int,\n        num_samples_per_period: int,\n        initial_angle: float = 0.1,\n        beta: float = 0,\n    ) -&gt; Dict[str, List[float]]:\n        \"\"\"Generate time series data for the pendulum.\n\n        Returns a list of floats representing the angle\n        of the pendulum at each time step.\n\n        :param num_periods: Number of periods to generate.\n        :param num_samples_per_period: Number of samples per period.\n        :param initial_angle: Initial angle of the pendulum.\n        \"\"\"\n        time_step = self.period / num_samples_per_period\n        steps = []\n        time_series = []\n        for i in range(num_periods * num_samples_per_period):\n            t = i * time_step\n            angle = (\n                initial_angle\n                * math.cos(2 * math.pi * t / self.period)\n                * math.exp(-beta * t)\n            )\n            steps.append(t)\n            time_series.append(angle)\n\n        return {\"t\": steps, \"theta\": time_series}\n</pre> class Pendulum:     r\"\"\"Class for generating time series data for a pendulum.      The pendulum is modelled as a damped harmonic oscillator, i.e.,      $$     \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t),     $$      where $\\theta(t)$ is the angle of the pendulum at time $t$.     The period $p$ is calculated using      $$     p = 2 \\pi \\sqrt(L / g),     $$      with $L$ being the length of the pendulum     and $g$ being the surface gravity.      :param length: Length of the pendulum.     :param gravity: Acceleration due to gravity.     \"\"\"      def __init__(self, length: float, gravity: float = 9.81) -&gt; None:         self.length = length         self.gravity = gravity      @cached_property     def period(self) -&gt; float:         \"\"\"Calculate the period of the pendulum.\"\"\"         return 2 * math.pi * math.sqrt(self.length / self.gravity)      def __call__(         self,         num_periods: int,         num_samples_per_period: int,         initial_angle: float = 0.1,         beta: float = 0,     ) -&gt; Dict[str, List[float]]:         \"\"\"Generate time series data for the pendulum.          Returns a list of floats representing the angle         of the pendulum at each time step.          :param num_periods: Number of periods to generate.         :param num_samples_per_period: Number of samples per period.         :param initial_angle: Initial angle of the pendulum.         \"\"\"         time_step = self.period / num_samples_per_period         steps = []         time_series = []         for i in range(num_periods * num_samples_per_period):             t = i * time_step             angle = (                 initial_angle                 * math.cos(2 * math.pi * t / self.period)                 * math.exp(-beta * t)             )             steps.append(t)             time_series.append(angle)          return {\"t\": steps, \"theta\": time_series} In\u00a0[\u00a0]: Copied! <pre>class PendulumDataModule(L.LightningDataModule):\n    def __init__(\n        self,\n        history_length: int,\n        horizon: int,\n        dataframe: pd.DataFrame,\n        gap: int = 0,\n        test_fraction: float = 0.3,\n        val_fraction: float = 0.1,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.history_length = history_length\n        self.horizon = horizon\n        self.batch_size = batch_size\n        self.dataframe = dataframe\n        self.gap = gap\n        self.test_fraction = test_fraction\n        self.val_fraction = val_fraction\n        self.num_workers = num_workers\n\n        self.train_dataset, self.val_dataset = self.split_train_val(\n            self.train_val_dataset\n        )\n\n    @cached_property\n    def df_length(self):\n        return len(self.dataframe)\n\n    @cached_property\n    def df_test_length(self):\n        return int(self.df_length * self.test_fraction)\n\n    @cached_property\n    def df_train_val_length(self):\n        return self.df_length - self.df_test_length\n\n    @cached_property\n    def train_val_dataframe(self):\n        return self.dataframe.iloc[: self.df_train_val_length]\n\n    @cached_property\n    def test_dataframe(self):\n        return self.dataframe.iloc[self.df_train_val_length :]\n\n    @cached_property\n    def train_val_dataset(self):\n        return DataFrameDataset(\n            dataframe=self.train_val_dataframe,\n            history_length=self.history_length,\n            horizon=self.horizon,\n            gap=self.gap,\n        )\n\n    @cached_property\n    def test_dataset(self):\n        return DataFrameDataset(\n            dataframe=self.test_dataframe,\n            history_length=self.history_length,\n            horizon=self.horizon,\n            gap=self.gap,\n        )\n\n    def split_train_val(self, dataset: Dataset):\n        return torch.utils.data.random_split(\n            dataset, [1 - self.val_fraction, self.val_fraction]\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            persistent_workers=True if self.num_workers &gt; 0 else False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            persistent_workers=True if self.num_workers &gt; 0 else False,\n        )\n\n    def predict_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False\n        )\n</pre> class PendulumDataModule(L.LightningDataModule):     def __init__(         self,         history_length: int,         horizon: int,         dataframe: pd.DataFrame,         gap: int = 0,         test_fraction: float = 0.3,         val_fraction: float = 0.1,         batch_size: int = 32,         num_workers: int = 0,     ):         super().__init__()         self.history_length = history_length         self.horizon = horizon         self.batch_size = batch_size         self.dataframe = dataframe         self.gap = gap         self.test_fraction = test_fraction         self.val_fraction = val_fraction         self.num_workers = num_workers          self.train_dataset, self.val_dataset = self.split_train_val(             self.train_val_dataset         )      @cached_property     def df_length(self):         return len(self.dataframe)      @cached_property     def df_test_length(self):         return int(self.df_length * self.test_fraction)      @cached_property     def df_train_val_length(self):         return self.df_length - self.df_test_length      @cached_property     def train_val_dataframe(self):         return self.dataframe.iloc[: self.df_train_val_length]      @cached_property     def test_dataframe(self):         return self.dataframe.iloc[self.df_train_val_length :]      @cached_property     def train_val_dataset(self):         return DataFrameDataset(             dataframe=self.train_val_dataframe,             history_length=self.history_length,             horizon=self.horizon,             gap=self.gap,         )      @cached_property     def test_dataset(self):         return DataFrameDataset(             dataframe=self.test_dataframe,             history_length=self.history_length,             horizon=self.horizon,             gap=self.gap,         )      def split_train_val(self, dataset: Dataset):         return torch.utils.data.random_split(             dataset, [1 - self.val_fraction, self.val_fraction]         )      def train_dataloader(self):         return DataLoader(             dataset=self.train_dataset,             batch_size=self.batch_size,             shuffle=True,             num_workers=self.num_workers,             persistent_workers=True if self.num_workers &gt; 0 else False,         )      def test_dataloader(self):         return DataLoader(             dataset=self.test_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,         )      def val_dataloader(self):         return DataLoader(             dataset=self.val_dataset,             batch_size=self.batch_size,             shuffle=False,             num_workers=self.num_workers,             persistent_workers=True if self.num_workers &gt; 0 else False,         )      def predict_dataloader(self):         return DataLoader(             dataset=self.test_dataset, batch_size=len(self.test_dataset), shuffle=False         )"}, {"location": "notebooks/ts_dl_utils/evaluation/__init__/", "title": "init", "text": ""}, {"location": "notebooks/ts_dl_utils/evaluation/evaluator/", "title": "Evaluator", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import Dict, List, Tuple\n</pre> from typing import Dict, List, Tuple In\u00a0[\u00a0]: Copied! <pre>import matplotlib as mpl\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchmetrics import MetricCollection\nfrom torchmetrics.regression import (\n    MeanAbsoluteError,\n    MeanAbsolutePercentageError,\n    MeanSquaredError,\n    SymmetricMeanAbsolutePercentageError,\n)\n</pre> import matplotlib as mpl import numpy as np import pandas as pd import torch from torch.utils.data import DataLoader from torchmetrics import MetricCollection from torchmetrics.regression import (     MeanAbsoluteError,     MeanAbsolutePercentageError,     MeanSquaredError,     SymmetricMeanAbsolutePercentageError, ) In\u00a0[\u00a0]: Copied! <pre>class Evaluator:\n    \"\"\"Evaluate the predictions\n\n    :param step: which prediction step to be evaluated.\n    :param gap: gap between input history and target/prediction.\n    \"\"\"\n\n    def __init__(self, step: int = 0, gap: int = 0):\n        self.step = step\n        self.gap = gap\n\n    @staticmethod\n    def get_one_history(\n        predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; torch.Tensor:\n        return predictions[batch_idx][0][idx, ...]\n\n    @staticmethod\n    def get_one_pred(predictions: List, idx: int, batch_idx: int = 0) -&gt; torch.Tensor:\n        return predictions[batch_idx][1][idx, ...]\n\n    @staticmethod\n    def get_y(predictions: List, step: int) -&gt; List[torch.Tensor]:\n        return [i[1][..., step] for i in predictions]\n\n    def y(self, predictions: List, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y(predictions, self.step)[batch_idx].detach()\n\n    @staticmethod\n    def get_y_true(dataloader: DataLoader, step: int) -&gt; torch.Tensor:\n        return [i[1].squeeze(-1)[..., step] for i in dataloader]\n\n    def y_true(self, dataloader: DataLoader, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y_true(dataloader, step=self.step)[batch_idx].detach()\n\n    def get_one_sample(\n        self, predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        return (\n            self.get_one_history(predictions, idx, batch_idx),\n            self.get_one_pred(predictions, idx, batch_idx),\n        )\n\n    def plot_one_sample(\n        self, ax: mpl.axes.Axes, predictions: List, idx: int, batch_idx: int = 0\n    ):\n        history, pred = self.get_one_sample(predictions, idx, batch_idx)\n\n        x_raw = np.arange(len(history) + len(pred) + self.gap)\n        x_history = x_raw[: len(history)]\n        x_pred = x_raw[len(history) + self.gap :]\n        x = np.concatenate([x_history, x_pred])\n\n        y = np.concatenate([history, pred])\n\n        ax.plot(x, y, marker=\".\", label=f\"input ({idx})\")\n\n        ax.axvspan(x_pred[0], x_pred[-1], color=\"orange\", alpha=0.1)\n\n    @property\n    def metric_collection(self) -&gt; MetricCollection:\n        return MetricCollection(\n            MeanAbsoluteError(),\n            MeanAbsolutePercentageError(),\n            MeanSquaredError(),\n            SymmetricMeanAbsolutePercentageError(),\n        )\n\n    @staticmethod\n    def metric_dataframe(metrics: Dict) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            [{k: float(v) for k, v in metrics.items()}], index=[\"values\"]\n        ).T\n\n    def metrics(\n        self, predictions: List, dataloader: DataLoader, batch_idx: int = 0\n    ) -&gt; pd.DataFrame:\n        truths = self.y_true(dataloader)\n        preds = self.y(predictions, batch_idx=batch_idx)\n\n        return self.metric_dataframe(self.metric_collection(preds, truths))\n</pre> class Evaluator:     \"\"\"Evaluate the predictions      :param step: which prediction step to be evaluated.     :param gap: gap between input history and target/prediction.     \"\"\"      def __init__(self, step: int = 0, gap: int = 0):         self.step = step         self.gap = gap      @staticmethod     def get_one_history(         predictions: List, idx: int, batch_idx: int = 0     ) -&gt; torch.Tensor:         return predictions[batch_idx][0][idx, ...]      @staticmethod     def get_one_pred(predictions: List, idx: int, batch_idx: int = 0) -&gt; torch.Tensor:         return predictions[batch_idx][1][idx, ...]      @staticmethod     def get_y(predictions: List, step: int) -&gt; List[torch.Tensor]:         return [i[1][..., step] for i in predictions]      def y(self, predictions: List, batch_idx: int = 0) -&gt; torch.Tensor:         return self.get_y(predictions, self.step)[batch_idx].detach()      @staticmethod     def get_y_true(dataloader: DataLoader, step: int) -&gt; torch.Tensor:         return [i[1].squeeze(-1)[..., step] for i in dataloader]      def y_true(self, dataloader: DataLoader, batch_idx: int = 0) -&gt; torch.Tensor:         return self.get_y_true(dataloader, step=self.step)[batch_idx].detach()      def get_one_sample(         self, predictions: List, idx: int, batch_idx: int = 0     ) -&gt; Tuple[torch.Tensor, torch.Tensor]:         return (             self.get_one_history(predictions, idx, batch_idx),             self.get_one_pred(predictions, idx, batch_idx),         )      def plot_one_sample(         self, ax: mpl.axes.Axes, predictions: List, idx: int, batch_idx: int = 0     ):         history, pred = self.get_one_sample(predictions, idx, batch_idx)          x_raw = np.arange(len(history) + len(pred) + self.gap)         x_history = x_raw[: len(history)]         x_pred = x_raw[len(history) + self.gap :]         x = np.concatenate([x_history, x_pred])          y = np.concatenate([history, pred])          ax.plot(x, y, marker=\".\", label=f\"input ({idx})\")          ax.axvspan(x_pred[0], x_pred[-1], color=\"orange\", alpha=0.1)      @property     def metric_collection(self) -&gt; MetricCollection:         return MetricCollection(             MeanAbsoluteError(),             MeanAbsolutePercentageError(),             MeanSquaredError(),             SymmetricMeanAbsolutePercentageError(),         )      @staticmethod     def metric_dataframe(metrics: Dict) -&gt; pd.DataFrame:         return pd.DataFrame(             [{k: float(v) for k, v in metrics.items()}], index=[\"values\"]         ).T      def metrics(         self, predictions: List, dataloader: DataLoader, batch_idx: int = 0     ) -&gt; pd.DataFrame:         truths = self.y_true(dataloader)         preds = self.y(predictions, batch_idx=batch_idx)          return self.metric_dataframe(self.metric_collection(preds, truths))"}, {"location": "notebooks/ts_dl_utils/naive_forecasters/__init__/", "title": "init", "text": ""}, {"location": "notebooks/ts_dl_utils/naive_forecasters/last_observation/", "title": "Last observation", "text": "In\u00a0[\u00a0]: Copied! <pre>from typing import List, Tuple\n</pre> from typing import List, Tuple In\u00a0[\u00a0]: Copied! <pre>import lightning.pytorch as L\nimport torch\n</pre> import lightning.pytorch as L import torch In\u00a0[\u00a0]: Copied! <pre>class LastObservationForecaster(L.LightningModule):\n    \"\"\"Spits out the forecasts using the last observation.\n\n    :param horizon: horizon of the forecast.\n    \"\"\"\n\n    def __init__(self, horizon: int):\n        super().__init__()\n        self.horizon = horizon\n\n    def _last_observation(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return x[..., -1:, :]\n\n    def predict_step(\n        self, batch: List, batch_idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x, y = batch\n\n        y_hat = self._last_observation(x)\n\n        y_hat = y_hat.repeat(1, self.horizon, 1)\n\n        return x.squeeze(-1), y_hat.squeeze(-1)\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x = x.type(self.dtype)\n        return (\n            x.squeeze(-1),\n            self._last_observation(x).repeat(1, self.horizon, 1).squeeze(-1),\n        )\n</pre> class LastObservationForecaster(L.LightningModule):     \"\"\"Spits out the forecasts using the last observation.      :param horizon: horizon of the forecast.     \"\"\"      def __init__(self, horizon: int):         super().__init__()         self.horizon = horizon      def _last_observation(self, x: torch.Tensor) -&gt; torch.Tensor:         return x[..., -1:, :]      def predict_step(         self, batch: List, batch_idx: int     ) -&gt; Tuple[torch.Tensor, torch.Tensor]:         x, y = batch          y_hat = self._last_observation(x)          y_hat = y_hat.repeat(1, self.horizon, 1)          return x.squeeze(-1), y_hat.squeeze(-1)      def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:         x = x.type(self.dtype)         return (             x.squeeze(-1),             self._last_observation(x).repeat(1, self.horizon, 1).squeeze(-1),         )"}, {"location": "self-supervised/adversarial/f-gan/", "title": "f-GAN", "text": "<p>The essence of GAN is comparing the generated distribution \\(p_G\\) and the data distribution \\(p_\\text{data}\\). The vanilla GAN considers the Jensen-Shannon divergence \\(\\operatorname{D}_\\text{JS}(p_\\text{data}\\Vert p_{G})\\). The discriminator \\({\\color{green}D}\\) serves the purpose of forcing this divergence to be small.</p> <p>Why do we need the discriminator?</p> <p>If the JS divergence is an objective, why do we need the discriminator? Even in f-GAN we need a functional to approximate the f-divergence. This functional we choose works like the discriminator of GAN.</p> <p>There exists a more generic form of JS divergence, which is called f-divergence<sup>1</sup>. f-GAN obtains the model by estimating the f-divergence between the data distribution and the generated distribution<sup>2</sup>.</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/f-gan/#variational-divergence-minimization", "title": "Variational Divergence Minimization", "text": "<p>The Variational Divergence Minimization (VDM) extends the variational estimation of f-divergence<sup>2</sup>. VDM searches for the saddle point of an objective \\(F({\\color{red}\\theta}, {\\color{blue}\\omega})\\), i.e., min w.r.t. \\(\\theta\\) and max w.r.t \\({\\color{blue}\\omega}\\), where \\({\\color{red}\\theta}\\) is the parameter set of the generator \\({\\color{red}Q_\\theta}\\), and \\({\\color{blue}\\omega}\\) is the parameter set of the variational approximation to estimate f-divergence, \\({\\color{blue}T_\\omega}\\).</p> <p>The objective \\(F({\\color{red}\\theta}, {\\color{blue}\\omega})\\) is related to the choice of \\(f\\) in f-divergence and the variational functional \\({\\color{blue}T}\\),</p> \\[ \\begin{align} &amp; F(\\theta, \\omega)\\\\ =&amp; \\mathbb E_{x\\sim p_\\text{data}} \\left[ {\\color{blue}T_\\omega}(x) \\right] - \\mathbb E_{x\\sim {\\color{red}Q_\\theta} } \\left[ f^*({\\color{blue}T_\\omega}(x)) \\right] \\\\ =&amp; \\mathbb E_{x\\sim p_\\text{data}} \\left[ g_f(V_{\\color{blue}\\omega}(x)) \\right] - \\mathbb E_{x\\sim {\\color{red}Q_\\theta} } \\left[ f^*(g_f(V_{\\color{blue}\\omega}(x))) \\right]. \\end{align} \\] <p>In the above objective,</p> <ul> <li>\\(f^*\\) is the Legendre\u2013Fenchel transformation of \\(f\\), i.e., \\(f^*(t) = \\operatorname{sup}_{u\\in \\mathrm{dom}_f}\\left\\{ ut - f(u) \\right\\}\\).</li> </ul> \\(T\\) <p>The function \\(T\\) is used to estimate the lower bound of f-divergence<sup>2</sup>.</p> Choice of \\(g_f\\) and \\(V\\) <p>Nowozin et al provided a table for \\(g_f\\) and \\(V\\)<sup>2</sup>.</p> <p></p> <p>We estimate</p> <ul> <li>\\(\\mathbb E_{x\\sim p_\\text{data}}\\) by sampling from the mini-batch, and</li> <li>\\(\\mathbb E_{x\\sim {\\color{red}Q_\\theta} }\\) by sampling from the generator.</li> </ul> <p>Reduce to GAN</p> <p>The VDM loss can be reduced to the loss of GAN by setting<sup>2</sup></p> \\[ \\begin{align} \\log {\\color{green}D_\\omega} =&amp; g_f(V_{\\color{blue}\\omega}(x))  \\\\ \\log \\left( 1 - {\\color{green}D_\\omega} \\right) =&amp; -f^*\\left( g_f(V_{\\color{blue}\\omega}(x)) \\right). \\end{align} \\] <p>It is straightforward to validate that the following result is a solution to the above set of equations,</p> \\[ g_f(V) = \\log \\frac{1}{1 + e^{-V}}. \\]", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/f-gan/#code", "title": "Code", "text": "<ul> <li>minlee077/f-GAN-pytorch</li> <li>shayneobrien/generative-models</li> </ul> <ol> <li> <p>Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence#Instances_of_f-divergences \u21a9</p> </li> <li> <p>Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709 \u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Convex conjugate. In: Wikipedia [Internet]. 20 Feb 2021 [cited 7 Sep 2021]. Available: https://en.wikipedia.org/wiki/Convex_conjugate \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/", "title": "GAN", "text": "<p>GAN is a generative neural sampler<sup>1</sup>. To train the sampler, the task of GAN is designed to generate features \\(X\\) from a latent space \\(\\xi\\) and class labels \\(Y\\),</p> \\[\\xi, Y \\to X.\\] <p>Many different formulations of GANs are proposed. As an introduction to this topic, we will discuss vanilla GAN in this section<sup>3</sup>.</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#gan-theory", "title": "GAN Theory", "text": "", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#the-minimax-game-loss", "title": "The Minimax Game Loss", "text": "<p>The minimax game is a game to \"minimizing the possible loss for a worst-case\"<sup>2</sup>. In GAN, the game is to train the generator \\({\\color{red}G}\\) to fool the discriminator \\({\\color{green}D}\\) while minimizing the discrimination error of \\({\\color{green}D}\\).</p> <p>Goodfellow prosed a loss<sup>3</sup></p> \\[ \\begin{equation} \\underset{{\\color{red}G}}{\\operatorname{min}}\\underset{{\\color{green}D}}{\\operatorname{max}} V({\\color{green}D}, {\\color{red}G}) = \\mathbb E_{x\\sim p_{data}} \\left[ \\log {\\color{green}D}(x) \\right] + \\mathbb E_{z\\sim p_z} \\left[ \\log( 1- {\\color{green}D}({\\color{red}G}(z)) ) \\right]. \\end{equation} \\]", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#divergence", "title": "Divergence", "text": "<p>Goodfellow et al proved that the global minimum of such a setup is reached only if only \\(p_{G} = p_\\text{data}\\). GAN compares the generated distribution to the data distribution, using the Jensen-Shannon divergence<sup>3</sup>,</p> \\[ \\operatorname{D}_{\\text{JS}}(p_\\text{data}\\Vert p_{{\\color{red}G}}) = \\frac{1}{2}\\left[ \\operatorname{D}_\\text{KL} \\left( p_\\text{data} \\bigg\\Vert \\frac{p_\\text{data} + p_{\\color{red}G}}{2} \\right) + \\operatorname{D}_\\text{KL} \\left( p_{{\\color{red}G}} \\bigg\\Vert \\frac{p_\\text{data} + p_{\\color{red}G}}{2} \\right) \\right]. \\] <p>Off by a Constant</p> <p>The value function of GAN for fixed \\(G\\) is slightly different from JS divergence<sup>3</sup>,</p> \\[ \\underset{G}{\\operatorname{max}}V({\\color{red}G},{\\color{green}D}) = 2 \\operatorname{D}_\\text{JS}( p_\\text{data} \\Vert p_{\\color{red}G} ) - \\log 4. \\]", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#alternating-training", "title": "Alternating Training", "text": "<p>GAN training requires two stages,</p> <ul> <li>train discriminator \\({\\color{green}D}\\), and</li> <li>train generator \\({\\color{red}G}\\).</li> </ul> <p></p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#gan-code", "title": "GAN Code", "text": "<p>We built a simple GAN using MNIST dataset.</p> ResultCode <p>The generated images look quite close to handwritings.</p> <p></p> <pre><code>import matplotlib.pyplot as plt\nimport torch\nfrom pathlib import Path\nimport torchvision\nimport torchvision.transforms as transforms\nfrom loguru import logger\nfrom torch import nn\nimport click\n\n\n\nlogger.debug(f\"Setting device ...\")\ndevice = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nlogger.info(f\"Device in use: {device}\")\n\n\ndef plot_images(image_samples, target):\n    \"\"\"Plot a grid of images and save to a file.\"\"\"\n\n    if not Path(target).parent.exists():\n        Path(target).parent.mkdir(parents=True)\n\n    # real_samples, mnist_labels = next(iter(train_loader))\n    for i in range(16):\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(image_samples[i].reshape(28, 28), cmap=\"gray_r\")\n        plt.xticks([])\n        plt.yticks([])\n    plt.savefig(target)\n\n\ndef get_data_loaders(batch_size=32, data_dir=\"data/mnist\", download=True, plot_samples=True):\n    \"\"\"Get MNIST data and built a dataloader for the dataset\"\"\"\n\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n    )\n\n    train_set = torchvision.datasets.MNIST(\n        root=data_dir, train=True, download=download, transform=transform\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=batch_size, shuffle=True\n    )\n\n    if plot_samples:\n        real_samples, mnist_labels = next(iter(train_loader))\n        plot_images(real_samples, target=\"assets/real_images/real_image_samples.png\")\n\n    return train_loader\n\n\nclass Discriminator(nn.Module):\n    \"\"\"The discrimnator should take data that has the dimension of the image and spit out a probability\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(784, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), 784)\n        output = self.model(x)\n        return output\n\n\nclass Generator(nn.Module):\n    \"\"\"The generator should take in some noise data (a latent space data) and spit out an image.\n    We use the input noise as a trick to make the generator more general\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            # nn.Linear(10, 100),\n            # nn.ReLU(),\n            nn.Linear(100, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 784),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        output = output.view(x.size(0), 1, 28, 28)\n        return output\n\n\n\n@click.command()\n@click.option(\"--epochs\", default=50, help=\"Number of epochs for the training\")\n@click.option(\"--learning_rate\", \"-lr\", default=0.0001, help=\"Learning rate for the optimizer\")\n@click.option(\"--batch_size\", default=32, help=\"Batch size\")\n@click.option(\"--data_dir\", default=\"data/mnist\", help=\"Directory for storing the dataset\")\n@click.option(\"--download_mnist\", \"-d\", default=True, type=bool, help=\"Whether to download MNIST data\")\n@click.option(\"--random_seed\", \"-rs\", default=42, type=int, help=\"Random seed for the random generators\")\ndef main(epochs, learning_rate, batch_size, data_dir, download_mnist, random_seed):\n\n    latent_space_dim = 100\n\n    torch.manual_seed(random_seed)\n\n    # check the dtypes\n    logger.debug(\n        f\"torch tensor dtype: {torch.tensor([1.2, 3]).dtype}\"\n    )\n    # torch.set_default_dtype(torch.float64)\n    # logger.debug(\n    #     f\"set torch tensor dtype to 64: {torch.tensor([1.2, 3]).dtype}\"\n    # )\n\n    train_loader = get_data_loaders(\n        batch_size=batch_size,\n        data_dir=data_dir,\n        download=download_mnist\n    )\n    logger.debug(f\"Training data is ready\")\n\n    discriminator = Discriminator().to(device=device)\n    generator = Generator().to(device=device)\n\n    loss_function = nn.BCELoss()\n\n    optimizer_discriminator = torch.optim.Adam(\n        discriminator.parameters(), lr=learning_rate\n    )\n    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n\n    for epoch in range(epochs):\n        for n, (real_samples, mnist_labels) in enumerate(train_loader):\n            # We prepare some data for training the discriminator\n            # Here we will prepare both the generated data and the real data\n            real_samples = real_samples.to(device=device)\n            real_samples_labels = torch.ones((batch_size, 1)).to(device=device)\n            latent_space_samples = torch.randn((batch_size, latent_space_dim)).to(device=device)\n            # logger.debug(f\"Latent space samples: {latent_space_samples}\")\n            generated_samples = generator(latent_space_samples)\n            # logger.debug(f\"Generated samples:{generated_samples}\")\n            generated_samples_labels = torch.zeros((batch_size, 1)).to(device=device)\n            all_samples = torch.cat((real_samples, generated_samples))\n            all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n\n            # Training the discriminator\n            # The discrinimator is trained using the samples we generated above, i.e.\n            # the generated samples and the real images\n            discriminator.zero_grad()\n            output_discriminator = discriminator(all_samples)\n            loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n            loss_discriminator.backward()\n            optimizer_discriminator.step()\n\n            # Generate some noise data for training the generator\n            #\n            latent_space_samples_generator = torch.randn((batch_size, latent_space_dim)).to(device=device)\n\n            # Training the generator using the training optimizer\n            generator.zero_grad()\n            generated_samples_generator = generator(latent_space_samples_generator)\n            output_discriminator_generated = discriminator(generated_samples_generator)\n            loss_generator = loss_function(\n                output_discriminator_generated, real_samples_labels\n            )\n            loss_generator.backward()\n            optimizer_generator.step()\n\n            # Show loss\n            if n == batch_size - 1:\n                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n                print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n\n        logger.debug(f\"Plotting for epoch: {epoch} ...\")\n        latent_space_samples_epoch = torch.randn(batch_size, latent_space_dim).to(device=device)\n        generated_samples_epoch = generator(latent_space_samples_epoch)\n        generated_samples_epoch = generated_samples_epoch.cpu().detach()\n        plot_images(generated_samples_epoch, target=f\"assets/generated_images/generated_image_samples_{epoch}.png\")\n        logger.debug(f\"Saved plots for epoch: {epoch}\")\n\n    latent_space_samples = torch.randn(batch_size, latent_space_dim).to(device=device)\n    generated_samples = generator(latent_space_samples)\n\n    logger.debug(f\"Plot generated images...\")\n    generated_samples = generated_samples.cpu().detach()\n    plot_images(generated_samples, target=\"assets/generated_images/generated_image_samples.png\")\n\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#f-gan", "title": "f-GAN", "text": "<p>The essence of GAN is comparing the generated distribution \\(p_G\\) and the data distribution \\(p_\\text{data}\\). The vanilla GAN considers the Jensen-Shannon divergence \\(\\operatorname{D}_\\text{JS}(p_\\text{data}\\Vert p_{G})\\). The discriminator \\({\\color{green}D}\\) serves the purpose of forcing this divergence to be small.</p> <p>Why do we need the discriminator?</p> <p>If the JS divergence is an objective, why do we need the discriminator? Even in f-GAN we need a functional to approximate the f-divergence. This functional we choose works like the discriminator of GAN.</p> <p>There exists a more generic form of JS divergence, which is called f-divergence<sup>6</sup>. f-GAN obtains the model by estimating the f-divergence between the data distribution and the generated distribution<sup>1</sup>.</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#variational-divergence-minimization", "title": "Variational Divergence Minimization", "text": "<p>The Variational Divergence Minimization (VDM) extends the variational estimation of f-divergence<sup>1</sup>. VDM searches for the saddle point of an objective \\(F({\\color{red}\\theta}, {\\color{blue}\\omega})\\), i.e., min w.r.t. \\(\\theta\\) and max w.r.t \\({\\color{blue}\\omega}\\), where \\({\\color{red}\\theta}\\) is the parameter set of the generator \\({\\color{red}Q_\\theta}\\), and \\({\\color{blue}\\omega}\\) is the parameter set of the variational approximation to estimate f-divergence, \\({\\color{blue}T_\\omega}\\).</p> <p>The objective \\(F({\\color{red}\\theta}, {\\color{blue}\\omega})\\) is related to the choice of \\(f\\) in f-divergence and the variational functional \\({\\color{blue}T}\\),</p> \\[ \\begin{align} &amp; F(\\theta, \\omega)\\\\ =&amp; \\mathbb E_{x\\sim p_\\text{data}} \\left[ {\\color{blue}T_\\omega}(x) \\right] - \\mathbb E_{x\\sim {\\color{red}Q_\\theta} } \\left[ f^*({\\color{blue}T_\\omega}(x)) \\right] \\\\ =&amp; \\mathbb E_{x\\sim p_\\text{data}} \\left[ g_f(V_{\\color{blue}\\omega}(x)) \\right] - \\mathbb E_{x\\sim {\\color{red}Q_\\theta} } \\left[ f^*(g_f(V_{\\color{blue}\\omega}(x))) \\right]. \\end{align} \\] <p>In the above objective,</p> <ul> <li>\\(f^*\\) is the Legendre\u2013Fenchel transformation of \\(f\\), i.e., \\(f^*(t) = \\operatorname{sup}_{u\\in \\mathrm{dom}_f}\\left\\{ ut - f(u) \\right\\}\\).</li> </ul> \\(T\\) <p>The function \\(T\\) is used to estimate the lower bound of f-divergence<sup>1</sup>.</p> Choice of \\(g_f\\) and \\(V\\) <p>Nowozin et al provided a table for \\(g_f\\) and \\(V\\)<sup>1</sup>.</p> <p></p> <p>We estimate</p> <ul> <li>\\(\\mathbb E_{x\\sim p_\\text{data}}\\) by sampling from the mini-batch, and</li> <li>\\(\\mathbb E_{x\\sim {\\color{red}Q_\\theta} }\\) by sampling from the generator.</li> </ul> <p>Reduce to GAN</p> <p>The VDM loss can be reduced to the loss of GAN by setting<sup>1</sup></p> \\[ \\begin{align} \\log {\\color{green}D_\\omega} =&amp; g_f(V_{\\color{blue}\\omega}(x))  \\\\ \\log \\left( 1 - {\\color{green}D_\\omega} \\right) =&amp; -f^*\\left( g_f(V_{\\color{blue}\\omega}(x)) \\right). \\end{align} \\] <p>It is straightforward to validate that the following result is a solution to the above set of equations,</p> \\[ g_f(V) = \\log \\frac{1}{1 + e^{-V}}. \\]", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#code", "title": "Code", "text": "<ul> <li>minlee077/f-GAN-pytorch</li> <li>shayneobrien/generative-models</li> </ul>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#infogan", "title": "InfoGAN", "text": "<p>In GAN, the latent space input is usually random noise, e.g., Gaussian noise. The objective of GAN is a very generic one. It doesn't say anything about how exactly the latent space will be used. This is not desirable in many problems. We would like to have more interpretability in the latent space. InfoGAN introduced constraints to the objective to enforce the interpretability of the latent space<sup>8</sup>.</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#constraint", "title": "Constraint", "text": "<p>The constraint InfoGAN proposed is mutual information,</p> \\[ \\underset{{\\color{red}G}}{\\operatorname{min}} \\underset{{\\color{green}D}}{\\operatorname{max}} V_I ({\\color{green}D}, {\\color{red}G}) = V({\\color{green}D}, {\\color{red}G}) - \\lambda I(c; {\\color{red}G}(z,c)), \\] <p>where</p> <ul> <li>\\(c\\) is the latent code,</li> <li>\\(z\\) is the random noise input,</li> <li>\\(V({\\color{green}D}, {\\color{red}G})\\) is the objective of GAN,</li> <li>\\(I(c; {\\color{red}G}(z,c))\\) is the mutual information between the input latent code and generated data.</li> </ul> <p>Using the lambda multiplier, we punish the model if the generator loses information in latent code \\(c\\).</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#training", "title": "Training", "text": "<p>The training steps are almost the same as GAN but with one extra loss to be calculated in each mini-batch.</p> <ol> <li>Train \\(\\color{red}G\\) using loss: \\(\\operatorname{MSE}(v', v)\\);</li> <li>Train \\(\\color{green}D\\) using loss: \\(\\operatorname{MSE}(v', v)\\);</li> <li>Apply Constraint:<ol> <li>Sample data from mini-batch;</li> <li>Calculate loss \\(\\lambda_{l} H(l';l)+\\lambda_c \\operatorname{MSE}(c,c')\\)</li> </ol> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/gan/#python-code", "title": "Python Code", "text": "<p>eriklindernoren/PyTorch-GAN</p> <ol> <li> <p>Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709 \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Minimax. In: Wikipedia [Internet]. 5 Aug 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/Minimax \u21a9</p> </li> <li> <p>Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative Adversarial Networks. arXiv [stat.ML]. 2014. Available: http://arxiv.org/abs/1406.2661 \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218 \u21a9</p> </li> <li> <p>Arjovsky M, Chintala S, Bottou L. Wasserstein GAN. arXiv [stat.ML]. 2017. Available: http://arxiv.org/abs/1701.07875 \u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence#Instances_of_f-divergences \u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Convex conjugate. In: Wikipedia [Internet]. 20 Feb 2021 [cited 7 Sep 2021]. Available: https://en.wikipedia.org/wiki/Convex_conjugate \u21a9</p> </li> <li> <p>Chen X, Duan Y, Houthooft R, Schulman J, Sutskever I, Abbeel P. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1606.03657 \u21a9</p> </li> <li> <p>Agakov DBF. The im algorithm: a variational approach to information maximization. Adv Neural Inf Process Syst. 2004. Available: https://books.google.com/books?hl=en&amp;lr=&amp;id=0F-9C7K8fQ8C&amp;oi=fnd&amp;pg=PA201&amp;dq=Algorithm+variational+approach+Information+Maximization+Barber+Agakov&amp;ots=TJGrkVS610&amp;sig=yTKM2ZdcZQBTY4e5Vqk42ayUDxo \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/infogan/", "title": "InfoGAN", "text": "<p>In GAN, the latent space input is usually random noise, e.g., Gaussian noise. The objective of GAN is a very generic one. It doesn't say anything about how exactly the latent space will be used. This is not desirable in many problems. We would like to have more interpretability in the latent space. InfoGAN introduced constraints to the objective to enforce interpretability of the latent space<sup>1</sup>.</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/infogan/#constraint", "title": "Constraint", "text": "<p>The constraint InfoGAN proposed is mutual information,</p> \\[ \\underset{{\\color{red}G}}{\\operatorname{min}} \\underset{{\\color{green}D}}{\\operatorname{max}} V_I ({\\color{green}D}, {\\color{red}G}) = V({\\color{green}D}, {\\color{red}G}) - \\lambda I(c; {\\color{red}G}(z,c)), \\] <p>where</p> <ul> <li>\\(c\\) is the latent code,</li> <li>\\(z\\) is the random noise input,</li> <li>\\(V({\\color{green}D}, {\\color{red}G})\\) is the objective of GAN,</li> <li>\\(I(c; {\\color{red}G}(z,c))\\) is the mutual information between the input latent code and generated data.</li> </ul> <p>Using the lambda multiplier, we punish the model if the generator loses information in latent code \\(c\\).</p>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/infogan/#training", "title": "Training", "text": "<p>The training steps are almost the same as GAN but with one extra loss to be calculated in each mini-batch.</p> <ol> <li>Train \\(\\color{red}G\\) using loss: \\(\\operatorname{MSE}(v', v)\\);</li> <li>Train \\(\\color{green}D\\) using loss: \\(\\operatorname{MSE}(v', v)\\);</li> <li>Apply Constraint:<ol> <li>Sample data from mini-batch;</li> <li>Calculate loss \\(\\lambda_{l} H(l';l)+\\lambda_c \\operatorname{MSE}(c,c')\\)</li> </ol> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/infogan/#code", "title": "Code", "text": "<p>eriklindernoren/PyTorch-GAN</p> <ol> <li> <p>Chen X, Duan Y, Houthooft R, Schulman J, Sutskever I, Abbeel P. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1606.03657 \u21a9</p> </li> <li> <p>Agakov DBF. The im algorithm: a variational approach to information maximization. Adv Neural Inf Process Syst. 2004. Available: https://books.google.com/books?hl=en&amp;lr=&amp;id=0F-9C7K8fQ8C&amp;oi=fnd&amp;pg=PA201&amp;dq=Algorithm+variational+approach+Information+Maximization+Barber+Agakov&amp;ots=TJGrkVS610&amp;sig=yTKM2ZdcZQBTY4e5Vqk42ayUDxo \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/adversarial/intro/", "title": "Adversarial Models", "text": "<ul> <li>GAN</li> <li>f-GAN</li> </ul>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/contrastive-predictive-coding/", "title": "Contrastive Predictive Coding", "text": "<p>Contrastive Predictive Coding, CPC, is an autoregressive model combined with InfoNCE loss<sup>1</sup>.</p> <p>Predictive Coding</p> <p>As a related topic, predictive coding is a different scheme than backpropagation. Predictive coding updates the weights using local updating rules only<sup>2</sup>.</p> <p>There are two key ideas in CPC:</p> <ul> <li>Autoregressive models in latent space, and</li> <li>InfoNCE loss that combines mutual information and NCE.</li> </ul> <p>For the series of segments, \\(\\{x_t\\}\\), we apply an encoder on each segment, and calculate the latent space, \\(\\{{\\color{blue}\\hat x_t}\\}\\). The latent space \\(\\{{\\color{blue}\\hat x_t}\\}\\) is then modeled using an autoregressive model to calculate the coding, \\(\\{{\\color{red}c_t}\\}\\).</p> van den Oord et al <p>The loss is built on NCE to estimate the lower bound of mutual information,</p> \\[ \\mathcal L = -\\mathbb E_X \\left[ \\log \\frac{f_k(x_{t+k}, c_t)}{\\sum_{j} f_k(x_{j}, c_t) } \\right], \\] <p>where \\(f_k(x_{x+i}, c_t)\\) is estimated using a log-bilinear model, \\(f_k(x_{x+i}, c_t) = \\exp\\left( z_{t+i} W_i c_t \\right)\\). This is also a cross entropy loss.</p> <p>Minimizing \\(\\mathcal L\\) leads to a \\(f_k\\) that estimates the ratio<sup>1</sup></p> \\[ \\frac{p(x_{t+k}\\mid c_t)}{p(x_{t+k})} = \\frac{p(x_{t+k}, c_t)}{p(x_{t+k})p(c_t)}. \\] <p>We can perform downstream tasks such as classifications using the encoders.</p> <p>Maximizing this lower bound?</p> <p>This so-called lower bound for mutual information in this case is not always going to work[^Newell2020]. In some cases, the representations learned using this lower bound doesn't help or even worsen the performance of downstream tasks.</p>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/contrastive-predictive-coding/#code", "title": "Code", "text": "<p>rschwarz15/CPCV2-PyTorch</p> <ol> <li> <p>van den Oord A, Li Y, Vinyals O. Representation learning with Contrastive Predictive Coding. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1807.03748 \u21a9\u21a9</p> </li> <li> <p>Millidge B, Tschantz A, Buckley CL. Predictive coding approximates backprop along arbitrary computation graphs. 2020.http://arxiv.org/abs/2006.04182.\u00a0\u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/deep-infomax/", "title": "Deep Infomax", "text": "<p>Max Global Mutual Information</p> <p>Why not just use the global mutual information of the input and encoder output as the objective?</p> <p>... maximizing MI between the complete input and the encoder output (i.e.,globalMI) is ofteninsufficient for learning useful representations.</p> <p>-- Devon et al<sup>1</sup></p> <p>Mutual information maximization is performed on the input of the encoder \\(X\\) and the encoded feature \\(\\hat X=E_\\theta (X)\\),</p> \\[ \\operatorname{arg~max}_\\theta I(X;E_\\theta (X)). \\] <p>Being a quantity that is notoriously hard to compute, mutual information \\(I(X;E_\\theta (X))\\) is usually estimated using its lower bound, which depends on a choice of a functional \\(T_\\omega\\). Thus the objective will be maximizing a parametrized mutual information estimation,</p> \\[ \\operatorname{arg~max}_{\\theta, \\omega} \\hat I_\\omega(X;E_\\theta (X)) \\] <p>Local or Global</p> <p>Two approaches to apply mutual information on encoders:</p> <ul> <li>Global mutual information of full input and full encoding. This is useful for reconstruction of the input.</li> <li>Local mutual information of local patches of input full encoding. This is useful for classification.</li> </ul>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/deep-infomax/#local-mutual-information", "title": "Local Mutual Information", "text": "<p>To compare local features to the encoder output, we need to extract values from inside the encoder, i.e.,</p> \\[ E_{\\theta_f, \\theta_C} = f_{\\theta_f} \\circ C_{\\theta_C}. \\] <p>The first step, \\(C_{\\theta_C}\\) is to map the input into feature maps, the second step, \\(f_{\\theta_f}\\) maps the feature maps into the encoding. The feature map \\(C_{\\theta_C}\\) is splitted into patches, \\(C_{\\theta_C}=\\left\\{ C_\\theta^{(i)} \\right\\}\\). The objective is</p> \\[ \\operatorname{arg~max}_{\\theta_f, \\theta_C, \\omega}\\mathbb E_{i} \\left[ \\hat I_\\omega( C_{\\theta_C}^{(i)} ;E_\\theta (X)) \\right]. \\] <p></p> <p>Why does local mutual information help</p> <p>Devon et al explained the idea behind choosing local mutual information<sup>1</sup>.</p> <p>Global mutual information doesn't specify what is the meaningful information. Some very local noise can also be treated as meaningful information too.</p> <p>Local mutual information splits the input into patches, and calculate the mutual information between each patch and the encoding. If the model only uses some information from a few local patches, the mutual information objective will be small after averaging all the patches. Thus local mutual information forces the model to use information that is global in the input.</p>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/deep-infomax/#code", "title": "Code", "text": "<ul> <li>rdevon/DIM: by the authors</li> <li>DuaneNielsen/DeepInfomaxPytorch: a clean implementation</li> </ul> <ol> <li> <p>Devon Hjelm R, Fedorov A, Lavoie-Marchildon S, Grewal K, Bachman P, Trischler A, et al. Learning deep representations by mutual information estimation and maximization. arXiv [stat.ML]. 2018. Available: http://arxiv.org/abs/1808.06670 \u21a9\u21a9</p> </li> <li> <p>Newell A, Deng J. How Useful is Self-Supervised Pretraining for Visual Tasks? arXiv [cs.CV]. 2020. Available: http://arxiv.org/abs/2003.14323 \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/contrastive/intro/", "title": "Contrastive Models", "text": "<p>Contrastive Models</p> <p>Learn to compare.</p> <ol> <li> <p>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218 \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/generative/ae/", "title": "Autoencoders", "text": "<p>Autoencoders (AE) are machines that encode inputs into a compact latent space.</p> <p></p> Notation: dot (\\(\\cdot\\)) <p>We use a single vertically centered dot, i.e., \\(\\cdot\\), to indicate that the function or machine can take in arguments.</p> <p>A simple autoencoder can be achieved using two neural nets, e.g.,</p> \\[ \\begin{align} {\\color{green}h} &amp;= {\\color{blue}g}{\\color{blue}(}{\\color{blue}b} + {\\color{blue}w} x{\\color{blue})} \\\\ \\hat x &amp;= {\\color{red}\\sigma}{\\color{red}(c} + {\\color{red}v} {\\color{green}h}{\\color{red})}, \\end{align} \\] <p>where in this simple example,</p> <ul> <li>\\({\\color{blue}g(b + w \\cdot )}\\) is the encoder, and</li> <li>\\({\\color{red}\\sigma(c + v \\cdot )}\\) is the decoder.</li> </ul> <p>For binary labels, we can use a simple cross entropy as the loss.</p>", "tags": ["WIP"]}, {"location": "self-supervised/generative/ae/#code", "title": "Code", "text": "<p>See Lippe<sup>1</sup>.</p> <ol> <li> <p>Lippe P. Tutorial 9: Deep Autoencoders \u2014 UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/generative/autoregressive/", "title": "Autoregressive Model", "text": "<p>An autoregressive (AR) model is autoregressive,</p> \\[ \\begin{equation} \\log p_\\theta (x) = \\sum_{t=1}^T \\log p_\\theta ( x_{t} \\mid \\{x_{&lt;t}\\} ). \\end{equation} \\] Notations and Conventions <p>In AR models, we have to mention the preceding nodes (\\(\\{x_{&lt;t}\\}\\)) of a specific node (\\(x_{t}\\)). For \\(t=5\\), the relations between \\(\\{x_{&lt;5}\\}\\) and \\(x_5\\) are shown in the following illustration.</p> <p></p> <p>There are different notations for such relations.</p> <ul> <li>In Uria et al., the authors use \\(p(x_{o_d}\\mid \\mathbf x_{o_{&lt;d}})\\) <sup>1</sup>.</li> <li>In Liu et al. and Papamakarios et al., the authors use \\(p(x_{t}\\mid \\mathbf x_{1:t-1})\\) <sup>6</sup><sup>4</sup>.</li> <li>In Germain et al., the authors use \\(p(x_t\\mid \\mathbf x_{&lt;t})\\) <sup>5</sup>.</li> </ul> <p>In the current review, we expanded the vector notation \\(\\mathbf x_{&lt;t}\\) into a set notation as it is not necessarily a vector.</p> <ol> <li> <p>Uria B, C\u00f4t\u00e9 M-A, Gregor K, Murray I, Larochelle H. Neural Autoregressive Distribution Estimation. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1605.02226 \u21a9</p> </li> <li> <p>Triebe O, Laptev N, Rajagopal R. AR-Net: A simple Auto-Regressive Neural Network for time-series. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1911.12436 \u21a9</p> </li> <li> <p>Ho G. George Ho. In: Eigenfoo [Internet]. 9 Mar 2019 [cited 19 Sep 2021]. Available: https://www.eigenfoo.xyz/deep-autoregressive-models/ \u21a9</p> </li> <li> <p>Papamakarios G, Pavlakou T, Murray I. Masked Autoregressive Flow for Density Estimation. arXiv [stat.ML]. 2017. Available: http://arxiv.org/abs/1705.07057 \u21a9</p> </li> <li> <p>Germain M, Gregor K, Murray I, Larochelle H. MADE: Masked autoencoder for distribution estimation. 32nd International Conference on Machine Learning, ICML 2015. 2015;2: 881\u2013889. Available: http://arxiv.org/abs/1502.03509 \u21a9</p> </li> <li> <p>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218 \u21a9</p> </li> <li> <p>Lippe P. Tutorial 12: Autoregressive Image Modeling \u2014 UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html \u21a9</p> </li> <li> <p>rogen-george. rogen-george/Deep-Autoregressive-Model. In: GitHub [Internet]. [cited 20 Sep 2021]. Available: https://github.com/rogen-george/Deep-Autoregressive-Model \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/generative/flow/", "title": "Flow", "text": "<p>For a probability density \\(p(x)\\) and a transformation of coordinate \\(x=g(z)\\) or \\(z=f(x)\\), the density can be expressed using the coordinate transformations, i.e.,</p> \\[ \\begin{align} p(x) &amp;= \\tilde p (f(x)) \\lvert \\operatorname{det} \\operatorname{D} g(f(x)) \\rvert^{-1} \\\\ &amp;= \\tilde p(f(x)) \\lvert \\operatorname{det}\\operatorname{D} f(x) \\rvert \\end{align} \\] <p>where the Jacobian is</p> \\[ \\operatorname{D} g(z) \\to \\frac{\\partial }{\\partial z} g. \\] <p>The operation \\(g_{*}\\circ \\tilde p(z)\\) is the push forward of \\(\\tilde p(z)\\). The operation \\(g_{*}\\) will pushforward simple distribution \\(\\tilde p(z)\\) to a more complex distribution \\(p(x)\\).</p> <ul> <li>The generative direction: sample \\(z\\) from distribution \\(\\tilde p(z)\\), apply transformation \\(g(z)\\);</li> <li>The normalizing direction: \"simplify\" \\(p(x)\\) to some simple distribution \\(\\tilde p(z)\\).</li> </ul> <p>The key to the flow model is the chaining of the transformations</p> \\[ \\operatorname{det} \\operatorname{D} f(x) = \\Pi_{i=1}^N \\operatorname{det} \\operatorname{D} f_i (x_i) \\] <p>where</p> \\[ \\begin{align} x_i &amp;= g_i \\circ \\cdots \\circ g_1 (z)\\\\ &amp;= f_{i+1} \\circ \\cdots \\circ f_N (x). \\end{align} \\] <ol> <li> <p>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218 \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "self-supervised/generative/intro/", "title": "Generative Models", "text": "<p>Generative models come with</p> <ul> <li>an encoder,</li> <li>an explicit latent space, and</li> <li>a decoder.</li> </ul>", "tags": ["WIP"]}, {"location": "self-supervised/generative/made/", "title": "MADE: Masked Autoencoder for Distribution Estimation", "text": "", "tags": ["WIP"]}, {"location": "self-supervised/generative/maf/", "title": "MAF: Masked Autoregressive Flow", "text": "", "tags": ["WIP"]}, {"location": "self-supervised/generative/vae/", "title": "Variational AutoEncoder", "text": "<p>Variational AutoEncoder (VAE) is very different from AE. In VAE, we introduce a variational distribution \\(q\\) to help us work out the weighted integral after introducing the latent space variable \\(z\\),</p> \\[ \\begin{align} \\ln p_\\theta(x) &amp;= \\int \\left(\\ln p_\\theta (x\\mid z) \\right)p(z) \\,\\mathrm d z \\\\ &amp;=  \\int \\left(\\ln\\frac{q_{\\phi}(z\\mid x)}{q_{\\phi}(z\\mid x)} p_\\theta (x\\mid z) \\right) p(z) \\, \\mathrm d z \\end{align} \\] <p>In the above derivation,</p> <ul> <li>\\({}_\\theta\\) is the model for inference, and</li> <li>\\({}_\\phi\\) is the model for variational approximation.</li> </ul> Tricks <ul> <li>\\(p_\\theta(x\\mid z)\\) is usually Gaussian distribution of \\(x\\) but with mean parameterized by the latent variable \\(z\\) and the model parameters \\(\\theta\\).</li> <li>The latent space variable \\(p(z)\\) is usually assumed to be a normal distribution.</li> <li>The marginalization of the latent variable increase the expressive power.</li> <li>Instead of modeling a complex likelihood \\(p(x\\mid z)\\) directly, we only need to model parameters of Gaussian distributions, e.g., a function \\(f(z, \\theta)\\) for the mean of the Gaussian distribution.</li> </ul> <p></p> <p>From simple distribution in latent space to a more complex distribution. [Doersch2016]</p> <p>The demo looks great. However, sampling from latent space becomes more difficult as the dimension of the latent space increase. We need a more efficient way to sample from the latent space. We use the variational method which uses a model that samples \\(z\\) based on \\(x\\) to sample \\(z\\), i.e., introduce a function \\(q(z\\mid x)\\) to help us with sampling.</p> \\[ \\begin{align} \\ln p_\\theta(x) &amp;= \\int \\left(\\ln p_\\theta (x\\mid z) \\right)p(z) \\,\\mathrm d z \\\\ &amp;=  \\int \\left(\\ln\\frac{q_{\\phi}(z\\mid x)}{q_{\\phi}(z\\mid x)} p_\\theta (x\\mid z) \\right) p(z) \\, \\mathrm d z \\\\ &amp;= \\int dz q(z\\mid x) \\ln \\frac{p(x,z)}{q(z\\mid x)} + \\int dz q(z\\mid x) \\ln \\frac{q(z\\mid x)}{p(z\\mid x)}  \\\\ &amp;= - \\left[ D_{\\mathrm{KL}} ( q_{\\phi}(z\\mid x) \\mathrel{\\Vert} p(z)  )  -  \\mathbb E_q ( \\ln p_\\theta (x\\mid z) ) \\right] + D_{\\mathrm{KL}}( q(z\\mid x)\\parallel p(z\\mid x) ) \\\\ &amp; \\geq - \\left[ D_{\\mathrm{KL}} ( q_{\\phi}(z\\mid x) \\mathrel{\\Vert} p(z)  )  -  \\mathbb E_q ( \\ln p_\\theta (x\\mid z) ) \\right] \\\\ &amp;\\equiv - F(x) \\\\ &amp;\\equiv \\mathcal L . \\end{align} \\] <p>In the derivation, we used \\(\\int dz q(z\\mid x) = 1\\).</p> <p>The term \\(F(x)\\) is the free energy, while the negative of it, \\(-F(x)=\\mathcal L\\), is the so-called Evidence Lower Bound (ELBO),</p> \\[ \\mathcal L = - D_{\\mathrm{KL}} ( q_{\\phi}(z\\mid x) \\mathrel{\\Vert} p(z)  )  +  \\mathbb E_q ( \\ln p_\\theta (x\\mid z) ). \\] <p>We also dropped the term \\(D_{\\mathrm{KL}}( q(z\\mid x)\\parallel p(z\\mid x) )\\) which is always non-negative. The reason is that we can not maximize this KL divergence as we do not know \\(p(z\\mid x)\\). But the KL divergence is always non-negative. So if we find a \\(q\\) that can maximize \\(\\mathcal L\\), then we are also minimizing the KL divergence (with a function \\(q(z\\mid x)\\) that is close to \\(p(z\\mid x)\\)) and maximizing the loglikelihood loss. Now we only need to find a way to maximize \\(\\mathcal L\\).</p> More about this ELBO <p>We do not know \\(p(x,z)\\) either but we can rewrite \\(\\mathcal L\\),</p> \\[\\begin{align} \\mathcal L(q) =&amp; \\int dz q(z\\mid x) \\ln\\frac{p(x,z)}{q(z\\mid x)} \\\\\\\\ =&amp; \\int dz q(z\\mid x)\\ln \\frac{p(x\\mid z)p(z)}{q(z\\mid x)} \\\\\\\\ = &amp; \\int dz q(z\\mid x) \\ln p(x\\mid z) + \\int dz q(z\\mid x) \\ln \\frac{p(z)}{q(z\\mid x)} \\\\\\\\ = &amp; \\int dz q(z\\mid x) \\ln p(x\\mid z) - \\operatorname{KL} \\left( q(z\\mid x) \\parallel p(z) \\right) \\end{align}\\] <p>Our loss function becomes</p> \\[- \\mathcal L(q) = - \\mathbb E_{q} \\ln {\\color{red}p(x\\mid z)} + \\operatorname{KL} \\left( {\\color{blue}q(z\\mid x) }\\parallel p(z) \\right),\\] <p>where \\({\\color{blue}q(z\\mid x) }\\) is our encoder which encodes data \\(x\\) to the latent data \\(z\\), and \\({\\color{red}p(x\\mid z)}\\) is our decoder. The second term ensures our encoder is similar to our priors.</p>", "tags": ["WIP"]}, {"location": "self-supervised/generative/vae/#using-neural-networks", "title": "Using Neural networks", "text": "<p>We model the parameters of the Gaussian distribution \\(p_\\theta(x\\mid z)\\), e.g., \\(f(z, \\theta)\\), using a neural network.</p> <p>In reality, we choose a gaussian form of the variational functional with the mean and variance depending on the data \\(x\\) and the latent variable \\(z\\)</p> \\[ q(z\\mid x) = \\mathcal N ( \\mu(x,z), \\Sigma (x,z) ). \\] <p>We have</p> \\[ \\begin{align} &amp;\\ln p_\\theta(x\\mid z) \\\\ =&amp; \\ln \\mathscr N( x\\mid f(z, \\theta), \\sigma^2 I )\\\\ =&amp; \\ln \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{\\left( -\\frac{(x -f(z,\\theta)^2)}{\\sigma^2} \\right)} \\right) \\\\ =&amp; -(x - f(z, \\theta))^2 + \\mathrm{Const.} \\end{align} \\] Why don't we simply draw  \\(q\\)  from  \\(p(z)\\)? <p>If we are sort of minimizing the KL divergence \\(\\operatorname{KL} \\left( {\\color{blue}q(z\\mid x) }\\parallel p(z) \\right)\\) too, why don't we simply draw \\(q\\) from \\(p(z)\\)? First of all, we also have to take care of the first term. Secondly, we need a latent space that connects to the actual data for reconstruction.</p>", "tags": ["WIP"]}, {"location": "self-supervised/generative/vae/#structure", "title": "Structure", "text": "<p>Doersch wrote a very nice tutorial on VAE<sup>1</sup>. We can find the detailed structures of VAE.</p> <p>Another key component of VAE is the reparametrization trick. The variational approximation \\(q_\\phi\\) is usually a Gaussian distribution. Once we get the parameters for the Gaussian distribution, we will have to sample from the Gaussian distribution based on the parameters. However, this sampling process prohibits us from propagating errors. The reparametrization trick solves this problem.</p>", "tags": ["WIP"]}, {"location": "self-supervised/generative/vae/#loss-explanation", "title": "Loss Explanation", "text": "<p>VAE Loss Explained <sup>1</sup></p>", "tags": ["WIP"]}, {"location": "self-supervised/generative/vae/#code", "title": "Code", "text": "<p>See Lippe<sup>2</sup>.</p> <ol> <li> <p>Doersch C. Tutorial on Variational Autoencoders. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.05908 \u21a9\u21a9</p> </li> <li> <p>Lippe P. Tutorial 9: Deep Autoencoders \u2014 UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "time-series/", "title": "Time Series Data and Statistical Forecasting Mothods", "text": ""}, {"location": "time-series/#time-series-data", "title": "Time Series Data", "text": "<p>Time series data comes from a variety of data generating processes. There are also different formulations and views of time series data.</p> <p>Time series data can be formulated as a sequence of vector functions of time <sup>1</sup>. There are many different types of tasks on time series data, for example,</p> <ul> <li>classification,</li> <li>anomaly detection, and</li> <li>forecasting.</li> </ul> <p>In this chapter, we focus on the forecasting problem.</p>"}, {"location": "time-series/#the-forecasting-problem", "title": "The Forecasting Problem", "text": "<p>To make it easier to formulate the forecasting problem, we group the time series features based on the role they play in a forecasting problem. Given a dataset \\(\\mathcal D\\), with</p> <ol> <li>\\(y^{(i)}_t\\), the sequential variable to be forecasted,</li> <li>\\(x^{(i)}_t\\), exogenous data for the time series data,</li> <li>\\(u^{(i)}_t\\), some features that can be obtained or planned in advance,</li> </ol> <p>where \\({}^{(i)}\\) indicates the \\(i\\)th variable, \\({}_ t\\) denotes time. In a forecasting task, we use \\(y^{(i)} _ {t-K:t}\\), \\(x^{(i) _ {t-K:t}}\\), and \\(u^{(i)} _ {t-K:t+H}\\), to forecast the future \\(y^{(i)} _ {t+1:t+H}\\). In these notations, \\(K\\) is the input sequence length and \\(H\\) is the forecast horizon.</p> <p></p> <p>A forecasting model \\(f\\) will use \\(x^{(i)} _ {t-K:t}\\) and \\(u^{(i)} _ {t-K:t+H}\\) to forecast \\(y^{(i)} _ {t+1:t+H}\\).</p> <p>In the section Time Series Forecasting Tasks, we will discuss more details of the forecasting problem.</p>"}, {"location": "time-series/#categories-of-forecasting-methods", "title": "Categories of Forecasting Methods", "text": "<p>Januschowsk et al proposed a framework to classify the different forecasting methods<sup>2</sup>. We illustrate the different methods in the following charts. For simplicity, we simply merge all the possible dimensions in one chart.</p> <pre><code>flowchart LR\nclassDef subjective fill:#EE8866;\nclassDef objective fill:#77AADD;\n\ndimensions[\"Dimensions of Forecasting Methods\"]\n\n%% Objective\n\nparams_shared[\"Parameter Shared Accross Series\"]:::objective\n\nparams_shared --\"True\"--&gt;Global:::objective\nparams_shared --\"False\"--&gt;Local:::objective\n\nuncertainty[\"Uncertainty in Forecasts\"]:::objective\nuncertainty --\"True\"--&gt; Probabilistic[\"Probabilistic Forecasts:\\n forecasts with predictive uncertainty\"]:::objective\nuncertainty --\"False\"--&gt; Point[\"Point Forecasts\"]:::objective\n\ncomputational_complexity[\"Computational Complexity\"]:::objective\nlinear_convexity[\"Linear and Convexity\"]:::objective\n\ndimensions --&gt; params_shared\ndimensions --&gt; uncertainty\ndimensions --&gt; computational_complexity\ndimensions --&gt; linear_convexity\n\n\n%% Subjective\n\nstructural_assumptions[\"Strong Structural Assumption\"]:::subjective --\"Yes\"--&gt; model_driven[\"Model-Driven\"]:::subjective\nstructural_assumptions --\"No\"--&gt; data_driven[\"Data-Driven\"]:::subjective\n\nmodel_comb[\"Model Combinations\"]:::subjective\n\ndiscriminative_generative[\"Discriminative or Generative\"]:::subjective\n\ntheoretical_guarantees[\"Theoretical Guarantees\"]:::subjective\n\npredictability_interpretability[\"Predictability and Interpretibility\"]:::subjective\n\ndimensions --&gt; structural_assumptions\ndimensions --&gt; model_comb\ndimensions --&gt; discriminative_generative\ndimensions --&gt; theoretical_guarantees\ndimensions --&gt; predictability_interpretability</code></pre> <p>We will mention those different dimensions later in our discussion of different forecasting models. For example, random forest is an ensemble method, which we will discuss in detail later.</p> <ol> <li> <p>Dorffner G. Neural networks for time series processing. Neural Network World 1996; 6: 447\u2013468.\u00a0\u21a9</p> </li> <li> <p>Januschowski T, Gasthaus J, Wang Y, Salinas D, Flunkert V, Bohlke-Schneider M et al. Criteria for classifying forecasting methods. International journal of forecasting 2020; 36: 167\u2013177.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-basics.ar/", "title": "AR", "text": "<p>Autoregressive (AR) models are simple models to model time series. A general AR(p) model is described by the following process:</p> \\[ s(t) = \\phi_0 + \\sum_{i=1}^p \\phi_i s(t-i) + \\epsilon. \\]"}, {"location": "time-series/timeseries-basics.ar/#ar1", "title": "AR(1)", "text": "<p>A first order AR model, aka AR(1), is as simple as</p> \\[ s(t) = \\phi_0 + \\phi_1 s(t-1) + \\epsilon. \\] <p>By staring at this equation, we can build up our intuitions.</p> \\(\\phi_0\\) \\(\\phi_1\\) \\(\\epsilon\\) Behavior - \\(0\\) - constant + noise \\(0\\) \\(1\\) - constant + noise \\(0\\) \\(\\phi_1&gt;1\\) or \\(0\\le\\phi_1 \\lt 1\\) - exponential + noise <p>Exponential Behavior doesn't Always Approach Positive Infinity</p> <p>For example, the combination \\(\\phi_0=0\\) and \\(\\phi_1&gt;1\\) without noise leads to exponential growth if the initial series value is positive. However, it approaches negative infinity if the initial series is negative.</p> Example: ConstantExample: DecayExample: ExponentialExample: LinearCode: Python <p></p> <p></p> <p></p> <p></p> <pre><code>import copy\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterator\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns; sns.set()\n\n\nclass GaussianEpsilon:\n    \"\"\"Gaussian noise\n\n    :param mu: mean value of the noise\n    :param std: standard deviation of the noise\n    \"\"\"\n\n    def __init__(self, mu, std, seed=None):\n        self.mu = mu\n        self.std = std\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __next__(self):\n        return self.rng.normal(self.mu, self.std)\n\n\nclass ZeroEpsilon:\n    \"\"\"Constant noise\n\n    :param epsilon: the constant value to be returned\n    \"\"\"\n\n    def __init__(self, epsilon=0):\n        self.epsilon = epsilon\n\n    def __next__(self):\n        return self.epsilon\n\n\n@dataclass(frozen=True)\nclass ARModelParams:\n    \"\"\"Parameters of our AR model,\n\n    $$s(t+1) = \\phi_0 + \\phi_1 s(t) + \\epsilon.$$\n\n    :param delta_t: step size of time in each iteration\n    :param phi0: pho_0 in the AR model\n    :param phi1: pho_1 in the AR model\n    :param epsilon: noise iterator, e.g., Gaussian noise\n    :param initial_state: a dictionary of the initial state, e.g., `{\"s\": 1}`\n    \"\"\"\n\n    delta_t: float\n    phi0: float\n    phi1: float\n    epsilon: Iterator\n    initial_state: Dict[str, float]\n\n\nclass AR1Stepper:\n    \"\"\"Stepper that calculates the next step in time in an AR model\n\n    :param model_params: parameters for the AR model\n    \"\"\"\n\n    def __init__(self, model_params):\n        self.model_params = model_params\n        self.current_state = copy.deepcopy(self.model_params.initial_state)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        phi0 = self.model_params.phi0\n        phi1 = self.model_params.phi1\n        epsilon = next(self.model_params.epsilon)\n\n        next_s = (\n            self.model_params.phi0\n            + self.model_params.phi1 * self.current_state[\"s\"]\n            + epsilon\n        )\n        self.current_state = {\"s\": next_s}\n\n        return copy.deepcopy(self.current_state)\n\n\ndef visualize_vr1(delta_t, phi0, phi1, length=200, savefig=False):\n    mu = 0\n    std = 0.1\n    geps = GaussianEpsilon(mu=mu, std=std)\n    zeps = ZeroEpsilon()\n\n    initial_state = {\"s\": -1}\n\n    ar1_params = ARModelParams(\n        delta_t=delta_t, phi0=phi0, phi1=phi1, epsilon=geps, initial_state=initial_state\n    )\n    ar1_params_zero_noise = ARModelParams(\n        delta_t=delta_t, phi0=phi0, phi1=phi1, epsilon=zeps, initial_state=initial_state\n    )\n\n    ar1_stepper = AR1Stepper(model_params=ar1_params)\n    ar1_stepper_no_noise = AR1Stepper(model_params=ar1_params_zero_noise)\n\n    history = []\n    history_zero_noise = []\n    for l in range(length):\n        history.append(next(ar1_stepper))\n        history_zero_noise.append(next(ar1_stepper_no_noise))\n\n    df = pd.DataFrame(history)\n    df_zero_noise = pd.DataFrame(history_zero_noise)\n\n    fig, ax = plt.subplots(figsize=(10, 6.18))\n    sns.lineplot(\n        x=np.linspace(0, length - 1, length) * delta_t,\n        y=df.s,\n        ax=ax,\n        marker=\".\",\n        label=\"AR1\",\n        color=\"r\",\n        alpha=0.9,\n    )\n    sns.lineplot(\n        x=np.linspace(0, length - 1, length) * delta_t,\n        y=df_zero_noise.s,\n        ax=ax,\n        marker=\".\",\n        label=\"AR1 (wihout Noise)\",\n        color=\"g\",\n        alpha=0.5,\n    )\n\n    ax.set_title(\n        f\"AR(1) Example ($\\phi_0={phi0}$, $\\phi_1={phi1}$; $\\epsilon$: $\\mu={mu}$, $\\sigma={std}$; $s(0)={initial_state['s']}$)\"\n    )\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Values\")\n\n    if savefig:\n        plt.savefig(\n            f\"/work/timeseries-dgp-ar-var/exports/ar1-phi0-{phi0}-phi1-{phi1}-std-{std}-init-{initial_state['s']}.png\"\n        )\n</code></pre> <p>Call the function <code>visualize_vr1</code> to make some plots.</p> <pre><code>visualize_vr1(delta_t = 0.01, phi0 = 0, phi1 = 1.1, length = 200, savefig=True)\n</code></pre> <ol> <li> <p>Kumar A. Autoregressive (AR) models with Python examples. In: Data Analytics [Internet]. 25 Apr 2022 [cited 11 Aug 2022]. Available: https://vitalflux.com/autoregressive-ar-models-with-python-examples/ \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-basics.statistical-models/", "title": "Statistical Models of Time Series", "text": "<p>Though statistical models are not our focus, it is always beneficial to understand how those famous statistical models work. To best understand how the models work, we will build some data generating process using these models and explore their behavior.</p> <p>In the following paragraphs, we list some of the most applied statistical models. For a comprehensive review of statistical models, please refer to Petropoulos et al., 2022 and Hyndman et al., 2021<sup>3</sup><sup>4</sup>.</p>"}, {"location": "time-series/timeseries-basics.statistical-models/#arima", "title": "ARIMA", "text": "<p>ARIMA is one of the most famous forecasting models<sup>1</sup>. We will not discuss the details of the model. However, for reference, we sketch the relations between the different components of the ARIMA model in the following chart.</p> <pre><code>flowchart TD\n\nAR --\"interdependencies\"--&gt; VAR\nMA --\"add autoregressive\"--&gt; ARMA\nAR --\"add moving average\"--&gt; ARMA\n\nARMA --\"difference between values\"--&gt; ARIMA\nARMA --\"interdependencies\"--&gt; VARMA\nVAR --\"moving average\"--&gt; VARMA\n\nARIMA --\"interdependencies\"--&gt; VARIMA\nVAR --\"difference and moving average\"--&gt; VARIMA\nVARMA --\"difference\"--&gt; VARIMA</code></pre>"}, {"location": "time-series/timeseries-basics.statistical-models/#exponential-smoothing", "title": "Exponential Smoothing", "text": "<p>A Naive Forecast</p> <p>In time series forecasting, one of the naive forecasts we can use it the previous observation, i.e.,</p> \\[ \\hat s(t+1) = s(t), \\] <p>where we use \\(\\hat s\\) to denote the forecasts and \\(s\\) for the observations.</p> <p>A naive version of the exponential smoothing model is the Simple Exponential Smoothing (SES)<sup>3</sup><sup>4</sup>. The SES is an average of the most recent observation and the previous forecasts.</p> \\[ \\hat s(t+1) = \\alpha s(t) + (1-\\alpha) \\hat s(t), \\] <p>where \\(\\hat s\\) is the forecast and \\(s\\) is the observation. Expanding this form, we observe the exponential decaying effect of history in the long past<sup>4</sup>.</p>"}, {"location": "time-series/timeseries-basics.statistical-models/#state-space-models", "title": "State Space Models", "text": "<p>State space models (SSM) are amazing models due to their simplicity. SSM applies Markov chains but is not limited to the Markovian assumptions<sup>5</sup>.</p> <ol> <li> <p>Cerqueira V, Torgo L, Soares C. Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1909.13316 \u21a9</p> </li> <li> <p>Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.11650 \u21a9</p> </li> <li> <p>Petropoulos F, Apiletti D, Assimakopoulos V, Babai MZ, Barrow DK, Ben Taieb S, et al. Forecasting: theory and practice. Int J Forecast. 2022;38: 705\u2013871. doi:10.1016/j.ijforecast.2021.11.001\u00a0\u21a9\u21a9</p> </li> <li> <p>Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2022-11-27.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Bishop CM. Pattern Recognition and Machine Learning. Springer; 2006. Available: https://play.google.com/store/books/details?id=qWPwnQEACAAJ \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-basics.var/", "title": "VAR", "text": ""}, {"location": "time-series/timeseries-basics.var/#var1", "title": "VAR(1)", "text": "<p>VAR(1) is similar to AR(1) but models time series with interactions between the series. For example, a two-dimensional VAR(1) model is</p> \\[ \\begin{pmatrix}s^{(1)}(t+1) \\\\ s^{(2)}(t+1) \\end{pmatrix} = \\begin{pmatrix} \\phi^{(1)}_0 \\\\ \\phi^{(2)}_0 \\end{pmatrix} +  \\begin{pmatrix}\\phi_{1, 11} &amp; \\phi_{1, 12}\\\\ \\phi_{1, 21} &amp; \\phi_{1, 22} \\end{pmatrix} \\begin{pmatrix}s^{(1)}(t) \\\\ s^{(2)}(t) \\end{pmatrix} + \\begin{pmatrix}\\epsilon^{(1)} \\\\ \\epsilon^{(2)} \\end{pmatrix}. \\] <p>A more compact form is</p> \\[ \\mathbf s (t+1) = \\boldsymbol \\phi_0 + \\boldsymbol \\phi_1 \\mathbf s(t) + \\boldsymbol \\epsilon. \\] <p>Stability of VAR</p> <p>For VAR(1), our series blows up when the max eigenvalue of the matrix \\(\\boldsymbol \\phi_1\\) is large than 1<sup>1</sup>. Otherwise, we get stable series.</p> <p>In the following examples, we denote the largest eigenvalue of \\(\\boldsymbol \\phi_1\\) as \\(\\lambda_0\\).</p> VAR(1) StableVAR(1) UnstableVAR(1) without NoiseVAR(1) with Zero Mean NoiseVAR(1) with Nonzero Mean NoisePython Code <p></p> <p>The figure is created using the code from the \"Python Code\" tab, and the following parameters.</p> <pre><code>var_params_stable = VAR1ModelParams(\n    delta_t = 0.01,\n    phi0 = np.array([0.1, 0.1]),\n    phi1 = np.array([\n        [0.5, -0.25],\n        [-0.35, 0.45+0.2]\n    ]),\n    epsilon = ConstantEpsilon(epsilon=np.array([0,0])),\n    initial_state = np.array([1, 0])\n)\n\nvar1_visualize(var_params=var_params_stable)\n</code></pre> <p></p> <p>The figure is created using the code from the \"Python Code\" tab, and the following parameters.</p> <pre><code>var_params_unstable = VAR1ModelParams(\n    delta_t = 0.01,\n    phi0 = np.array([0.1, 0.1]),\n    phi1 = np.array([\n        [0.5, -0.25],\n        [-0.35, 0.45+0.5]\n    ]),\n    epsilon = ConstantEpsilon(epsilon=np.array([0,0])),\n    initial_state = np.array([1, 0])\n)\n\nvar1_visualize(var_params=var_params_unstable)\n</code></pre> <p></p> <p>The figure is created using the code from the \"Python Code\" tab, and the following parameters.</p> <pre><code>var_params_no_noise = VAR1ModelParams(\n    delta_t = 0.01,\n    phi0 = np.array([-1, 1]),\n    phi1 = np.array([\n        [0.7, 0.2],\n        [0.2, 0.7]\n    ]),\n    epsilon = ConstantEpsilon(epsilon=np.array([0,0])),\n    initial_state = np.array([1, 0])\n)\n\nvar1_visualize(var_params=var_params_no_noise)\n</code></pre> <p></p> <p>The figure is created using the code from the \"Python Code\" tab, and the following parameters.</p> <pre><code>var_params_zero_mean_noise = VAR1ModelParams(\n    delta_t = 0.01,\n    phi0 = np.array([-1, 1]),\n    phi1 = np.array([\n        [0.7, 0.2],\n        [0.2, 0.7]\n    ]),\n    epsilon = MultiGaussianNoise(mu=np.array([0, 0]), cov=np.array([[1, 0.5],[0.5, 1]])),\n    initial_state = np.array([1, 0])\n)\n\nvar1_visualize(var_params=var_params_zero_mean_noise)\n</code></pre> <p></p> <p>The figure is created using the code from the \"Python Code\" tab, and the following parameters.</p> <pre><code>var_params_nonzero_mean_noise = VAR1ModelParams(\n    delta_t = 0.01,\n    phi0 = np.array([-1, 1]),\n    phi1 = np.array([\n        [0.7, 0.2],\n        [0.2, 0.7]\n    ]),\n    epsilon = MultiGaussianNoise(mu=np.array([1, 2]), cov=np.array([[1, 0.5],[0.5, 1]])),\n    initial_state = np.array([1, 0])\n)\n\nvar1_visualize(var_params=var_params_nonzero_mean_noise)\n</code></pre> <pre><code>import copy\nfrom dataclasses import dataclass\nfrom typing import Iterator\nimport numpy as np\n\nclass MultiGaussianNoise:\n\"\"\"A multivariate Gaussian noise\n\n:param mu: means of the variables\n:param cov: covariance of the variables\n:param seed: seed of the random number generator for reproducibility\n\"\"\"\n\ndef __init__(self, mu: np.ndarray, cov: np.ndarray, seed: Optional[float] = None):\n    self.mu = mu\n    self.cov = cov\n    self.rng = np.random.default_rng(seed=seed)\n\ndef __next__(self) -&gt; np.ndarray:\n    return self.rng.multivariate_normal(self.mu, self.cov)\n\nclass ConstantEpsilon:\n\"\"\"Constant noise\n\n:param epsilon: the constant value to be returned\n\"\"\"\ndef __init__(self, epsilon=0):\n    self.epsilon = epsilon\n\ndef __next__(self):\n    return self.epsilon\n\n\n@dataclass(frozen=True)\nclass VAR1ModelParams:\n    \"\"\"Parameters of our VAR model,\n\n    :param delta_t: step size of time in each iteration\n    :param phi0: pho_0 in the AR model\n    :param phi1: pho_1 in the AR model\n    :param epsilon: noise iterator, e.g., Gaussian noise\n    :param initial_state: a dictionary of the initial state, e.g., `{\"s\": 1}`\n    \"\"\"\n\n    delta_t: float\n    phi0: np.ndarray\n    phi1: np.ndarray\n    epsilon: Iterator\n    initial_state: np.ndarray\n\n\nclass VAR1Stepper:\n    \"\"\"Calculate the next values using VAR(1) model.\n\n    :param model_params: the parameters of the VAR(1) model, e.g.,\n        [`VAR1ModelParams`][eerily.data.generators.var.VAR1ModelParams]\n    \"\"\"\n\n    def __init__(self, model_params):\n        self.model_params = model_params\n        self.current_state = copy.deepcopy(self.model_params.initial_state)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n\n        epsilon = next(self.model_params.epsilon)\n        phi0 = self.model_params.phi0\n        phi1 = self.model_params.phi1\n\n        self.current_state = phi0 + np.matmul(phi1, self.current_state) + epsilon\n\n        return copy.deepcopy(self.current_state)\n\n\nclass Factory:\n    \"\"\"A generator that creates the data points based on the stepper.\"\"\"\n    def __init__(self):\n        pass\n\n    def __call__(self, stepper, length):\n        i = 0\n        while i &lt; length:\n            yield next(stepper)\n            i += 1\n</code></pre> <p>We create a function to visualize the series.</p> <pre><code>def var1_visualize(var_params):\n    phi1_eig_max = max(np.linalg.eig(var_params.phi1)[0])\n\n    var1_stepper = VAR1Stepper(model_params=var_params)\n\n    length = 200\n    fact = Factory()\n    history = list(fact(var1_stepper, length=length))\n\n    df = pd.DataFrame(history, columns=[\"s1\", \"s2\"])\n    print(df.head())\n\n    fig, ax = plt.subplots(figsize=(10, 6.18))\n\n    sns.lineplot(\n        x=np.linspace(0, length-1, length) * var_params.delta_t,\n        y=df.s1,\n        ax=ax,\n        marker=\"o\",\n    )\n\n    sns.lineplot(\n        x=np.linspace(0, length-1, length) * var_params.delta_t,\n        y=df.s2,\n        ax=ax,\n        marker=\"o\",\n    )\n\n    ax.set_title(f\"VAR(1) Example ($\\lambda_0={phi1_eig_max:0.2f}$)\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Values\")\n</code></pre> <ol> <li> <p>Zivot E, Wang J. Modeling Financial Time Series with S-PLUS\u00ae. Springer New York; 2006. doi:10.1007/978-0-387-32348-0 \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-data.analysis/", "title": "Time Series Analysis", "text": "<p>Time series analysis is not our focus here. However, it is beneficial to grasp some basic ideas of time series.</p>"}, {"location": "time-series/timeseries-data.analysis/#stationarity", "title": "Stationarity", "text": "<p>Time series data is stationary if the distribution of the observables do not change<sup>1</sup><sup>2</sup><sup>6</sup>.</p> <p>A strict stationary series guarantees the same distribution for a segment \\(\\{x_{i+1}, \\cdots, x_{x+k}\\}\\) and a time-shifted segment \\(x_{i+1+\\Delta}, \\cdots, x_{x+k+\\Delta}\\}\\) for integer \\(\\Delta\\)<sup>1</sup>.</p> <p>A less strict form (WSS) concerns only the mean and autocorrelation<sup>1</sup><sup>3</sup>, i.e.,</p> \\[ \\begin{align} \\mathbb E[x_{i+1}] &amp;= \\mathbb E[x_{i+\\Delta}] \\\\ \\mathbb{Cov}[x_{i+1}, x_{i+k}] &amp;= \\mathbb{Cov}[x_{i+1+\\Delta}, x_{x+k+\\Delta}] \\end{align} \\] <p>In deep learning, a lot of models require the training data to be I.I.D.<sup>4</sup><sup>7</sup>. The I.I.D. requirement in time series is stationarity.</p> <p>A stationary time series is clean and pure. However, real-world data is not necessarily stationary, e.g., macroeconomic series data are non-stationary<sup>6</sup>.</p>"}, {"location": "time-series/timeseries-data.analysis/#serial-dependence", "title": "Serial Dependence", "text": "<p>Autocorrelation measures the serial dependency of a time series<sup>5</sup>. By definition, the autocorrelation is the autocovariance normalized by the variance,</p> \\[ \\rho = \\frac{\\mathbb{Cov}[x_t, x_{t+\\delta}]}{\\mathbb{Var}[x_t]}. \\] <p>One naive expectation is that the autocorrelation diminishes if \\(\\delta \\to \\infty\\)<sup>3</sup>.</p>"}, {"location": "time-series/timeseries-data.analysis/#terminology", "title": "Terminology", "text": "<p>Terminologies for time series data may be different in different fields<sup>8</sup>. For example, we may encounter the term \"panel data\" in econometrics, which is the same as \"multivariate time series\" in \"data science\".</p> <p>Panel Data</p> <p>Panel data is multivariate time series data,</p> \\[ \\mathbf y_t \\to y_{it}. \\] time variable \\(y_1\\) variable \\(y_2\\) variable \\(y_3\\) \\(t_1\\) \\(y_{11}\\) \\(y_{21}\\) \\(y_{31}\\) \\(t_2\\) \\(y_{12}\\) \\(y_{22}\\) \\(y_{32}\\) \\(t_3\\) \\(y_{13}\\) \\(y_{23}\\) \\(y_{33}\\) \\(t_4\\) \\(y_{14}\\) \\(y_{24}\\) \\(y_{34}\\) \\(t_5\\) \\(y_{15}\\) \\(y_{25}\\) \\(y_{35}\\) <ol> <li>doi:10.1162/99608f92.dec7d780</li> </ol> <ol> <li> <p>Contributors to Wikimedia projects. Stationary process. In: Wikipedia [Internet]. 18 Sep 2022 [cited 13 Nov 2022]. Available: https://en.wikipedia.org/wiki/Stationary_process \u21a9\u21a9\u21a9</p> </li> <li> <p>6.4.4.2. Stationarity. In: Engineering Statistics Handbook [Internet]. NIST; [cited 13 Nov 2022]. Available: https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm#:~:text=Stationarity%20can%20be%20defined%20in,no%20periodic%20fluctuations%20(seasonality).\u00a0\u21a9</p> </li> <li> <p>Shalizi C. 36-402, Undergraduate Advanced Data Analysis (2012). In: Undergraduate Advanced Data Analysis [Internet]. 2012 [cited 13 Nov 2022]. Available: https://www.stat.cmu.edu/~cshalizi/uADA/12/ \u21a9\u21a9</p> </li> <li> <p>Sch\u00f6lkopf B, Locatello F, Bauer S, Ke NR, Kalchbrenner N, Goyal A, et al. Toward Causal Representation Learning. Proc IEEE. 2021;109: 612\u2013634. doi:10.1109/JPROC.2021.3058954\u00a0\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Autocorrelation. In: Wikipedia [Internet]. 10 Nov 2022 [cited 13 Nov 2022]. Available: https://en.wikipedia.org/wiki/Autocorrelation \u21a9</p> </li> <li> <p>Das P. Econometrics in Theory and Practice. Springer Nature Singapore; doi:10.1007/978-981-32-9019-8 \u21a9\u21a9</p> </li> <li> <p>Dawid P, Tewari A. On learnability under general stochastic processes. Harvard Data Science Review. 2022;\u00a0\u21a9</p> </li> <li> <p>Hyndman R. Rob J Hyndman - Terminology matters. In: Rob J Hyndman [Internet]. 26 Jun 2020 [cited 9 Nov 2023]. Available: https://robjhyndman.com/hyndsight/terminology-matters/#same-concept-different-terminology \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-data.analysis.twfe/", "title": "Two-Way Fixed Effects", "text": "<p>Two-way fixed effects on [panel data is a handy method for establishing linear models from time series data. To keep our notations consistent, we will use the term multivariate time series to refer to panel data in the following content.</p>"}, {"location": "time-series/timeseries-data.analysis.twfe/#two-way-fixed-effects-model", "title": "Two-way Fixed Effects Model", "text": "<p>A two-way fixed effects model is a linear model that allows the parameters to vary across both time and the variables<sup>1</sup>,</p> \\[ y_{it} = \\beta X_{it} + \\alpha_i + \\gamma_t + \\epsilon_{it}, \\] <p>where \\(\\alpha_i\\) and \\(\\gamma_t\\) represent the effect coming from the variable and time, respectively.</p>"}, {"location": "time-series/timeseries-data.analysis.twfe/#example", "title": "Example", "text": "<p>To help readers outside of econometrics or causal inference get started with this model, we will use a simple example to illustrate the idea. We will construct a naive dataset with three groups and two variables linearly related to each other.</p>  Data and Model Results Required Packages Code <p>We construct a naive dataset that contains three articles (column <code>name</code>), each having a different distribution of prices and demand, while all of them are generated with the same linear relation between the variable <code>log_demand</code> and <code>log_price</code>. The data points also fluctuate in time (column <code>step</code>).</p> <p>Using a simple linear model with both time (<code>step</code>) and variable (<code>name</code>) fixed effects, we obtain the following results.</p> <pre><code>Estimation:  OLS\nDep. var.: log_demand, Fixed effects: name+step\nInference:  CRV1\nObservations:  1450\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5 % |   97.5 % |\n|:--------------|-----------:|-------------:|----------:|-----------:|--------:|---------:|\n| log_price     |     -2.972 |        0.004 |  -680.195 |      0.000 |  -2.991 |   -2.953 |\n---\nRMSE: 0.003  Adj. R2: 1.0  Adj. R2 Within: 1.0\n</code></pre> <pre><code>pyfixest==0.10.10.0\nseaborn==0.13.0\neerily==0.2.1\n</code></pre> <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport random\n\nfrom pyfixest.estimation import feols\nimport seaborn as sns; sns.set()\n\nimport matplotlib.pyplot as plt\n\nfrom eerily.generators.elasticity import ElasticityStepper, LinearElasticityParams\nfrom eerily.generators.naive import (\n    ConstantStepper,\n    ConstStepperParams,\n    SequenceStepper,\n    SequenceStepperParams,\n)\n\nfrom eerily.generators.utils.choices import Choices\n\n# %% [markdown]\n# ## Generate Data\n\n# %%\ndef create_one_article(\n    elasticity_value, length, article_id, initial_condition,\n    log_prices, first_step=0\n):\n\n    es = ElasticityStepper(\n        model_params=LinearElasticityParams(\n            initial_state=initial_condition,\n            log_prices=iter(log_prices),\n            elasticity=iter([elasticity_value + (random.random() - 0.5)/10] * length),\n            variable_names=[\"log_demand\", \"log_price\", \"elasticity\"],\n        ),\n        length=length\n    )\n\n    ss = SequenceStepper(\n        model_params=SequenceStepperParams(\n            initial_state=[first_step], variable_names=[\"step\"], step_sizes=[1]\n        ),\n        length=length\n    )\n    cs = ConstantStepper(\n        model_params=ConstStepperParams(initial_state=[article_id], variable_names=[\"name\"]),\n        length=length\n    )\n\n    return (es &amp; ss &amp; cs)\n\n\ninitial_condition = {\"log_demand\": 3, \"log_price\": 1, \"elasticity\": None}\n\nlength_1 = 200\nlength_2 = 400\nlength_3 = 850\n\nlog_price_choices_1 = Choices(elements=[1,1.1, 1.2, 1.3, 1.4, 1.5])\nlog_price_choices_2 = Choices(elements=[1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])\nlog_price_choices_3 = Choices(elements=[2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8])\n\nlog_prices_1 = [next(log_price_choices_1) for i in range(length_1)]\nlog_prices_2 = [next(log_price_choices_2) for i in range(length_2)]\nlog_prices_3 = [next(log_price_choices_3) for i in range(length_3)]\n\n\ndata_gen = (\n    create_one_article(elasticity_value=-3, length=length_1, article_id=\"article_1\", initial_condition=initial_condition, log_prices=log_prices_1)\n    + create_one_article(elasticity_value=-3, length=length_2, article_id=\"article_2\", initial_condition=initial_condition, log_prices=log_prices_2)\n    + create_one_article(elasticity_value=-3, length=length_3, article_id=\"article_3\", initial_condition=initial_condition, log_prices=log_prices_3)\n)\n\n# %%\ndf = pd.DataFrame(list(data_gen))\n\n\n# %% [markdown]\n# ## Visualizations\n\n# %%\nfig, ax = plt.subplots(figsize=(10, 6.18))\nsns.scatterplot(\n    df,\n    x=\"log_price\",\n    y=\"log_demand\",\n    hue=\"step\",\n    style=\"name\"\n)\n\n# %% [markdown]\n# ## Estimation\n\n# %%\nfit_feols = feols(\n    fml=\"log_demand ~ log_price | name + step\",\n    data=df\n)\n\n# %%\nfit_feols.summary()\n</code></pre> <p>Tools and Further Reading</p> <p>In the R world, <code>fixest</code> is a popular package for estimating two-way fixed effects models. In the Python world, we have something similar called pyfixest.</p> <ol> <li> <p>Imai K, Kim IS. On the use of two-way fixed effects regression models for causal inference with panel data. Political analysis: an annual publication of the Methodology Section of the American Political Science Association 2021; 29: 405\u2013415.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-data.box-cox/", "title": "Box-Cox Transformation", "text": "<p>Many time series models require stationary data. However, real-world time series data may be non-stationary and heteroscedastic<sup>1</sup>. Box-cox transformation is useful when reducing the non-stationarity and heteroscedasticity.</p> <p>Rob J Hyndman and George Athanasopoulos's famous textbook FPP2 provides some nice examples of box-cox transformations.</p> <p>To see Box-Cox transformation in action, we show an example using the air passenger dataset.</p> <p>The air passenger dataset is a monthly dataset. We can observe the trend and varying variance simply by eyes.</p> <p></p> <p>Applying Box-Cox transformations with different lambdas leads to different results shown below.</p> <p></p> <p>To check the variance, we plot out the variance rolling on a 12-month window.</p> <p></p> <p>Box-Cox transformation with \\(\\lambda =0.1\\) reduces the variability in variance.</p> <p></p> <p>Box-Cox May not Always Reach Perfect Stationary Data</p> <p>Box-Cox transformation is a simple transformation that helps us reduce the non-stationarity and heteroscedasticy. However, we may not always be able to convert the dataset to stationary and homoscedastic data. This can be observed by performing checks using tools such as <code>stationarity_tests</code> in Darts.</p> <ol> <li> <p>Homoscedasticity and heteroscedasticity. (2023, June 2). In Wikipedia. https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-data.data-augmentation/", "title": "Data Augmentation for Time Series", "text": "<p>In deep learning, our dataset should help the optimization mechanism locate a good spot in the parameter space. However, real-world data is not necessarily diverse enough that covers the required situations with enough records. For example, some datasets may be extremely imbalanced class labels which leads to poor performance in classification tasks <sup>3</sup>. Another problem with a limited dataset is that the trained model may not generalize well <sup>4</sup><sup>5</sup>.</p> <p>We will cover two topics in this section: Augmenting the dataset and application of the augmented data to model training.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#augmenting-the-dataset", "title": "Augmenting the Dataset", "text": "<p>There are many different ways of augmenting time series data <sup>4</sup><sup>6</sup>. We categorize the methods into the following groups:</p> <ul> <li>Random transformations, e.g., jittering;</li> <li>Pattern mixing, e.g., DBA;<sup>7</sup></li> <li>Generative models, e.g.,<ul> <li>phenomenological generative models such as AR <sup>8</sup>,</li> <li>first principle models such as economical models <sup>9</sup>,</li> <li>deep generative models such as TimeGAN or TS GAN <sup>10</sup><sup>11</sup>.</li> </ul> </li> </ul> <p>We also treat the first two methods, random transformations and pattern mixing as basic methods.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#basic-methods", "title": "Basic Methods", "text": "<p>In the following table, we group some of the data augmentation methods by two dimensions, the category of the method, and the domain of where the method is applied.</p> Projected Domain Time Scale Magnitude Random Transformation Frequency Masking, Frequency Warping, Fourier Transform, STFT Permutation, Slicing, Time Warping, Time Masking, Cropping Jittering, Flipping, Scaling, Magnitude Warping Pattern Mixing EMDA<sup>12</sup>, SFM<sup>13</sup> Guided Warping<sup>14</sup> DFM<sup>9</sup>, Interpolation, DBA<sup>7</sup> <p>For completeness, we will explain some of the methods in more detail in the following.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#perturbation-in-fourier-domain", "title": "Perturbation in Fourier Domain", "text": "<p>In the Fourier domain, for each the amplitude \\(A_f\\) and phase \\(\\phi_f\\) at a specific frequency, we can perform<sup>15</sup></p> <ul> <li>magnitude replacement using a Gaussian distribution, and</li> <li>phase shift by adding Gaussian noise.</li> </ul> <p>We perform such perturbations at some chosen frequency.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#slicing-permutation-and-bootstrapping", "title": "Slicing, Permutation, and Bootstrapping", "text": "<p>We can slice a series into small segments. With the slices, we can perform different operations to create new series.</p> <ul> <li>Window Slicing (WS): In a classification task, we can take the slices from the original series and assign the same class label to the slice <sup>16</sup>. The slices can also be interpolated to match the length of the original series <sup>4</sup>.</li> <li>Permutation: We take the slices and permute them to form a new series <sup>17</sup>.</li> <li>Moving Block Bootstrapping (MBB): First, we remove the trend and seasonability. Then we draw blocks of fixed length from the residual of the series until the desired length of the series is met. Finally, we combine the newly formed residual with trend and seasonality to form a new series <sup>18</sup>.</li> </ul>"}, {"location": "time-series/timeseries-data.data-augmentation/#warping", "title": "Warping", "text": "<p>Both the time scale and magnitude can be warped. For example,</p> <ul> <li>Time Warping: We distort time intervals by taking a range of data points and upsample or downsample it <sup>6</sup>.</li> <li>Magnitude Warping: the magnitude of the time series is rescaled.</li> </ul> Dynamic Time Warping (DTW) <p>Given two sequences, \\(S^{(1)}\\) and \\(S^{(2)}\\), the Dynamic Time Warping (DTW) algorithm finds the best way to align two sequences. During this alignment process, we quantify the misalignment using a distance similar to the Levenshtein distance, where the distance between two series \\(S^{(1)}_{1:i}\\) (with \\(i\\) elements) and \\(S^{(2)}_{1:j}\\) (with \\(j\\) elements) is<sup>7</sup></p> \\[ \\begin{align} D(S^{(1)}_{1:i}, S^{(2)}_{1:j}) =&amp; d(S^{(1)}_i, S^{(2)}_j)\\\\ &amp; + \\operatorname{min}\\left[ D(S^{(1)}_{1:i-1}, S^{(2)}_{1:j-1}), D(S^{(1)}_{1:i}, S^{(2)}_{1:j-1}), D(S^{(1)}_{1:i-1}, S^{(2)}_{1:j}) \\right], \\end{align} \\] <p>where \\(S^{(1)}_i\\) is the \\(i\\)the element of the series \\(S^{(1)}\\), \\(d(x,y)\\) is a predetermined distance, e.g., Euclidean distance. This definition reveals the recursive nature of the DTW distance.</p> Notations in the Definition: \\(S_{1:i}\\) and \\(S_{i}\\) <p>The notation \\(S_{1:i}\\) stands for a series that contains the elements starting from the first to the \\(i\\)th in series \\(S\\). For example, we have a series</p> \\[ S^1 = [s^1_1, s^1_2, s^1_3, s^1_4, s^1_5, s^1_6]. \\] <p>The notation \\(S^1_{1:4}\\) represents</p> \\[ S^1_{1:4} = [s^1_1, s^1_2, s^1_3, s^1_4]. \\] <p>The notation \\(S_i\\) indicates the \\(i\\)th element in \\(S\\). For example,</p> \\[ S^1_4 = s^1_4. \\] <p>If we map these two notations to Python,</p> <ul> <li>\\(S_{1:i}\\) is equivalent to <code>S[0:i]</code>, and</li> <li>\\(S_i\\) is equivalent to <code>S[i-1]</code>.</li> </ul> <p>Note that the indices in Python look strange. This is also the reason we choose to use subscripts not square brackets in our definition.</p> Levenshtein Distance <p>Given two words, e.g., \\(w^{a} = \\mathrm{cats}\\) and \\(w^{b} = \\mathrm{katz}\\). Suppose we can only use three operations: insertions, deletions and substitutions. The Levenshtein distance calculates the number of such operations needed to change from the first word \\(w^a\\) to the second one \\(w^b\\) by applying single-character edits. In this example, we need two replacements, i.e., <code>\"c\" -&gt; \"k\"</code> and <code>\"s\" -&gt; \"z\"</code>.</p> <p>The Levenshtein distance can be solved using recursive algorithms <sup>1</sup>.</p> <p>DTW is very useful when comparing series with different lengths. For example, most error metrics require the actual time series and predicted series to have the same length. In the case of different lengths, we can perform DTW when calculating these metrics<sup>2</sup>.</p> <p>The forecasting package darts provides a demo of DTW.</p> DTW Barycenter Averaging <p>DTW Barycenter Averaging (DBA) constructs a series \\(\\bar{\\mathcal S}\\) out of a set of series \\(\\{\\mathcal S^{(\\alpha)}\\}\\) so that \\(\\bar{\\mathcal S}\\) is the barycenter of \\(\\{\\mathcal S^{(\\alpha)}\\}\\) measured by Dynamic Time Warping (DTW) distance <sup>7</sup>.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#barycenter-averaging-based-on-dtw-distance", "title": "Barycenter Averaging Based on DTW Distance", "text": "<p>Petitjean et al proposed a time series averaging algorithm based on DTW distance which is dubbed DTW Barycenter Averaging (DBA).</p> <p>DBA Implementation</p> <p>https://github.com/fpetitjean/DBA</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#series-mixing", "title": "Series Mixing", "text": "<p>Another class of data augmentation methods is mixing the series. For example, we take two randomly drawn series and average them using DTW Barycenter Averaging (DBA) <sup>7</sup>. (DTW, dynamic time warping, is an algorithm to calculate the distance between sequential datasets by matching the data points on each of the series <sup>7</sup><sup>19</sup>.) To augment a dataset, we can choose from a list of strategies <sup>20</sup><sup>21</sup>:</p> <ul> <li>Average All series using different sets of weights to create new synthetic series.</li> <li>Average Selected series based on some strategies. For example, Forestier et al proposed choosing an initial series and combining it with its nearest neighbors <sup>21</sup>.</li> <li>Average Selected with Distance is Average Selected but neighbors that are far from the initial series are down-weighted <sup>21</sup>.</li> </ul> <p>Some other similar methods are</p> <ul> <li>Equalized Mixture Data Augmentation (EMDA) calculates the weighted average of spectrograms of the same class label<sup>12</sup>.</li> <li>Stochastic Feature Mapping (SFM) is a data augmentation method in audio data<sup>13</sup>.</li> </ul>"}, {"location": "time-series/timeseries-data.data-augmentation/#data-generating-process", "title": "Data Generating Process", "text": "<p>Time series data can also be augmented using some assumed data generating process (DGP). Some methods, such as GRATIS <sup>8</sup>, utilize simple generic methods such as AR/MAR. Some other methods, such as Gaussian Trees <sup>22</sup>, utilize more complicated hidden structures using graphs, which can approximate more complicated data generating processes. These methods do not necessarily reflect the actual data generating process but the data is generated using some parsimonious phenomenological models. Some other methods are more tuned toward detailed mechanisms. There are also methods using generative deep neural networks such as GAN.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#dynamic-factor-model-dfm", "title": "Dynamic Factor Model (DFM)", "text": "<p>For example, we have a series \\(X(t)\\) which depends on a latent variable \\(f(t)\\)<sup>9</sup>,</p> \\[ X(t) = \\mathbf A f(t) + \\eta(t), \\] <p>where \\(f(t)\\) is determined by a differential equation</p> \\[ \\frac{f(t)}{dt} = \\mathbf B f(t) + \\xi(t). \\] <p>In the above equations, \\(\\eta(t)\\) and \\(\\xi(t)\\) are the irreducible noise.</p> <p>The above two equations can be combined into one first-order differential equation.</p> <p>Once the model is fit, it can be used to generate new data points. However, we will have to understand whether the data is generated in such processes.</p>"}, {"location": "time-series/timeseries-data.data-augmentation/#applying-the-synthetic-data-to-model-training", "title": "Applying the Synthetic Data to Model Training", "text": "<p>Once we prepared the synthetic dataset, there are two strategies to include them in our model training <sup>20</sup>.</p> Strategy Description Pooled Strategy Synthetic data + original data -&gt; model Transfer Strategy Synthetic data -&gt; pre-trained model; pre-trained model + original data -&gt; model <p>The pooled strategy takes the synthetic data and original data then feeds them together into the training pipeline. The transfer strategy uses the synthetic data to pre-train the model, then uses transfer learning methods (e.g., freeze weights of some layers) to train the model on the original data.</p> <ol> <li> <p>trekhleb. javascript-algorithms/src/algorithms/string/levenshtein-distance at master \u00b7 trekhleb/javascript-algorithms. In: GitHub [Internet]. [cited 27 Jul 2022]. Available: https://github.com/trekhleb/javascript-algorithms/tree/master/src/algorithms/string/levenshtein-distance \u21a9</p> </li> <li> <p>Unit8. Metrics \u2014 darts\u00a0 documentation. In: Darts [Internet]. [cited 7 Mar 2023]. Available: https://unit8co.github.io/darts/generated_api/darts.metrics.metrics.html?highlight=dtw#darts.metrics.metrics.dtw_metric \u21a9</p> </li> <li> <p>Hasibi R, Shokri M, Dehghan M. Augmentation scheme for dealing with imbalanced network traffic classification using deep learning. 2019.http://arxiv.org/abs/1901.00204.\u00a0\u21a9</p> </li> <li> <p>Iwana BK, Uchida S. An empirical survey of data augmentation for time series classification with neural networks. 2020.http://arxiv.org/abs/2007.15951.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. Journal of Big Data 2019; 6: 1\u201348.\u00a0\u21a9</p> </li> <li> <p>Wen Q, Sun L, Yang F, Song X, Gao J, Wang X et al. Time series data augmentation for deep learning: A survey. 2020.http://arxiv.org/abs/2002.12478.\u00a0\u21a9\u21a9</p> </li> <li> <p>Petitjean F, Ketterlin A, Gan\u00e7arski P. A global averaging method for dynamic time warping, with applications to clustering. Pattern recognition 2011; 44: 678\u2013693.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Kang Y, Hyndman RJ, Li F. GRATIS: GeneRAting TIme series with diverse and controllable characteristics. 2019.http://arxiv.org/abs/1903.02787.\u00a0\u21a9\u21a9</p> </li> <li> <p>Stock JH, Watson MW. Chapter 8 - dynamic factor models, Factor-Augmented vector autoregressions, and structural vector autoregressions in macroeconomics. In: Taylor JB, Uhlig H (eds). Handbook of macroeconomics. Elsevier, 2016, pp 415\u2013525.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Yoon J, Jarrett D, Schaar M van der. Time-series generative adversarial networks. In: Wallach H, Larochell H, Beygelzime A, Buc F dAlche, Fox E, Garnett R (eds). Advances in neural information processing systems. Curran Associates, Inc., 2019https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html.\u00a0\u21a9</p> </li> <li> <p>Brophy E, Wang Z, She Q, Ward T. Generative adversarial networks in time series: A survey and taxonomy. 2021.http://arxiv.org/abs/2107.11098.\u00a0\u21a9</p> </li> <li> <p>Takahashi N, Gygli M, Van Gool L. AENet: Learning deep audio features for video analysis. 2017.http://arxiv.org/abs/1701.00599.\u00a0\u21a9\u21a9</p> </li> <li> <p>Cui X, Goel V, Kingsbury B. Data augmentation for deep neural network acoustic modeling. In: 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). 2014, pp 5582\u20135586.\u00a0\u21a9\u21a9</p> </li> <li> <p>Iwana BK, Uchida S. Time series data augmentation for neural networks by time warping with a discriminative teacher. 2020.http://arxiv.org/abs/2004.08780.\u00a0\u21a9</p> </li> <li> <p>Gao J, Song X, Wen Q, Wang P, Sun L, Xu H. RobustTAD: Robust time series anomaly detection via decomposition and convolutional neural networks. 2020.http://arxiv.org/abs/2002.09545.\u00a0\u21a9</p> </li> <li> <p>Le Guennec A, Malinowski S, Tavenard R. Data augmentation for time series classification using convolutional neural networks. In: ECML/PKDD workshop on advanced analytics and learning on temporal data. 2016https://halshs.archives-ouvertes.fr/halshs-01357973/document.\u00a0\u21a9</p> </li> <li> <p>Um TT, Pfister FMJ, Pichler D, Endo S, Lang M, Hirche S et al. Data augmentation of wearable sensor data for parkinson\u2019s disease monitoring using convolutional neural networks. 2017.http://arxiv.org/abs/1706.00527.\u00a0\u21a9</p> </li> <li> <p>Bergmeir C, Hyndman RJ, Ben\u0131\u0301tez JM. Bagging exponential smoothing methods using STL decomposition and Box\u2013Cox transformation. International journal of forecasting 2016; 32: 303\u2013312.\u00a0\u21a9</p> </li> <li> <p>Hewamalage H, Bergmeir C, Bandara K. Recurrent neural networks for time series forecasting: Current status and future directions. 2019.http://arxiv.org/abs/1909.00590.\u00a0\u21a9</p> </li> <li> <p>Bandara K, Hewamalage H, Liu Y-H, Kang Y, Bergmeir C. Improving the accuracy of global forecasting models using time series data augmentation. 2020.http://arxiv.org/abs/2008.02663.\u00a0\u21a9\u21a9</p> </li> <li> <p>Forestier G, Petitjean F, Dau HA, Webb GI, Keogh E. Generating synthetic time series to augment sparse datasets. In: 2017 IEEE international conference on data mining (ICDM). 2017, pp 865\u2013870.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Cao H, Tan VYF, Pang JZF. A parsimonious mixture of gaussian trees model for oversampling in imbalanced and multimodal time-series classification. IEEE transactions on neural networks and learning systems 2014; 25: 2226\u20132239.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-data.time-delayed-embedding/", "title": "The Time Delay Embedding Representation", "text": "<p>The time delay embedding representation of time series data is widely used in deep learning forecasting models<sup>1</sup>. This is also called rolling in many time series analyses <sup>2</sup>.</p> <p>For simplicity, we only write down the representation for a problem with time series \\(y_{1}, \\cdots, y_{t}\\), and forecasting \\(y_{t+1}\\). We rewrite the series into a matrix, in an autoregressive way,</p> \\[ \\begin{align} \\mathbf Y = \\begin{bmatrix} y_1 &amp; y_2 &amp; \\cdots &amp; y_p &amp;\\Big| &amp; {\\color{red}y_{p+1}} \\\\ y_{1+1} &amp; y_{1+2} &amp; \\cdots &amp; y_{1+p} &amp;\\Big| &amp;  {\\color{red}y_{1+p+1}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;\\Big| &amp;  {\\color{red}\\vdots} \\\\ y_{i-p+1} &amp; y_{i-p+2} &amp; \\cdots &amp; y_{i} &amp;\\Big| &amp;  {\\color{red}y_{i+1}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;\\Big| &amp;  {\\color{red}\\vdots} \\\\ y_{t-p+1} &amp; y_{t-p+2} &amp; \\cdots &amp; y_{t} &amp;\\Big| &amp;  {\\color{red}y_{t+1}} \\\\ \\end{bmatrix} \\end{align} \\] <p>which indicates that we will use everything on the left, a matrix of shape \\((t-p+1,p)\\), to predict the vector on the right (in red). This is a useful representation when building deep learning models as many of the neural networks require fixed-length inputs.</p>"}, {"location": "time-series/timeseries-data.time-delayed-embedding/#takens-theorem", "title": "Taken's Theorem", "text": "<p>The reason that time delayed embedding representation is useful is that it is a representation of the original time series that preserves the dynamics of the original time series, if any. The math behind it is the Taken's theorem <sup>3</sup>.</p> <p>To illustrate the idea, we take our pendulum dataset as an example. The pendulum dataset describes a damped pendulum, for the math and visualizations please refer to the corresponding page. Here we apply the time delay embedding representation to the pendulum dataset by setting both the history length and the target length to 1, so that we can better visualize it.</p>  Animation of Delayed Embedding Code <p>We plot out the delayed embedding representation of the pendulum dataset. The x-axis is the value of the pendulum angle at time \\(t\\), and the y-axis is the value of the pendulum angle at time \\(t+1\\). The animation shows how the delayed embedding representation evolves over time and shows attractor behavior. If a model can capture this dynamics, it can make good predictions.</p> <p></p> <p>The notebook for more about the dataset itself is here.</p> <pre><code>from functools import cached_property\nfrom typing import List, Tuple\n\nimport matplotlib as mpl\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom ts_dl_utils.datasets.dataset import DataFrameDataset\nfrom ts_dl_utils.datasets.pendulum import Pendulum\n\n\nds_de = DataFrameDataset(dataframe=df[\"theta\"][:200], history_length=1, horizon=1)\n\nclass DelayedEmbeddingAnimation:\n    \"\"\"Builds an animation for univariate time series\n    using delayed embedding.\n\n    ```python\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    dea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)\n    ani = dea.build(interval=10, save_count=dea.time_steps)\n    ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")\n    ```\n\n    :param dataset: a PyTorch dataset, input and target should have only length 1\n    :param fig: figure object from matplotlib\n    :param ax: axis object from matplotlib\n    \"\"\"\n    def __init__(\n        self, dataset: DataFrameDataset, fig: mpl.figure.Figure, ax: mpl.axes.Axes\n    ):\n        self.dataset = dataset\n        self.ax = ax\n        self.fig = fig\n\n    @cached_property\n    def data(self) -&gt; List[Tuple[float, float]]:\n        return [(i[0][0], i[1][0]) for i in self.dataset]\n\n    @cached_property\n    def x(self):\n        return [i[0] for i in self.data]\n\n    @cached_property\n    def y(self):\n        return [i[1] for i in self.data]\n\n    def data_gen(self):\n        for i in self.data:\n            yield i\n\n    def animation_init(self) -&gt; mpl.axes.Axes:\n        ax.plot(\n            self.x,\n            self.y,\n        )\n        ax.set_xlim([-1.1, 1.1])\n        ax.set_ylim([-1.1, 1.1])\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"t+1\")\n\n        return self.ax\n\n    def animation_run(self, data: Tuple[float, float]) -&gt; mpl.axes.Axes:\n        x, y = data\n        self.ax.scatter(x, y)\n        return self.ax\n\n    @cached_property\n    def time_steps(self):\n        return len(self.data)\n\n    def build(self, interval: int = 10, save_count: int = 10):\n        return animation.FuncAnimation(\n            self.fig,\n            self.animation_run,\n            self.data_gen,\n            interval=interval,\n            init_func=self.animation_init,\n            save_count=save_count,\n        )\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\ndea = DelayedEmbeddingAnimation(dataset=ds_de, fig=fig, ax=ax)\n\nani = dea.build(interval=10, save_count=dea.time_steps)\n\ngif_writer = animation.PillowWriter(fps=5, metadata=dict(artist=\"Lei Ma\"), bitrate=100)\n\nani.save(\"results/pendulum_dataset/delayed_embedding_animation.gif\", writer=gif_writer)\n# ani.save(\"results/pendulum_dataset/delayed_embedding_animation.mp4\")\n</code></pre> <p>In some advanced deep learning models, delayed embedding plays a crucial role. For example, Large Language Models (LLM) can perform good forecasts by taking in delayed embedding of time series<sup>4</sup>.</p> <ol> <li> <p>Hewamalage H, Ackermann K, Bergmeir C. Forecast evaluation for data scientists: Common pitfalls and best practices. 2022.http://arxiv.org/abs/2203.10716.\u00a0\u21a9</p> </li> <li> <p>Zivot E, Wang J. Modeling financial time series with s-PLUS. Springer New York, 2006 doi:10.1007/978-0-387-32348-0.\u00a0\u21a9</p> </li> <li> <p>Takens F. Detecting strange attractors in turbulence. In: Lecture notes in mathematics. Springer Berlin Heidelberg: Berlin, Heidelberg, 1981, pp 366\u2013381.\u00a0\u21a9</p> </li> <li> <p>Rasul K, Ashok A, Williams AR, Khorasani A, Adamopoulos G, Bhagwatkar R et al. Lag-llama: Towards foundation models for time series forecasting. arXiv [csLG] 2023.http://arxiv.org/abs/2310.08278.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-datasets.dgp.langevin/", "title": "Time Series Data Generating Process: Langevin Equation", "text": "<p>Among the many data generating processes (DGP), the Langevin equation is one of the most interesting DGP.</p>"}, {"location": "time-series/timeseries-datasets.dgp.langevin/#brownian-motion", "title": "Brownian Motion", "text": "<p>Brownian motion as a very simple stochastic process can be described by the Langevin equation<sup>1</sup>. In this section, we simulate a time series dataset from Brownian motion.</p> <p>Macroscopically, Brownian Motion can be described by the notion of random forces on the particles,</p> \\[ \\frac{d}{dt} v(t) + \\gamma v(t) = R(t), \\] <p>where \\(v(t)\\) is the velocity at time \\(t\\) and \\(R(t)\\) is the stochastic force density from the reservoir particles. Solving the equation, we get</p> \\[ v(t) = v(0)e^{-\\gamma t} + \\int_0^t dt' e^{-\\gamma (t-t')} R(t') . \\] <p>To generate a dataset numerically, we discretize it by replacing the integral with a sum,</p> \\[ v(t) = v(0) e^{-\\gamma t} + \\sum_{n=0}^N \\Delta t e^{-\\gamma (t - t_n)} R(t_n) \\] <p>where \\(t_i = i * \\Delta t\\) and \\(t = t_n\\), thus the equation is further simplified,</p> \\[ v(N\\Delta t) = v(0) e^{-\\gamma N\\Delta t} + \\sum_{n=0}^N  e^{-\\gamma (N - n)\\Delta t} R(n\\Delta t) \\Delta t. \\] <p>The first term in the solution is responsible for the exponential decay and the second term calculates the effect of the stochastic force.</p> <p>To simulate a Brownian motion, we can either use the formal solution or the differential equation itself. Here we choose to use the differential equation itself. To simulate the process numerically, we rewrite</p> \\[ \\frac{d}{dt} v(t) + \\gamma v(t) = R(t), \\] <p>as</p> \\[ \\Delta v (t+1) = R(t) \\Delta t - \\gamma v(t) \\Delta t. \\] Brownian MotionPython Code <p>The following is a simulated 1D Brownian motion.</p> <p></p> <p>We create a stepper to calculate the next steps.</p> <pre><code>import numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n## Define Brownian Motion\nclass GaussianForce:\n\"\"\"A Gaussian stochastic force iterator.\nEach iteration returns a single sample from the corresponding\nGaussian distribution.\n\n:param mu: mean of the Gaussian distribution\n:param std: standard deviation of the Gaussian distribution\n:param seed: seed for the random generator\n\"\"\"\n\ndef __init__(self, mu: float, std: float, seed: Optional[float] = None):\n    self.mu = mu\n    self.std = std\n    self.rng = np.random.default_rng(seed=seed)\n\ndef __next__(self) -&gt; float:\n    return self.rng.normal(self.mu, self.std)\n\n\nclass BrownianMotionStepper:\n    \"\"\"Calculates the next step in a brownian motion.\n\n    :param gamma: the damping factor $\\gamma$ of the Brownian motion.\n    :param delta_t: the minimum time step $\\Delta t$.\n    :param force_densities: the stochastic force densities, e.g. [`GaussianForce`][eerily.data.generators.brownian.GaussianForce].\n    :param initial_state: the initial velocity $v(0)$.\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: float,\n        delta_t: float,\n        force_densities: Iterator,\n        initial_state: Dict[str, float],\n    ):\n        self.gamma = gamma\n        self.delta_t = delta_t\n        self.forece_densities = copy.deepcopy(force_densities)\n        self.current_state = copy.deepcopy(initial_state)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self) -&gt; Dict[str, float]:\n\n        force_density = next(self.forece_densities)\n        v_current = self.current_state[\"v\"]\n\n        v_next = v_current + force_density * self.delta_t - self.gamma * v_current * self.delta_t\n\n        self.current_state[\"force_density\"] = force_density\n        self.current_state[\"v\"] = v_next\n\n        return copy.deepcopy(self.current_state)\n\n## Generating time series\ndelta_t = 0.1\nstepper = BrownianMotionStepper(\n    gamma=0,\n    delta_t=delta_t,\n    force_densities=GaussianForece(mu=0, std=1),\n    initial_state={\"v\": 0},\n)\n\nlength = 200\n\nhistory = []\nfor _ in range(length):\n    history.append(next(stepper))\n\ndf = pd.DataFrame(history)\n\nfig, ax = plt.subplots(figsize=(10, 6.18))\nsns.lineplot(\n    x=np.linspace(0, length-1, length) * delta_t,\n    y=df.v,\n    ax=ax,\n    marker=\"o\",\n)\n\nax.set_title(\"Brownian Motion\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Velocity\")\n</code></pre> <ol> <li> <p>Ma L. Brownian Motion \u2014 Statistical Physics Notes. In: Statistical Physics [Internet]. [cited 17 Nov 2022]. Available: https://statisticalphysics.leima.is/nonequilibrium/brownian-motion.html \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-datasets.dgp/", "title": "Generating Processes for Time Series", "text": "<p>The data generating processes (DGP) for time series are diverse. For example, in physics, we have all sort of dynamical systems that generates time series data and many dynamics models are formulated based on the time series data. In industries, time series data are often coming from stochastic processes.</p> <p>We present some data generating processes to help us build up intuition when modeling real-world data.</p>"}, {"location": "time-series/timeseries-datasets.dgp/#simple-examples-of-dgp", "title": "Simple Examples of DGP", "text": "<p>Exponential Growth</p> <p>Exponential growth is a frequently observed natural and economical phenomenon.</p> \\[ y = e^{c \\cdot t} \\] <p> </p> <p>Circular Motion</p> <p>The circular motion shows some cyclic patterns.</p> \\[ y = \\sin(w \\cdot t) \\] <p></p> <p></p> <p>Random Gaussian</p> <p>Time series can also be noisy Gaussian samples.</p> <p> </p>"}, {"location": "time-series/timeseries-datasets.dgp/#general-linear-processes", "title": "General Linear Processes", "text": "<p>A popular model for modeling as well as generating time series is the autoregressive (AR) model. An AR is formulated as</p> \\[ x_t = \\phi_0 + \\phi_1 x_{t-1} + \\epsilon_t. \\] AR(p) and the Lag Operator <p>A general autoregressive model of p-th order is</p> \\[ x_t = \\sum_l \\phi _i x_{t-i} + \\epsilon_t, \\] <p>where \\(l\\) is the lag.</p> <p>Define a lag operator \\(\\hat L\\) with \\(\\hat L x_t = x_{t-1}\\). The definition can also be rewritten using the lag operator</p> \\[ x_t = \\sum_l \\phi _i {\\hat L}^i x_{t} + \\epsilon_t. \\] <p>We write down each time step in the following table.</p> \\(t\\) \\(x_t\\) 0 \\(y_0\\) 1 \\(\\phi_0 + \\phi_1 y_0 + \\epsilon_1\\) 2 \\(\\phi_0 + \\phi_1 (\\phi_0 + \\phi_1 y_0 + \\epsilon_1) + \\epsilon_2 = \\phi_0 (1 +  \\phi_1) + \\phi_1^2 y_0 + \\phi_1\\epsilon_1 + \\epsilon_2\\) 3 \\(\\phi_0 + \\phi_1 (\\phi_0 +  \\phi_1\\phi_0 + \\phi_1^2 y_0 + \\phi_1\\epsilon_1 + \\epsilon_2) + \\epsilon_3 = \\phi_0(1 + \\phi_1 +  \\phi_1^2) + \\phi_1^3 y_0 + \\phi_1^2\\epsilon_1 + \\phi_1\\epsilon_2 + \\epsilon_3\\) ... ... \\(t\\) \\(\\phi_0 \\sum_{i=0}^{t-1} \\phi_1^i + \\phi_1^t y_0 + \\sum_{i=1}^{t-1} \\phi_1^{t-i} \\epsilon_{i}\\) <p>We have found a new formula for AR(1), i.e.</p> \\[ x_t = \\phi_0 \\sum_{i=0}^{t-1} \\phi_1^i + \\phi_1^t y_0 + \\sum_{i=1}^{t-1} \\phi_1^{t-i} \\epsilon_{i}, \\] <p>which is very similar to a general linear process<sup>1</sup></p> \\[ x_t - \\mu = \\sum_{i=0}^\\tau \\alpha_i \\epsilon_{t-i}. \\] <p>The general linear process is the Taylor expansion of an arbitrary DGP \\(x_t = \\operatorname{DGP}(\\epsilon_t, ...)\\)<sup>1</sup>.</p>"}, {"location": "time-series/timeseries-datasets.dgp/#interactions-between-series", "title": "Interactions between Series", "text": "<p>The interactions between the series can be modeled as explicit interactions, e.g., many spiking neurons, or through hidden variables, e.g., hidden state model<sup>2</sup>. Among these models, Vector Autoregressive model, aka VAR, is a simple but popular model.</p> <ol> <li> <p>Das P. Econometrics in Theory and Practice. Springer Nature Singapore; doi:10.1007/978-981-32-9019-8 \u21a9\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Hidden Markov model. In: Wikipedia [Internet]. 22 Oct 2022 [cited 22 Nov 2022]. Available: https://en.wikipedia.org/wiki/Hidden_Markov_model \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-datasets.ecb-exchange-rate/", "title": "Time Series Dataset: ECB Exchange Rate", "text": "<p>We download the time series data in zip format using this link.</p> <p>We find 41 currencies in this dataset. The earliest date is 1999-01-04.</p> Example PlotsMissing Values <p></p> <p></p>"}, {"location": "time-series/timeseries-datasets/", "title": "Time Series Datasets", "text": "<p>We list a few useful real-world time series datasets here.</p> name link descriptions ECB Exchange Rate Website ECB Exchange Rate Details NREL Solar Power Website Electricity UCI ElectricityLoadDiagrams20112014 Data Set PEMS Caltrans PeMS <p>Apart from real-world data, we also use synthetic data to demonstrate time series analysis and forecasting. The following are some synthetic time series datasets.</p> name link descriptions Chaotic Systems williamgilpin/dysts williamgilpin/dysts"}, {"location": "time-series/timeseries-datasets.nrel-solar-energy/", "title": "Time Series Dataset: Solar Energy", "text": "<p>We download the time series data from this link.</p> <p>NREL's Solar Power Data for Integration Studies are synthetic solar photovoltaic (PV) power plant data points for the United States representing the year 2006.</p> <p>When we downloaded Alabama on 2022-11-05, and loaded <code>Actual_30.45_-88.25_2006_UPV_70MW_5_Min.csv</code> as an example. We found a lot of <code>0</code> entries (which is expected as the will be no power during dark nights).</p> Power is Zero Number of Records False 57868 True 47252 <p>The dataset contains multiple files with each file containing a time series with a time step of 5 minutes (naming convention explained here).</p> Example PlotsPower Distribution in this ExampleMissing Values <p></p> <p></p> <p></p>"}, {"location": "time-series/timeseries-datasets.pems/", "title": "Time Series Dataset: PEMS", "text": "<p>California Department of Transportation (Caltrans) Performance Measurement System (PeMS) provides traffic data on their website. To download the data<sup>1</sup>,</p> <ul> <li>Register on the website and wait for approval, then login.</li> <li>Go to this page and choose the data we need using the filter on the top.<ul> <li>For example, we choose Type = <code>Station 5-Minute</code> and District = <code>District 3</code>.</li> </ul> </li> </ul> <p>We do not show examples of this dataset here.</p> <ol> <li> <p>VeritasYin. How to download the dataset from PeMS website? \u00b7 Issue #6 \u00b7 VeritasYin/STGCN_IJCAI-18. In: GitHub [Internet]. [cited 5 Nov 2022]. Available: https://github.com/VeritasYin/STGCN_IJCAI-18/issues/6 \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-datasets.synthetic/", "title": "Synthetic Time Series", "text": "<p>With a proper understanding of the DGP, we can build data generators around the DGP we choose.</p>", "tags": ["WIP"]}, {"location": "time-series/timeseries-datasets.synthetic/#gluonts", "title": "GluonTS", "text": "<p>GluonTS is a python package for probabilistic time series modeling. It comes with a simple yet easy-to-use synthetic data generator. For example, to generate a time series of random Gaussian, we only need the following code<sup>1</sup>.</p> <pre><code>from gluonts.dataset.artificial import recipe as rcp\ng_rg = rcp.RandomGaussian(stddev=2)\ng_rp_series = rcp.evaluate(g_rg, 100)\n</code></pre> <p>For more complicated multivariate time series, we create recipes for our variables. We steal the example in the GluonTS tutorial.</p> <pre><code>from gluonts.dataset.artificial import recipe as rcp\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\ndaily_smooth_seasonality = rcp.SmoothSeasonality(period=288, phase=-72)\nnoise = rcp.RandomGaussian(stddev=0.1)\nsignal = daily_smooth_seasonality + noise\n\nrecipe = dict(\n    daily_smooth_seasonality=daily_smooth_seasonality, noise=noise, signal=signal\n)\nrec_eval = rcp.evaluate(recipe, 500)\n\nfig, ax = plt.subplots(figsize=(10, 6.18))\nsns.lineplot(rec_eval)\n</code></pre> <p></p> <ol> <li> <p>Synthetic data generation. In: GluonTS documentation [Internet]. [cited 13 Nov 2022]. Available: https://ts.gluon.ai/stable/tutorials/data_manipulation/synthetic_data_generation.html \u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "time-series/timeseries-datasets.uci-electricity/", "title": "Time Series Dataset: Electricity", "text": "<p>This dataset is provided as the \"ElectricityLoadDiagrams20112014 Data Set\" on the UCI website. It is the time series of electricity consumption of 370 points/clients.</p> <p>We download the time series data in zip format using this link.</p> <p>We find that</p> <ul> <li>in total 140256 rows and 370 series,</li> <li>the earliest time is 2011-01-01 00:15:00,</li> <li>the latest time is 2015-01-01 00:00:00,</li> <li>a fixed time interval of 15 minutes.</li> </ul> Example PlotsMissing Values <p>We only plot out three series. We only plot every 100 time steps.</p> <p></p> <p>We fine no missing values.</p> <p></p>"}, {"location": "time-series/timeseries-datasets.uci-electricity/#loading-and-basic-cleaning", "title": "Loading and Basic Cleaning", "text": "<p>We provide some code to load the data from the UCI website.</p> <pre><code>import requests, zipfile, io\nimport pandas as pd\n\n# Download from remote URL\ndata_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\"\n\nr = requests.get(data_uri)\n\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall(\"data/uci_electricity/\")\n\n# Load as pandas dataframe\ndf = pd.read_csv(\n    \"data/uci_electricity/LD2011_2014.txt\", delimiter=\";\", decimal=','\n).rename(columns={\"Unnamed: 0\": \"date\"}).set_index(\"date\")\ndf.index = pd.to_datetime(df.index)\n</code></pre>"}, {"location": "time-series/timeseries-evaluation.forecasting/", "title": "Time Series Forecasting Evaluation", "text": "<p>Evaluating time series forecasting models is very important yet sometimes difficult. For example, it is very easy to bring in information leakage when evaluating time series forecasting models. In this section, we will discuss some of the common pitfalls and best practices when evaluating time series forecasting models.</p>"}, {"location": "time-series/timeseries-evaluation.forecasting/#train-test-split-for-time-series-data", "title": "Train Test Split for Time Series Data", "text": "<p>Evaluating time series models is usually different from most other machine learning tasks as we usually don't have strictly i.i.d. data. On the other hand, the time dimension in our time series data is a natural dimension to split the data into train and test sets. In this section, we will discuss the different ways to split the data into train and test sets.</p>"}, {"location": "time-series/timeseries-evaluation.forecasting/#backtesting", "title": "Backtesting", "text": "<p>We choose the specific time step to split the data, assuming the series used to train the model is \\(Y_t\\) with length \\(T_t\\), and the series used to evaluate the model is \\(Y_e\\) with length \\(T_e\\).</p> <p>Slicing the Training Data \\(Y_t\\) for Training</p> <p>In many deep learning models, the input length and output length are fixed. To train the model using time series data, we usually apply the time delayed embedding method to prepare the train data \\(Y_t\\). Refer to our deep learning forecasting examples for more details.</p> <p>Keeping the length of the train and test unchanged, we can move forward in time, where we require the split time point to fall inside the window<sup>1</sup>. In this way, we create multiple train test splits and perform multiple evaluations, or backtesting, on the model. The following illustration shows an example of this technique. The uppermost panel shows the original series, with each block indicating a time step.</p> <p></p> Expanding the Length of the Training Set <p>Keeping the train set length equal when sliding through time is to simulate the use case where we always take a fixed length of historical data to train the model. For example, some dataset has gradual data shift, and using fixed length historical data can help relieviate data shift problems.</p> <p>In some other use cases, we would take in as much data as possible. In this case, we could also expand the train set when silidng the window.</p> <p></p> <p>In some use cases, we do not get to know the most recent data when we are performing inference. For example, if we are forecasting the demand for the next week, we might not know the demand for the last week as the data might be ready. In this case, we can also use a gap between the train and test sets to simulate this situation.</p> <p></p>"}, {"location": "time-series/timeseries-evaluation.forecasting/#using-multiple-time-windows", "title": "Using Multiple Time Windows", "text": "<p>When we move the split time point forward in time, we could constrain the split to fall inside a specific time window. In the following, we have assumed the sliding window size to be 4, where we move forward one time step for each test set.</p> <p></p> <p>For some large time series forecasting datasets, we might be interested in the performance of some specific types of periods. For example, if Amazon is evaluating its demand forecasting model, it may be more interested in the performance of the model during some normal days as well as the holiday seasons. In this case, we can use multiple sliding windows to evaluate the model.</p>"}, {"location": "time-series/timeseries-evaluation.forecasting/#cross-validation", "title": "Cross-validation", "text": "<p>Not a Common Practice</p> <p>This is not a common practice in time series forecasting problems but it is still worth mentioning here.</p> <p>If our dataset is i.i.d. through time and we do not have information leakage if we randomly take a subsequence of the data, we could also use the cross-validation technique to evaluate the model. The following illustration shows an example of this technique. The uppermost panel shows the original series, with each block indicating a time step.</p> <p></p> <p>Similar to the gap technique in backtesting, we can also use a gap between the train and test sets.</p> <p></p> <ol> <li> <p>Cerqueira V, Torgo L, Mozetic I. Evaluating time series forecasting models: An empirical study on performance estimation methods. 2019.http://arxiv.org/abs/1905.11744.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-forecast.naive/", "title": "Naive Forecasts", "text": "<p>In some sense, time series forecasting is easy, if we have low expectations. From a dynamical system point of view, our future is usually not too different from our current state.</p>"}, {"location": "time-series/timeseries-forecast.naive/#last-observation", "title": "Last Observation", "text": "<p>Assuming our time series is not changing dramatically, we can take our last observation as our forecast.</p> <p>Example: Last Observation as Forecast</p> <p>Assuming we have the simplest dynamical system,</p> \\[ \\frac{\\mathrm d y(t)}{\\mathrm dt} = f(y, t, \\theta), \\] <p>where \\(y(t)\\) is the time series generator function, \\(t\\) is time, \\(\\theta\\) is some parameters defining the function \\(f\\).</p> <p>For example,</p> \\[ \\frac{\\mathrm d y(t)}{\\mathrm dt} = t \\] <p>is a linear growing time series.</p> <p>We would imagine, it won't be too crazy if we just take the last observed value as our forecast.</p>  Naive Last Value Forecasts Code <p></p> <pre><code>import matplotlib.pyplot as plt\n\nfrom darts.utils.timeseries_generation import linear_timeseries\n\nts = linear_timeseries(length=30)\nts.plot(marker=\".\")\n\nts_train, ts_test = ts.split_before(0.9)\nts_train.plot(marker=\".\", label=\"Train\")\nts_test.plot(marker=\"+\", label=\"Test\")\n\nts_last_value_naive_forecast = ts_train.shift(1)[-1]\n\nfig, ax = plt.subplots(figsize=(10, 6.18))\nts_train.plot(marker=\".\", label=\"Train\", ax=ax)\nts_test.plot(marker=\"+\", label=\"Test\", ax=ax)\nts_last_value_naive_forecast.plot(marker=\".\", label=\"Last Value Naive Forecast\")\n</code></pre> <p>There are also slightly more complicated naive forecasting methods.</p>"}, {"location": "time-series/timeseries-forecast.naive/#mean-forecast", "title": "Mean Forecast", "text": "<p>In some bounded time series, the mean of the past values is also a good naive candidate<sup>1</sup>.</p> <p>Example: Naive Mean Forecast</p>  Naive Mean Forecast Code <p></p> <pre><code>import matplotlib.pyplot as plt\n\nfrom darts.utils.timeseries_generation import sine_timeseries\nfrom darts.models.forecasting.baselines import NaiveMean\n\nts_sin = sine_timeseries(length=30, value_frequency=0.05)\n\nts_sin.plot(marker=\".\")\n\n\nts_sin_train, ts_sin_test = ts_sin.split_before(0.9)\n\nts_sin_train.plot(marker=\".\", label=\"Train\")\nts_sin_test.plot(marker=\"+\", label=\"Test\")\n\n\nnaive_mean_model = NaiveMean()\nnaive_mean_model.fit(ts_sin_train)\nts_mean_naive_forecast = naive_mean_model.predict(1)\n\nfig, ax = plt.subplots(figsize=(10, 6.18))\nts_sin_train.plot(marker=\".\", label=\"Train\", ax=ax)\nts_sin_test.plot(marker=\"+\", label=\"Test\", ax=ax)\nts_mean_naive_forecast.plot(marker=\".\", label=\"Naive Mean Forecast\")\n</code></pre>"}, {"location": "time-series/timeseries-forecast.naive/#simple-exponential-smoothing", "title": "Simple Exponential Smoothing", "text": "<p>Simple Exponential Smoothing (SES) is a naive smoothing method to account for the historical values of a time series when forecasting. The expanded form of SES is<sup>1</sup></p> \\[ y(t) = \\alpha ( y(t-1) + (1-\\alpha) y(t-2) + (1-\\alpha)^2 y(t-3) + \\cdots ) \\] <p>Truncated SES is Biased</p> <p>Naively speaking, if history is constant, we have to forecast the same constant. For example, if we have \\(y(t) = y(t_0)\\), the smoothing</p> \\[ y(t) = \\alpha (1 + (1-\\alpha) + \\cdots) y(t_0) \\] <p>should equal to \\(y(t_0)\\), i.e.,</p> \\[ \\alpha (1 + (1-\\alpha) + \\cdots)  = 1. \\] <p>The series indeed sums up to \\(1/\\alpha\\) when \\(n\\to\\infty\\) since</p> \\[ \\sum_{n=0}^\\infty (1-\\alpha) = \\frac{1}{\\alpha}. \\] <p>However, if we truncate the series to finite values, we will have</p> \\[ \\sum_{n=0}^N (1-\\alpha) \\leq \\frac{1}{\\alpha}. \\] <p>Then our naive forecast for constant series is</p> \\[ y(t) = \\alpha \\sum_{n=0}^N  (1-\\alpha)  y(t_0) \\leq y(t_0), \\] <p>when \\(y(t_0)\\) is positive.</p> <p>As an intuition, we plot out the sum of the coefficients for different orders and \\(\\alpha\\)s.</p>  SES Coefficients Code <p></p> <pre><code>from itertools import product\nimport pandas as pd\nimport seaborn as sns; sns.set()\nfrom matplotlib.colors import LogNorm\n\ndef ses_coefficients(alpha, order):\n    return (\n        np.power(\n                np.ones(int(order)) * (1-alpha), np.arange((order))\n            ) * alpha\n    )\n\nalphas = np.linspace(0.05, 0.95, 19)\norders = list(range(1, 16))\n\n# Create dataframes for visualizations\ndf_ses_coefficients = pd.DataFrame(\n    [[alpha, order] for alpha, order in product(alphas, orders)],\n    columns=[\"alpha\", \"order\"]\n)\ndf_ses_coefficients[\"ses_coefficients_sum\"] = df_ses_coefficients.apply(\n    lambda x: ses_coefficients(x[\"alpha\"], x[\"order\"]).sum(), axis=1\n)\n\n# Visualization\ng = sns.heatmap(\n    data=df_ses_coefficients.pivot(\n        \"alpha\", \"order\", \"ses_coefficients_sum\"\n    ),\n    square=True, norm=LogNorm(),\n    fmt=\"0.2g\",\n    yticklabels=[f\"{i:0.2f}\" for i in alphas],\n)\n\ng.set_title(\"SES Sum of Coefficients\");\n</code></pre> <p>Holt-Winters' Exponential Smoothing</p> <p>In applications, the Holt-Winters' exponential smoothing is more practical<sup>1</sup><sup>2</sup><sup>3</sup>.</p> <p>We created some demo time series and apply the Holt-Winters' exponential smoothing. To build see where exponential smoothing works, we forecast at different dates.</p>  Linear Example Sine Example 1 Sine Example 2 Sine Example 3 Sine Example 3, Longer History Code <p></p> <p></p> <p></p> <p></p> <p></p> <pre><code>import matplotlib.pyplot as plt\n\nfrom darts.utils.timeseries_generation import sine_timeseries\nfrom darts.models.forecasting.baselines import NaiveMean\n\nts_sin = sine_timeseries(length=30, value_frequency=0.05)\n\nts_sin.plot(marker=\".\")\n\nts_sin_train, ts_sin_test = ts_sin.split_before(0.7)\n\nes_model = ExponentialSmoothing()\n\nes_model.fit(ts_sin_train)\nes_model_sin_forecast = es_model.predict(4)\n\nfig, ax = plt.subplots(figsize=(10, 6.18))\nts_sin_train.plot(marker=\".\", label=\"Train\", ax=ax)\nts_sin_test.plot(marker=\"+\", label=\"Test\", ax=ax)\nes_model_sin_forecast.plot(marker=\".\", label=\"Exponential Smoothing Forecast\")\n</code></pre>"}, {"location": "time-series/timeseries-forecast.naive/#other", "title": "Other", "text": "<p>Other naive forecasts, such as naive drift, are introduced in Hyndman, et al., (2021)<sup>1</sup>.</p> <ol> <li> <p>Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-02-13.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>6.4.3.5. Triple Exponential Smoothing. In: NIST Engineering Statistics Handbook [Internet]. [cited 16 Feb 2023]. Available: https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm \u21a9</p> </li> <li> <p>Example: Holt-Winters Exponential Smoothing \u2014 NumPyro\u00a0 documentation. In: NumPyro [Internet]. [cited 16 Feb 2023]. Available: https://num.pyro.ai/en/stable/examples/holt_winters.html \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-forecast.tasks/", "title": "Time Series Forecasting Tasks", "text": "<p>There are many different types of time series forecasting tasks. Forecasting tasks can be categorized by different criteria. For example, we can categorize them by the number of variables in the series and their relations to each other.</p> <p>In the introduction of this chapter, we already discussed some terminologies of time series forecasting. In this section, we dive deep into the details of univariate time series forecasting and multivariate time series forecasting.</p>"}, {"location": "time-series/timeseries-forecast.tasks/#forecasting-univariate-time-series", "title": "Forecasting Univariate Time Series", "text": "<p>In a univariate time series forecasting task, we are given a single time series and asked to forecast future steps of the series.</p> <p></p> <p>Given a time series \\(\\{y_{t}\\}\\), we train a model to forecast \\(\\color{red}y_{t+1:t+H}\\) using input \\(\\color{blue}y_{t-K:t}\\), i.e., we build a model \\(f\\) such that</p> \\[ f({\\color{blue}y_{t-K:t}}) \\to {\\color{red}y_{t+1:t+H}}. \\] <p></p>"}, {"location": "time-series/timeseries-forecast.tasks/#forecasting-multivariate-time-series", "title": "Forecasting Multivariate Time Series", "text": "<p>In a multivariate time series forecasting task, we will deal with multiple time series. Naively, we expect multivariate time series forecasting to be nothing special but adding more series. However, the complication comes from the fact that different series may not be aligned well at all time steps.</p> <p>In the introduction of this chapter, we have shown the basic ideas of targets \\(\\mathbf y\\) and covariates \\(\\mathbf x\\) and \\(\\mathbf u\\). In the following illustration, we expand the idea to the multivariate case.</p> <p></p>"}, {"location": "time-series/timeseries-hierarchical.data/", "title": "Hierarchical Time Series Data", "text": "<p>Many real-world time series data assert some internal structure among the series. For example, the dataset used in the M5 competition is the sales data of different items but with the store and category information provided<sup>1</sup>. For simplicity, we simplified the dataset to only include the hierarchy of stores.</p>  Hierarchy Structure Visualization <p>The simplified dataset can be found here. The original data can be found on the website of IIF. In this simplified version of the M5 dataset, we have the following hierarchy.</p> <pre><code>flowchart LR\n\ntop[\"Total Sales\"]\n\nca[\"Sales in California\"]\ntx[\"Sales in Texas\"]\nwi[\"Sales in Wisconsin\"]\n\ntop --- ca\ntop --- tx\ntop --- wi\n\nsubgraph California\nca1[\"Sales in Store #1 in CA\"]\nca2[\"Sales in Store #2 in CA\"]\nca3[\"Sales in Store #3 in CA\"]\nca4[\"Sales in Store #4 in CA\"]\n\nca --- ca1\nca --- ca2\nca --- ca3\nca --- ca4\nend\n\n\nsubgraph Texas\ntx1[\"Sales in Store #1 in TX\"]\ntx2[\"Sales in Store #2 in TX\"]\ntx3[\"Sales in Store #3 in TX\"]\n\ntx --- tx1\ntx --- tx2\ntx --- tx3\n\nend\n\n\nsubgraph Wisconsin\nwi1[\"Sales in Store #1 in WI\"]\nwi2[\"Sales in Store #2 in WI\"]\nwi3[\"Sales in Store #3 in WI\"]\n\nwi --- wi1\nwi --- wi2\nwi --- wi3\n\nend</code></pre> <p>The above tree is useful when thinking about the hierarchies. For example, it explicitly tells us that the sales in stores #1, #2, #3 in TX should sum up to the sales in TX.</p> <p>We plotted the sales in CA as well as the individual stores in CA. We can already observe some synchronized anomalies.</p> <p></p> <p></p>"}, {"location": "time-series/timeseries-hierarchical.data/#summing-matrix", "title": "Summing Matrix", "text": "<p>The relations between the series is represented using a summing matrix \\(\\mathbf S\\), which connects the bottom level series \\(\\mathbf b\\) and all the possible levels \\(\\mathbf s\\)<sup>2</sup></p> \\[ \\mathbf y(t) = \\mathbf S \\mathbf b(t). \\] <p>If our forecasts satisfy this relation, we claim our forecasts to be coherent<sup>2</sup>.</p> <p>Summing Matrix Example</p> <p>We take part of the above dataset and only consider the hierarchy of states,</p> \\[ s(t) = s_\\mathrm{CA}(t) + s_\\mathrm{TX}(t) + s_\\mathrm{WI}(t). \\] <p>The hierarchy is also revealed in the following tree.</p> <pre><code>flowchart TD\n\ntop[\"Total Sales\"]\n\nca[\"Sales in California\"]\ntx[\"Sales in Texas\"]\nwi[\"Sales in Wisconsin\"]\n\ntop --- ca\ntop --- tx\ntop --- wi</code></pre> <p>In this example, the bottom level series are denoted as</p> \\[ \\mathbf b(t) = \\begin{pmatrix} s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}, \\] <p>and all the possible levels are denoted as</p> \\[ \\mathbf y(t) = \\begin{pmatrix} s(t) \\\\ s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>The summing matrix is</p> \\[ \\mathbf S = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}. \\] <ol> <li> <p>Makridakis S, Spiliotis E, Assimakopoulos V. The M5 competition: Background, organization, and implementation. Int J Forecast. 2022;38: 1325\u20131336. doi:10.1016/j.ijforecast.2021.07.007\u00a0\u21a9</p> </li> <li> <p>Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2022-11-27.\u00a0\u21a9\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-hierarchical.reconciliation/", "title": "Hierarchical Time Series Reconciliation", "text": "<p>Reconciliation is a post-processing method to adjust the forecasts to be coherent. Given base forecasts \\(\\hat{\\mathbf y}(t)\\) (forecasts for all levels but each level forecasted independently), we use \\(\\mathbf P\\) to map them to the bottom-level forecasts</p> \\[ \\begin{equation} \\tilde{\\mathbf b}(t) = \\mathbf P \\hat{\\mathbf y}(t). \\end{equation} \\] <p>\\(P\\) and \\(S\\)</p> <p>In the previous section, we discussed the summing matrix \\(\\color{blue}S\\). The summing matrix maps the bottom-level forecasts \\(\\color{red}{\\mathbf b}(t)\\) to all forecasts on all levels \\(\\color{green}\\mathbf y(t)\\). The example we provided was</p> \\[{\\color{green}\\begin{pmatrix} s(t) \\\\ s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}} = {\\color{blue}\\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}} {\\color{red}\\begin{pmatrix} s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}}. \\] <p>If we forecast different levels independently, the forecasts we get</p> \\[ \\hat{\\mathbf y}(t) = \\begin{pmatrix} \\hat s(t) \\\\ \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}, \\] <p>are not necessarily coherent. However, if we can choose a proper \\(\\mathbf P\\), we can convert the base forecasts into some bottom-level forecasts</p> \\[ \\begin{pmatrix} \\tilde s_\\mathrm{CA}(t) \\\\ \\tilde s_\\mathrm{TX}(t) \\\\ \\tilde s_\\mathrm{WI}(t) \\end{pmatrix} = \\mathbf P \\begin{pmatrix} \\hat s(t) \\\\ \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>From the usage, \\(\\mathbf S\\) and \\(\\mathbf P\\) are like conjugates. We have the following relation</p> \\[ \\begin{pmatrix} \\tilde s_\\mathrm{CA}(t) \\\\ \\tilde s_\\mathrm{TX}(t) \\\\ \\tilde s_\\mathrm{WI}(t) \\end{pmatrix} = \\mathbf P \\mathbf S {\\color{red}\\begin{pmatrix} s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}}. \\] <p>It is clear that \\(\\mathbf P \\mathbf S\\) is identity if we set</p> \\[ \\begin{pmatrix} \\tilde s_\\mathrm{CA}(t) \\\\ \\tilde s_\\mathrm{TX}(t) \\\\ \\tilde s_\\mathrm{WI}(t) \\end{pmatrix} = {\\color{red}\\begin{pmatrix} s_\\mathrm{CA}(t) \\\\ s_\\mathrm{TX}(t) \\\\ s_\\mathrm{WI}(t) \\end{pmatrix}}. \\] <p>However, this is not the only \\(\\mathbf P\\) we can choose.</p> <p>To generate the coherent forecasts \\(\\tilde{\\mathbf y}(t)\\), we use the summing matrix to map the bottom level forecasts to base forecasts of all levels<sup>1</sup><sup>2</sup></p> \\[ \\begin{equation} \\tilde{\\mathbf y}(t) = \\mathbf S\\tilde{\\mathbf b}(t) = \\mathbf S \\mathbf P \\hat{\\mathbf y}(t). \\label{eq:reconciliation-compact-form} \\end{equation} \\] <p>Walmart Sales in Stores</p> <p>We reuse the example of the Walmart sales data. The base forecasts for all levels are</p> \\[ \\hat{\\mathbf y}(t) = \\begin{pmatrix} \\hat s(t) \\\\ \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>The simplest mapping to the bottom-level forecasts is</p> \\[ \\tilde{\\mathbf b}(t) = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\begin{pmatrix} \\hat s(t) \\\\ \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>where</p> \\[ \\tilde{\\mathbf b}(t) = \\begin{pmatrix} \\tilde s_\\mathrm{CA}(t) \\\\ \\tilde s_\\mathrm{TX}(t) \\\\ \\tilde s_\\mathrm{WI}(t) \\end{pmatrix} \\] <p>are the bottom-level forecasts to be transformed into coherent forecasts.</p> <p>In this simple method, our mapping matrix \\(\\mathbf P\\) can be</p> \\[ \\mathbf P = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}. \\] <p>Using this \\(\\mathbf P\\), we get</p> \\[ \\tilde{\\mathbf b}(t) = \\hat{\\mathbf b}(t) = \\begin{pmatrix} \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>The last step is to apply the summing matrix</p> \\[ \\mathbf S = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, \\] <p>so that</p> \\[ \\tilde{\\mathbf y}(t) = \\mathbf S \\tilde{\\mathbf b}(t) = \\begin{pmatrix}  \\hat s_\\mathrm{CA}(t) + \\hat s_\\mathrm{TX}(t) + \\hat s_\\mathrm{WI}(t) \\\\ \\hat s_\\mathrm{CA}(t) \\\\ \\hat s_\\mathrm{TX}(t) \\\\ \\hat s_\\mathrm{WI}(t) \\end{pmatrix}. \\] <p>In summary, our coherent forecasts for each level are</p> \\[ \\begin{align} \\tilde s (t) &amp;= \\hat s_\\mathrm{CA}(t) + \\hat s_\\mathrm{TX}(t) + \\hat s_\\mathrm{WI}(t) \\\\ \\tilde s_\\mathrm{CA}(t) &amp;= \\hat s_\\mathrm{CA}(t) \\\\ \\tilde s_\\mathrm{TX}(t) &amp;= \\hat s_\\mathrm{TX}(t) \\\\ \\tilde s_\\mathrm{WI}(t) &amp;= \\hat s_\\mathrm{WI}(t). \\end{align} \\] <p>The \\(\\mathbf P\\) we used in this example represents the bottom-up method.</p> <p>Results like \\(\\tilde s_\\mathrm{CA}(t) = \\hat s_\\mathrm{CA}(t)\\) look comfortable but they are not necessary. In other reconciliation methods, these relations might be broken, i.e., \\(\\tilde s_\\mathrm{CA}(t) = \\hat s_\\mathrm{CA}(t)\\) may not be true.</p> <p>Component Form</p> <p>We rewrite</p> \\[ \\tilde{\\mathbf y}(t) = \\mathbf S \\mathbf P \\hat{\\mathbf y}(t) \\] <p>using the component form</p> \\[ \\tilde y_i = S_{ij} G_{jk} \\hat y_k. \\] <p>There is more than one \\(\\mathbf P\\) that can map the forecasts to the bottom-level forecasts. Three of the so-called single-level approaches<sup>1</sup> are bottom-up, top-down, and middle-out<sup>2</sup>.</p> <p>Apart from these intuitive methods, Wickramasuriya et al. (2017) proposed the MinT method to find the optimal \\(\\mathbf P\\) matrix that gives us the minimal trace of the covariance of the reconciled forecast error<sup>3</sup>,</p> \\[ \\tilde{\\boldsymbol \\epsilon} = \\mathbf y(t) - \\tilde{\\mathbf y}(t), \\] <p>with \\(\\mathbf y\\) being the ground truth and \\(\\tilde{\\mathbf y}\\) being the coherent forecasts. Wickramasuriya et al. (2017) showed that the optimal \\(\\mathbf P\\) is</p> \\[ \\begin{equation} \\hat{\\mathbf P} = (\\mathbf S^T \\mathbf W(t)^{-1} \\mathbf S)^{-1} (\\mathbf S^{T}\\mathbf W(t)^{-1}), \\label{eq-mint-p} \\end{equation} \\] <p>where \\(W_{h} = \\mathbb E\\left[ \\tilde{\\boldsymbol \\epsilon} \\tilde{\\boldsymbol \\epsilon}^T \\right] = \\mathbb E \\left[ (\\mathbf y(t) - \\tilde{\\mathbf y}(t))(\\mathbf y(t) - \\tilde{\\mathbf y}(t))^T \\right]\\) is the covariance matrix of the reconciled forecast error.</p> <p>\\(\\hat{\\mathbf P} \\neq \\mathbf I\\)</p> <p>Note that \\(\\mathbf S\\) is not a square matrix and we can't simply apply the inverse on each element,</p> \\[ (\\mathbf S^T \\mathbf W(t)^{-1} \\mathbf S)^{-1} \\neq \\mathbf S^{-1}  \\mathbf W(t)  {\\mathbf S^T}^{-1}. \\] <p>MinT is easy to calculate but it assumes that the forecasts are unbiased. To relieve this constraint, Van Erven &amp; Cugliari (2013) proposed a game-theoretic method called GTOP<sup>4</sup>. In deep learning, Rangapuram et al. (2021) developed an end-to-end model for coherent probabilistic hierarchical forecasts<sup>2</sup>. For these advanced topics, we redirect the readers to the original papers.</p>"}, {"location": "time-series/timeseries-hierarchical.reconciliation/#mint-examples", "title": "MinT Examples", "text": ""}, {"location": "time-series/timeseries-hierarchical.reconciliation/#theories", "title": "Theories", "text": "<p>To see how the MinT method works, we calculate a few examples based on equation \\(\\eqref{eq-mint-p}\\). For simplicity, we assume that the variance \\(\\mathbf W\\) is diagonal<sup>3</sup>. Note that the matrix \\(\\mathbf S \\mathbf P\\) decides how each original forecast is combined, \\(\\tilde{\\mathbf y} = \\mathbf S \\mathbf P \\hat{\\mathbf y}\\). It will be the key for us to understand how MinT works.</p> <p>In the following examples, we observe that the lower variance of the reconciled forecast error \\(W_{ii}\\), the less change in the reconciled result. Since lower values of \\(W_{ii}\\) indicate lower reconciled forecast error, reconciliation should not adjust it by a lot.</p> Two Levels: 2 Bottom Level SeriesCode <p>For a 2-level hierarchical forecasting problem, the shape of the \\(\\mathbf S\\) matrix is (3,2) and we have three values to pre-compute or assume, i.e., the diagonal elements of \\(\\mathbf W\\).</p> \\(\\mathbf S\\) \\(\\mathbf P\\) \\(\\mathbf S \\mathbf P\\) \\(\\left[\\begin{matrix}1 &amp; 1\\\\1 &amp; 0\\\\0 &amp; 1\\end{matrix}\\right]\\) \\(\\left[\\begin{matrix}\\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{1}} &amp; \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{2} \\left(W_{1} + W_{2} + W_{3}\\right)} &amp; - \\frac{W_{2}}{W_{1} + W_{2} + W_{3}}\\\\\\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{1}} &amp; - \\frac{W_{3}}{W_{1} + W_{2} + W_{3}} &amp; \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{3} \\left(W_{1} + W_{2} + W_{3}\\right)}\\end{matrix}\\right]\\) \\(\\left[\\begin{matrix}\\frac{- \\frac{2 W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{1}} &amp; \\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{2}} &amp; \\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{3}}\\\\\\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{1}} &amp; \\frac{W_{1} W_{2} + W_{2} W_{3}}{W_{2} \\left(W_{1} + W_{2} + W_{3}\\right)} &amp; - \\frac{W_{2}}{W_{1} + W_{2} + W_{3}}\\\\\\frac{- \\frac{W_{2} W_{3}}{W_{1} + W_{2} + W_{3}} + \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{1} + W_{2} + W_{3}}}{W_{1}} &amp; - \\frac{W_{3}}{W_{1} + W_{2} + W_{3}} &amp; \\frac{W_{1} W_{3} + W_{2} W_{3}}{W_{3} \\left(W_{1} + W_{2} + W_{3}\\right)}\\end{matrix}\\right]\\) <p>We visualize the matrix \\(\\mathbf S\\mathbf P\\). It is straightforward to verify that it always leads to coherent results. </p> <pre><code>import sympy as sp\nimport numpy as np\nimport seaborn as sns\n\nclass MinTMatrices:\n    def __init__(self, levels: int):\n        self.levels = levels\n\n    @property\n    def s(self):\n        s_ident_diag = np.diag([1] * (self.levels - 1)).tolist()\n        return sp.Matrix(\n            [\n                [1] * (self.levels - 1),\n            ] + s_ident_diag\n        )\n    @property\n    def w_diag_elements(self):\n        return tuple(\n            sp.Symbol(f\"W_{i}\")\n            for i in range(1, self.levels + 1)\n        )\n\n    @property\n    def w(self):\n        return sp.Matrix(np.diag(self.w_diag_elements).tolist())\n\n    @property\n    def p_left(self):\n        return sp.Inverse(\n            sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w), self.s)\n        )\n\n    @property\n    def p_right(self):\n        return (\n            sp.MatMul(sp.Transpose(self.s), sp.Inverse(self.w))\n        )\n\n    @property\n    def p(self):\n        return sp.MatMul(self.p_left, self.p_right)\n\n    @property\n    def s_p(self):\n        return sp.MatMul(self.s, self.p)\n\n    @property\n    def s_p_numerical(self):\n        return sp.lambdify(\n            self.w_diag_elements,\n            self.s_p\n        )\n\n    def visualize_s_p(self, w_elements, ax):\n        sns.heatmap(self.s_p_numerical(*w_elements), annot=True, cbar=False, ax=ax)\n        ax.grid(False)\n        ax.set(xticklabels=[], yticklabels=[])\n        ax.tick_params(bottom=False, left=False)\n        ax.set_title(f\"$W_{{diag}} = {w_elements}$\")\n        return ax\n\nmtm_3 = MinTMatrices(levels=3)\nprint(\n    f\"s: {sp.latex(mtm_3.s)}\\n\"\n    f\"p: {sp.latex(mtm_3.p.as_explicit())}\\n\"\n    f\"s_p: {sp.latex(mtm_3.s_p.as_explicit())}\\n\"\n)\n\n# 2 bottom series, in total three series\nmtm_3.s\n\nmtm_3.p\n\nmtm_3.s_p.as_explicit()\n\nw_elements = [\n    (1,1,1),\n    (2,1,1)\n]\n\nfig, axes = plt.subplots(nrows = 1, ncols=2, figsize=(4 * 2, 4))\n\nfor idx, w in enumerate(w_elements):\n    mtm_3.visualize_s_p(w, axes[idx])\nfig.show()\n</code></pre> <p>Implementations</p> <p>There are different methods to get the covariance matrix \\(\\mathbf W\\). We discuss a few examples and their implications.</p> method \\(\\mathbf W\\) Note OLS \\(\\mathbf I\\) More weight on the higher levels in the hierarchy Structual Scaling \\(\\operatorname{diag}(\\mathbf S \\mathbf I)\\) Less weight on higher levels compared to OLS"}, {"location": "time-series/timeseries-hierarchical.reconciliation/#real-world-data", "title": "Real-world Data", "text": "<p>Code</p> <p>The code for this subsection can be found in this notebook (also available on Google Colab).</p> <p>We use a small subset of the M5 competition data to show that MinT works by shifting the values on different hierarchies.</p> date CA TX WI CA_1 CA_2 CA_3 CA_4 TX_1 TX_2 TX_3 WI_1 WI_2 WI_3 Total 2011-01-29 00:00:00 14195 9438 8998 4337 3494 4739 1625 2556 3852 3030 2704 2256 4038 32631 2011-01-30 00:00:00 13805 9630 8314 4155 3046 4827 1777 2687 3937 3006 2194 1922 4198 31749 2011-01-31 00:00:00 10108 6778 6897 2816 2121 3785 1386 1822 2731 2225 1562 2018 3317 23783 2011-02-01 00:00:00 11047 7381 6984 3051 2324 4232 1440 2258 2954 2169 1251 2522 3211 25412 2011-02-02 00:00:00 9925 5912 3309 2630 1942 3817 1536 1694 2492 1726 2 1175 2132 19146 <p>We apply a simple LightGBM model using Darts. The forecasts are not coherent.</p> <p></p> <p>Applying MinT method, we reached coherent forecasts for all levels. The following charts shows the example for the top two levels.</p> <p></p> <p>Each step was adjusted differently since the forecasted values are different. To see how exactly the forecasted are adjusted to reach coherency, we plot out the difference between the reconciled results and the original forecasts, \\(\\tilde{\\mathbf y} - \\hat{\\mathbf y}\\).</p> <p></p>"}, {"location": "time-series/timeseries-hierarchical.reconciliation/#tools-and-packages", "title": "Tools and Packages", "text": "<p>Darts and hierarchicalforecast from Nixtla provide good support for reconciliations.</p> <ol> <li> <p>Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2022-11-27.\u00a0\u21a9\u21a9</p> </li> <li> <p>Rangapuram SS, Werner LD, Benidis K, Mercado P, Gasthaus J, Januschowski T. [End-to-End\u00a0\u21a9\u21a9\u21a9 <li> <p>Wickramasuriya SL, Athanasopoulos G, Hyndman RJ. Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association 2019; 114: 804\u2013819.\u00a0\u21a9\u21a9</p> </li> <li> <p>Erven T van, Cugliari J. Game-Theoretically optimal reconciliation of contemporaneous hierarchical time series forecasts. In: Modeling and stochastic learning for forecasting in high dimensions. Springer International Publishing, 2015, pp 297\u2013317.\u00a0\u21a9</p> </li>"}, {"location": "time-series/timeseries-metrics.forecasting.crps/", "title": "Continuous Ranked Probability Score (CRPS)", "text": "<p>The Continuous Ranked Probability Score, aka CRPS, is a score to measure how a proposed distribution approximates the data, without knowledge about the true distributions of the data.</p>"}, {"location": "time-series/timeseries-metrics.forecasting.crps/#definition", "title": "Definition", "text": "<p>CRPS is defined as<sup>1</sup></p> \\[ \\operatorname{CRPS}(P, x_a) = \\int_{-\\infty}^\\infty  \\lVert P(x) - H(x - x_a) \\rVert_2 dx, \\] <p>where</p> <ul> <li>\\(x_a\\) is the true value of \\(x\\),</li> <li>P(x) is our proposed cumulative distribution for \\(x\\),</li> <li>\\(H(x)\\) is the Heaviside step function,</li> <li>\\(\\lVert \\cdot \\rVert_2\\) is the L2 norm.</li> </ul> <p>Heaviside Step Function</p> \\[ H(x) = \\begin{cases} 1, &amp;\\qquad x=0\\\\ 0, &amp;\\qquad x\\leq 0\\\\ \\end{cases} \\]"}, {"location": "time-series/timeseries-metrics.forecasting.crps/#explain-it", "title": "Explain it", "text": "<p>The formula looks abstract on first sight, but it becomes crystal clear once we understand it.</p> <p>Note that the distribution that corresponds to a Heaviside CDF is the delta function \\(\\delta(x-x_a)\\). What this score is calculating is the difference between our distribution and a delta function. If we have a model that minimizes CRPS, then we are looking for a distribution that is close to the delta function \\(\\delta(x-x_a)\\). In other words, we want our distribution to be large around \\(x_a\\).</p> <p>To illustrate what the integrand \\(\\lVert P(x) - H(x - x_a) \\rVert_2\\) means, we apply some shades to the integrand of the integral in CRPS. We visualize four difference scenarios.</p> <p>Scenario 1: The predicted CDF \\(P(x)\\) is reaching 1 very fast.</p> <p></p> <p>Scenario 2: The predicted CDF \\(P(x)\\) is reaching 1 quite late.</p> <p></p> <p>Scenario 3: The predicted CDF \\(P(x)\\) is reaching 1 around the Heaviside function jump.</p> <p></p> <p>Scenario 4: The predicted CDF \\(P(x)\\) is steadily increasing but very dispersed.</p> <p></p> <p>With the four different scenarios visualized, intuitively, the only way to get a small CRPS score is to choose a distribution that is focused around \\(x_a\\). Echoing a previous note on the delta function being the density function of the Heaviside function, we expect a small CRPS reflects a scenario of the following: the predicted distribution \\(\\rho(x)\\) is very focused around the observation \\(x_a\\).</p> <p></p>"}, {"location": "time-series/timeseries-metrics.forecasting.crps/#discussions", "title": "Discussions", "text": "<p>Gebetsberger et al found that CRPS is more robust compared to Likelihood while producing similar results if we use a good assumption for the data distribution<sup>3</sup>.</p> <p>CRPS is also very useful in time series forecasting. For example, the integrand of CRPS can be used as the loss function in model training <sup>2</sup>.</p> <ol> <li> <p>Hersbach H. Decomposition of the Continuous Ranked Probability Score for Ensemble Prediction Systems. Weather Forecast. 2000;15: 559\u2013570. doi:10.1175/1520-0434(2000)015&lt;0559:DOTCRP&gt;2.0.CO;2 \u21a9</p> </li> <li> <p>Gouttes A, Rasul K, Koren M, Stephan J, Naghibi T. Probabilistic Time Series Forecasting with Implicit Quantile Networks. arXiv [cs.LG]. 2021. doi:10.1109/icdmw.2017.19 \u21a9</p> </li> <li> <p>Gebetsberger M, Messner JW, Mayr GJ, Zeileis A. Estimation Methods for Nonhomogeneous Regression Models: Minimum Continuous Ranked Probability Score versus Maximum Likelihood. Mon Weather Rev. 2018;146: 4323\u20134338. doi:10.1175/MWR-D-17-0364.1 \u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-metrics.forecasting/", "title": "Time Series Forecasting Metrics", "text": "<p>Measuring the goodness of a forecaster is nontrivial. Tons of metrics are devised to measure forecasting results, applying the wrong metric may lead to \"consequences\" in decisions.</p> <p>In the following discussion, we assume the forecast at time \\(t\\) to be \\(\\hat y(t)\\) and the actual value is \\(y(t)\\). The forecast horizon is defined as \\(H\\). In general, we look for a function</p> \\[ L\\left(\\{y(t_1), \\cdots, y(t_H)\\}, \\{\\hat y(t_1), \\cdots, \\hat y(t_H)\\}; \\{C(t)\\}, \\{y(t)\\} \\right), \\] <p>where \\(\\{C(t)\\}\\) are the covariates and \\(\\{y(t)\\}\\) represents the past target variables.</p> <p>Distance between Truths and Forecasts</p> <p>Naive choices of such metrics are distances between the truth vector \\(\\{y(t_1), \\cdots, y(t_H)\\}\\) and the forecast vector \\(\\{\\hat y(t_1), \\cdots, \\hat y(t_H)\\}\\).</p> <p>For example, we can use norms of the deviation vector \\(\\{y(t_1) - \\hat y(t_1), \\cdots, y(t_H) - \\hat y(t_H)\\}\\).</p> <p>In Hyndman &amp; Koehler (2006), \\(y(t_i) - \\hat y(t_i)\\) is defined as the forecast error \\(e_i\\equiv y(t_i) - \\hat y(t_i)\\)<sup>4</sup>. While it is a bit confusing, the term forecast error is used in many kinds of literature.</p> <p>The authors also defined the relative error \\(r_i = e_i/e^*_i\\) with \\(e^*_i\\) being the reference forecast error from the baseline.</p> <p>In this section, we explore some frequently used metrics. Hyndman &amp; Koehler (2006) discussed four different types of metrics<sup>4</sup></p> <ol> <li>scaled-dependent measures, e.g., errors based on \\(\\{y(t_1) - \\hat y(t_1), \\cdots, y(t_H) - \\hat y(t_H)\\}\\),</li> <li>percentage errors, e.g., errors based on \\(\\{\\frac{y(t_1) - \\hat y(t_1)}{y(t_1)}, \\cdots, \\frac{y(t_H) - \\hat y(t_H)}{y(t_H)}\\}\\),</li> <li>relative errors, e.g., errors based on \\(\\{\\frac{y(t_1) - \\hat y(t_1)}{y(t_1) - \\hat y^*(t_1)}, \\cdots, \\frac{y(t_H) - \\hat y(t_H)}{y(t_H) - \\hat y^*(t_H)}\\}\\), where \\(\\hat y^*(t_i)\\) is a baseline forecast at time \\(t_i\\),</li> <li>relative metrics, e.g., the ratio of the MAE for the experiment and the baseline, \\(\\operatorname{MAE}/\\operatorname{MAE}_{\\text{baseline}}\\),</li> <li>in-sample scaled errors, e.g., MASE.</li> </ol> <p>Apart from the above categories, there are some other properties of metrics. Some metrics are bounded while others are not. Also, some metrics specifically require probabilistic forecasts. In the following table, we list some of the useful metrics.</p> Metric Probabilistic Theoretical Range Notes MAE \\([0,\\infty)\\) MSE \\([0,\\infty)\\) RMSE \\([0,\\infty)\\) MASE \\([0,\\infty)\\) Scaled in practice; requires insample data RMSLE \\([0,\\infty)\\) MAPE \\([0,\\infty]\\) sMAPE \\([0, 2]\\) For values of the same sign wMAPE - Depends on what weights are used Quantile Score \\([0,\\infty)\\) CRPS \\([0,\\infty)\\) <p>Recommended Reading</p> <p>Hyndman &amp; Athanasopoulos (2021) is a good reference for forecast errors <sup>1</sup>.</p> <p>To find implementations of metrics, Darts and GluonTS both have a handful of metrics implemented.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#list-of-metrics", "title": "List of Metrics", "text": "Code to Reproduce the Results <pre><code># %%\nfrom loguru import logger\nimport datetime\nimport numpy as np\nfrom itertools import product\n\nfrom matplotlib.ticker import FormatStrFormatter\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n\nfrom darts.utils.timeseries_generation import (\n    sine_timeseries,\n    linear_timeseries,\n    constant_timeseries,\n)\n\nfrom darts.metrics.metrics import (\n    mae,\n    mape,\n    marre,\n    mse,\n    ope,\n    rho_risk,\n    rmse,\n    rmsle,\n    smape,\n    mase\n)\n\n# %%\nlength = 600\nstart = 0\nts_sin = sine_timeseries(length=length, value_frequency=0.01, start=0)\nts_lin = linear_timeseries(\n    length=length, start_value=0, end_value=1.5, start=0\n)\n\nts = (ts_sin + ts_lin).with_columns_renamed(\"sine\", \"sin+linear\")\n\nsplit_at = 500\nts_train, ts_test = ts.split_before(split_at)\nts_train = ts_train.with_columns_renamed(\"sin+linear\", \"train\")\nts_test = ts_test.with_columns_renamed(\"sin+linear\", \"actual\")\n\n\n_, ts_pred_lin = ts_lin.split_before(split_at)\nts_pred_lin = ts_pred_lin.with_columns_renamed(\"linear\", \"linear_prediction\")\n\n\n_, ts_pred_sin = ts_sin.split_before(split_at)\nts_pred_sin = ts_pred_sin.with_columns_renamed(\"sine\", \"sin_prediction\")\n\nts_pred_const = constant_timeseries(\n    value=ts_train.last_value(),\n    start=ts_test.start_time(),\n    end=ts_test.end_time()\n)\nts_pred_const = ts_pred_const.with_columns_renamed(\"constant\", \"constant_prediction\")\n\n\n# %%\nts.plot(marker=\".\")\n# ts_lin.plot(linestyle=\"dashed\")\n# ts_sin.plot(linestyle=\"dashed\")\n\nts_train.plot()\nts_test.plot(color=\"r\")\n\nts_pred_lin.plot(color=\"orange\")\nts_pred_sin.plot(color=\"green\")\nts_pred_const.plot(color=\"black\")\n\n# %%\nclass MetricBench:\n    def __init__(self, metric_fn, metric_name=None):\n        self.metric_fn = metric_fn\n        if metric_name is None:\n            metric_name = self.metric_fn.__name__\n        self.metric_name = metric_name\n\n    def _heatmap_data(self, actual_range=None, pred_range=None):\n        if actual_range is None:\n            actual_range = np.linspace(-1,1, 21)\n        if pred_range is None:\n            pred_range = np.linspace(-1,1, 21)\n        hm_data = []\n        for y, y_hat in product(actual_range, pred_range):\n            ts_y = constant_timeseries(value=y, length=1)\n            ts_y_hat = constant_timeseries(value=y_hat, length=1)\n            try:\n                hm_data.append(\n                    {\n                        \"y\": y,\n                        \"y_hat\": y_hat,\n                        \"metric\": f\"{self.metric_name}\",\n                        \"value\": self.metric_fn(ts_y, ts_y_hat)\n                    }\n                )\n            except Exception as e:\n                logger.warning(f\"Skipping due to {e}\")\n\n        df_hm_data = pd.DataFrame(hm_data)\n\n        return df_hm_data\n\n    def heatmap(self, ax=None, cmap=\"viridis_r\", actual_range=None, pred_range=None):\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(12, 10))\n\n        df_hm_data = self._heatmap_data(actual_range=actual_range, pred_range=pred_range)\n\n        sns.heatmap(\n            df_hm_data.pivot(\"y_hat\", \"y\", \"value\"),\n            fmt=\".2g\",\n            cmap=cmap,\n            ax=ax,\n        )\n\n        ax.set_xticklabels(\n            [self._heatmap_fmt(label.get_text())\n            for label in ax.get_xticklabels()]\n        )\n        ax.set_yticklabels(\n            [self._heatmap_fmt(label.get_text())\n            for label in ax.get_yticklabels()]\n        )\n        ax.set_title(f\"Metric: {self.metric_name}\")\n\n        return ax\n\n    @staticmethod\n    def _heatmap_fmt(s):\n        try:\n            n = \"{:.2f}\".format(float(s))\n        except:\n            n = \"\"\n        return n\n\n    def _ratio_data(self, pred_range=None):\n        if pred_range is None:\n            pred_range = np.linspace(-1, 3, 41)\n        ratio_data = []\n        y = 1\n        for y_hat in pred_range:\n            ts_y = constant_timeseries(value=y, length=1)\n            ts_y_hat = constant_timeseries(value=y_hat, length=1)\n            try:\n                ratio_data.append(\n                    {\n                        \"y\": y,\n                        \"y_hat\": y_hat,\n                        \"metric\": f\"{self.metric_name}\",\n                        \"value\": self.metric_fn(ts_y, ts_y_hat)\n                    }\n                )\n            except Exception as e:\n                logger.warning(f\"Skipping due to {e}\")\n\n        df_ratio_data = pd.DataFrame(ratio_data)\n\n        return df_ratio_data\n\n    def ratio_plot(self, ax=None, color=\"k\", pred_range=None):\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(12, 10))\n\n        df_ratio_data = self._ratio_data(pred_range=pred_range)\n\n        sns.lineplot(df_ratio_data, x=\"y_hat\", y=\"value\", ax=ax)\n\n        ax.set_title(f\"Metric {self.metric_name} (y=1)\")\n\n        return ax\n\n\n# %% [markdown]\n# ## Norms (MAE, MSE)\n\n# %%\nmse_bench = MetricBench(metric_fn=mse)\nmse_bench.heatmap()\n\n# %%\nmse_bench.ratio_plot()\n\n# %%\nmae_bench = MetricBench(metric_fn=mae)\nmae_bench.heatmap()\n\n\n# %%\nmae_bench.ratio_plot()\n\n# %%\nrmsle_bench = MetricBench(metric_fn=rmsle)\nrmsle_bench.heatmap()\n\n# %%\nrmsle_bench.ratio_plot()\n\n# %%\nrmse_bench = MetricBench(metric_fn=rmse)\nrmse_bench.heatmap()\n\n# %%\nrmse_bench.ratio_plot()\n\n# %%\ny_pos = np.linspace(0.1, 1, 20)\n\nmape_bench = MetricBench(metric_fn=mape)\nmape_bench.heatmap(actual_range=y_pos)\n\n# %%\nmape_bench.ratio_plot()\n\n# %%\nsmape_bench = MetricBench(metric_fn=smape)\nsmape_bench.heatmap(actual_range=y_pos, pred_range=y_pos)\n\n# %%\nsmape_bench.ratio_plot(pred_range=y_pos)\n\n# %% [markdown]\n# ## Naive MultiHorizon Forecasts\n\n# %%\ntwo_args_metrics = [\n    mse, mae, rmse, rmsle, mape, smape\n]\n\ninsample_metrics = [mase]\n\n\nmetrics_tests = []\n\nfor m in two_args_metrics:\n    metrics_tests.append(\n        {\n            \"metric\": m.__name__,\n            \"value_lin_pred\": m(ts_test, ts_pred_lin),\n            \"value_sin_pred\": m(ts_test, ts_pred_sin),\n            \"value_const_pred\": m(ts_test, ts_pred_const)\n        }\n    )\n\nfor m in insample_metrics:\n    metrics_tests.append(\n        {\n            \"metric\": m.__name__,\n            \"value_lin_pred\": m(ts_test, ts_pred_lin, insample=ts_train),\n            \"value_sin_pred\": m(ts_test, ts_pred_sin, insample=ts_train),\n            \"value_const_pred\": m(ts_test, ts_pred_const, insample=ts_train)\n        }\n    )\n\ndf_metrics_tests = (\n    pd.DataFrame(metrics_tests)\n    .round(3)\n    .set_index(\"metric\")\n    .sort_values(by=\"value_const_pred\")\n    .sort_values(by=\"mape\", axis=1)\n)\n\ndf_metrics_tests.rename(\n    columns={\n        \"value_lin_pred\": \"Linear Prediction\",\n        \"value_sin_pred\": \"Sine Prediction\",\n        \"value_const_pred\": \"Last Observed\"\n    },\n    inplace=True\n)\n\ndf_metrics_tests\n\n# %%\nfrom matplotlib.colors import LogNorm\n\n# %%\nmetrics_tests_min_value = df_metrics_tests.min().values.min()\nmetrics_tests_max_value = np.ma.masked_invalid(df_metrics_tests.max()).max()\nmetrics_tests_min_value, metrics_tests_max_value\n\n# %%\nfig, ax = plt.subplots(figsize=(10, 6.18))\n\nsns.heatmap(\n    df_metrics_tests,\n    norm=LogNorm(\n        vmin=0.1,\n        vmax=100\n    ),\n    cbar_kws={\"ticks\":[0,1,10,1e2]},\n    vmin = 0.1, vmax=100,\n    annot=True,\n    fmt=\"0.3f\",\n    ax=ax,\n)\n</code></pre>"}, {"location": "time-series/timeseries-metrics.forecasting/#1-norm-mae", "title": "1-Norm: MAE", "text": "<p>The Mean Absolute Error (MAE) is</p> \\[ \\operatorname{MAE}(y, \\hat y) = \\frac{1}{H}\\sum_{t=1}^{t=H}\\lvert y(t) - \\hat y(t)\\rvert. \\] <p>We can check some special and extreme cases:</p> <ul> <li> <p>All forecasts are zeros: \\(\\hat y(t)=0\\), the value of \\(\\operatorname{MAE}(y, \\hat y)\\) is determined by the true value \\(y\\).</p> <p>The Interpretation of MAE is Scale Dependent</p> <p>This also tells us that MAE depends on the scale of the true values: MAE value of \\(100\\) for larger true values such as true value \\(y=1000\\) with forecast \\(\\hat y=900\\) doesn't seem to be bad, but MAE value for smaller true values such as true value \\(y=100\\) with forecast \\(\\hat y=0\\) seems to be quite off. Of course, the actual perception depends on the problem we are solving.</p> <p>This brings in a lot of trouble when we are dealing with forecasts on different scales, such as sales forecasts for all kinds of items on an e-commerce platform. Different types of items, e.g., expensive watches vs cheat T-shirts, have very different sales. In fact, in a paper from Amazon, the sales on Amazon are even scale-free<sup>5</sup>.</p> </li> <li> <p>All forecasts are infinite: \\(\\hat y=\\infty\\), the MAE value will also be \\(\\infty\\). This means MAE is not bounded.</p> </li> </ul>  Forecasts and Actuals Forecasts and Fixed Actuals <p></p> <p></p>"}, {"location": "time-series/timeseries-metrics.forecasting/#2-norm-mse", "title": "2-Norm: MSE", "text": "<p>The Mean Square Error (MSE) is</p> \\[ \\operatorname{MSE}(y, \\hat y) = \\frac{1}{H}\\sum_{t=1}^{t=H}(y(t) - \\hat y(t))^2. \\] <p>Similar to MAE, the interpretation of MSE is also scale dependent and the value is unbounded. However, due to the \\({}^2\\), MSE can be really large or small. Obtaining insights from MSE is even harder than MAE in most situations unless MSE matches a meaningful quantity in the dynamical system we are forecasting. Nevertheless, we can know that large deviations (\\(\\lvert y(t) - \\hat y(t)\\rvert \\gg 1\\)) dominates the metric even more than MAE.</p>  Forecasts and Actuals Forecasts and Fixed Actuals <p></p> <p></p> Other Norms <p>Other norms are not usually seen in literature but might provide insights into forecasts.</p> Maximum norm <p>The Max Norm error of a forecast can be defined as<sup>2</sup></p> \\[\\operatorname{MAE}(y, \\hat y) = \\operatorname{max}\\left( \\{y(t) - \\hat y(t)\\}\\right).\\]"}, {"location": "time-series/timeseries-metrics.forecasting/#rmse", "title": "RMSE", "text": "<p>The Root Mean Square Error (RMSE) is</p> \\[ \\operatorname{RMSE}(y, \\hat y) = \\sqrt{\\operatorname{MSE}(y, \\hat y)} = \\sqrt{\\frac{1}{H}\\sum_{t=1}^{t=H}(y(t) - \\hat y(t))^2}. \\] <p>RMSE essentially brings the scale of the metric from the MSE scale back to something similar to MAE. However, we have to be mindful that large deviations dominate the metric more than that in MAE.</p> <p>Domination by Large Deviations</p> <p>For example, in a horizon 2 forecasting problem, suppose we have the true values \\([100, 1]\\) and we forecast \\([0, 0]\\)</p> \\[ \\operatorname{RMSE} = \\sqrt{ \\frac{1}{2}\\left((100-0)^2 + (1-0)^2 \\right)} \\sim 70.71 \\] <p>If we assume the second step is forecasted perfectly, i.e., forecasts \\([0, 1]\\), we have almost the same RMSE</p> \\[ \\sqrt{ \\frac{1}{2}(100-0)^2 + (1-1)^2} \\sim 70.71 \\] <p>For MAE, assuming forecasts \\([0,0]\\), we get</p> \\[ \\operatorname{MAE} = \\frac{1}{2} \\left( \\lvert 1 - 0 \\rvert + \\lvert 100 - 0 \\rvert \\right) = 50.5. \\] <p>If we assume the forecast \\([0,1]\\), we get something slightly different</p> \\[ \\frac{1}{2} \\lvert 100 - 0 \\rvert  = 50. \\] <p>To see the difference between RMSE and MAE visually, we compute the following quantities</p> \\[ MAE(y=[x, 1], \\hat y=[0, 1]) / MAE(y=[x, 1], \\hat y=[0, 0]) \\] <p>as well as</p> \\[ RMSE(y=[x, 1], \\hat y=[0, 1]) / RMSE(y=[x, 1], \\hat y=[0, 0]). \\] <p>Using these ratios, we investigate the contributions from the large deviations for MAE and RMSE.</p>  MAE vs RMSE Code for Chart <p></p> <pre><code>import numpy as np\nfrom darts.metrics.metrics import mae, rmse\nfrom darts import TimeSeries\n\nmetric_contrib_x = np.linspace(0, 50, 101)\n\nmae_contrib_ratio = []\nfor i in metric_contrib_x:\n    mae_contrib_ratio.append(\n        mae(\n            TimeSeries.from_values(np.array([i,1,])),\n            TimeSeries.from_values(np.array([0,1,])),\n        )/mae(\n            TimeSeries.from_values(np.array([i,1,])),\n            TimeSeries.from_values(np.array([0,0,])),\n        )\n    )\n\nrmse_contrib_ratio = []\nfor i in metric_contrib_x:\n    rmse_contrib_ratio.append(\n        rmse(\n            TimeSeries.from_values(np.array([ i, 1,])),\n            TimeSeries.from_values(np.array([0, 1,])),\n        )/rmse(\n            TimeSeries.from_values(np.array([i, 1,])),\n            TimeSeries.from_values(np.array([0,0])),\n        )\n    )\n</code></pre> <p>The above chart shows that RMSE is more dominated by large deviations.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#mase", "title": "MASE", "text": "<p>The Mean Absolute Scaled Error (MASE) is the MAE scaled by the one-step ahead naive forecast error on the training data (\\(\\{y(t_i)\\}\\), with \\(i\\in {1, \\cdots, T}\\))<sup>3</sup></p> \\[ \\operatorname{MASE}(y(t), \\hat y(t)) = \\frac{\\operatorname{MAE}(y(t), \\hat y(t))}{ \\frac{1}{H-1} \\sum_{i=1}^H \\lvert y(t_i) - y(t_{i-1})\\rvert }. \\] <p>Due to the scaling by the one-step ahead naive forecast, MASE is easier to interpret. If MASE is large, the deviation in our forecasts is comparable to the rough scale of the time series. Naively, we expect a good MASE to be smaller than 1.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#rmsle", "title": "RMSLE", "text": "<p>The Root Mean Squared Log Error (RMSLE) is</p> \\[ \\begin{align} &amp;\\operatorname{RMSLE(y(t), \\hat y(t))} \\\\ = &amp;\\operatorname{MSE\\left( \\ln(y(t) + 1), \\ln(\\hat y(t) + 1) \\right)} \\\\ = &amp;\\sqrt{ \\frac{1}{H} \\sum_{i=1}^T \\left( \\ln (y(t) + 1) - \\ln (\\hat y(t) +1 ) \\right)^2 }. \\end{align} \\]  Forecasts and Actuals Forecasts and Fixed Actuals <p></p> <p></p>"}, {"location": "time-series/timeseries-metrics.forecasting/#mape", "title": "MAPE", "text": "<p>The Mean Absolute Percent Error (MAPE) is a bounded metric defined as</p> \\[ \\operatorname{MAPE(y(t), \\hat y(t))} = \\frac{1}{H} \\sum_{i=1}^H \\left\\lvert \\frac{y(t_i) - \\hat y(t_i)}{y(t_i)} \\right\\rvert. \\]  Forecasts and Actuals Forecasts and Fixed Actuals <p></p> <p></p>"}, {"location": "time-series/timeseries-metrics.forecasting/#smape", "title": "sMAPE", "text": "<p>The symmetric Mean Absolute Percent Error (sMAPE) is a symmetrized version of MAPE</p> \\[ \\operatorname{sMAPE}(y(t), \\hat y(t)) = \\frac{1}{H} \\sum_{i=1}^H \\frac{\\lvert y(t_i) - \\hat y(t_i) \\rvert}{ \\lvert y(t_i) + \\hat y(t_i) \\rvert/2 }. \\]  Forecasts and Actuals Forecasts and Fixed Actuals <p></p> <p></p> <p>sMAPE is Bounded but Hard to Get a Feeling</p> <p>Even though sMAPE is bounded and it solves the blow-up problem in MAPE, it is dangerous to use sMAPE alone. For example, given true values \\([1]\\), forecasting \\([10]\\) gives us sMAPE value \\(1.636\\); Forecasting \\([100]\\) gives us sMAPE value \\(1.960\\); Forecasting \\([1000]\\) gives us sMAPE value \\(1.996\\). The later are not too different judging by the sMAPE values.</p> <p>That being said, as the sMAPE value gets a bit larger, it is hard to get stable intuitions on how bad the forecast is.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#wmape", "title": "wMAPE", "text": "<p>The weighted Mean Absolute Percent Error (wMAPE) is</p> \\[ \\operatorname{wMAPE}(y(t), \\hat y(t)) = \\frac{\\sum_{i=1}^H w_i \\lvert y(t_i) - \\hat y(t_i) \\rvert }{\\sum_{i=1}^H w_i}. \\]"}, {"location": "time-series/timeseries-metrics.forecasting/#quantile-loss", "title": "Quantile Loss", "text": "<p>The Quantile loss is defined as <sup>6</sup><sup>7</sup><sup>8</sup></p> \\[ \\operatorname{QL}(y(t), \\hat y(t)) = \\sum_{t=1}^H \\sum_{q}\\left(     q (y(t) - \\hat y(t))_+ + (1-q) (\\hat y(t) - y(t))_+ \\right), \\] <p>where \\({}_{+}\\) indicates that we only take positive values.</p> <p>Quantile Loss has many names</p> <p>The quantile loss is also called quantile score, pinball loss, quantile risk or \\(\\rho\\)-risk.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#other-metrics", "title": "Other Metrics", "text": "<p>We do not have a full collection of all metrics available. But we also explain some more complicated metrics, e.g., CRPS, as individual sections.</p>"}, {"location": "time-series/timeseries-metrics.forecasting/#metrics-applied-on-a-toy-problem", "title": "Metrics Applied on a Toy Problem", "text": "<p>To feel the difference between each metric, we assume a simple forecasting problem with some artificial time series data.</p> <p>We construct the artificial data by summing a sine series and a linear series.</p> <p></p> <p>We have prepared three naive forecasts,</p> <ol> <li>forecasting constant values using the last observation,</li> <li>forecasting the sin component of the actual data,</li> <li>forecasting the linear component of the actual data.</li> </ol> <p>We calculated the metrics for the three different scenarios.</p> <p></p> <ol> <li> <p>Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2022-11-27.\u00a0\u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Uniform norm. In: Wikipedia [Internet]. 23 Oct 2022 [cited 5 Mar 2023]. Available: https://en.wikipedia.org/wiki/Uniform_norm \u21a9</p> </li> <li> <p>Contributors to Wikimedia projects. Mean absolute scaled error. In: Wikipedia [Internet]. 11 Jan 2023 [cited 5 Mar 2023]. Available: https://en.wikipedia.org/wiki/Mean_absolute_scaled_error \u21a9</p> </li> <li> <p>Hyndman RJ, Koehler AB. Another look at measures of forecast accuracy. International journal of forecasting 2006; 22: 679\u2013688.\u00a0\u21a9\u21a9</p> </li> <li> <p>Salinas D, Flunkert V, Gasthaus J. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. 2017.http://arxiv.org/abs/1704.04110.\u00a0\u21a9</p> </li> <li> <p>Gneiting T. Quantiles as optimal point forecasts. International journal of forecasting 2011; 27: 197\u2013207.\u00a0\u21a9</p> </li> <li> <p>Koenker R, Bassett G. Regression quantiles. Econometrica: journal of the Econometric Society 1978; 46: 33\u201350.\u00a0\u21a9</p> </li> <li> <p>Vargas Staudacher JMR de, Wu B, Struss C, Mettenleiter N. Uncertainty quantification and probabilistic forecasting of big data time series at amazon supply chain. TUM Data Innovation Lab, 2022https://www.mdsi.tum.de/fileadmin/w00cet/di-lab/pdf/Amazon\\SS2022\\Final\\Report.pdf.\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series/timeseries-synthetic/", "title": "Synthetic Time Series", "text": "<p>Synthetic time series data is useful in time series modeling, such as forecasting.</p> <p>Real world time series data often comes with complex dynamics in the data generating process. Benchmarking models using real world data often doesn't reflect the special designs in forecasting models. Synthetic time series data provides a good playground for benchmarking models as it can provide useful insights.</p> <p>Another application of synthetic data is to improve model performance. Synthetic data can be used to augment the training data<sup>2</sup> as well as in transfer learning<sup>1</sup>.</p> <p>A third application of synthetic data is data sharing without compromising privacy and business secrets<sup>3</sup>.</p> <p>Though useful, synthesizing proper artificial time series data can be very complicated as there are an enormous amount of diverse theories associated with time series data. On the other hand, many time series generators are quite universal. For example, GAN can be used to generate realistic time series<sup>4</sup>.</p> <p>In this chapter, we will explain the basic ideas and demonstrate our generic programming framework for synthetic time series. With the basics explored, we will focus on a special case of synthetic time series: time series with interactions.</p> <ol> <li> <p>Rotem Y, Shimoni N, Rokach L, Shapira B. Transfer learning for time series classification using synthetic data generation. arXiv [cs.LG]. 2022. Available: http://arxiv.org/abs/2207.07897 \u21a9</p> </li> <li> <p>Bandara K, Hewamalage H, Liu Y-H, Kang Y, Bergmeir C. Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2008.02663 \u21a9</p> </li> <li> <p>Lin Z, Jain A, Wang C, Fanti G, Sekar V. Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1909.13403 \u21a9</p> </li> <li> <p>Leznik M, Michalsky P, Willis P, Schanzel B, \u00d6stberg P-O, Domaschka J. Multivariate Time Series Synthesis Using Generative Adversarial Networks. Proceedings of the ACM/SPEC International Conference on Performance Engineering. New York, NY, USA: Association for Computing Machinery; 2021. pp. 43\u201350. doi:10.1145/3427921.3450257\u00a0\u21a9</p> </li> </ol>"}, {"location": "time-series-deep-learning/", "title": "Time Series Forecasting with Deep Learning", "text": "<p>In the chapter Deep Learning Fundamentals, we discussed some deep learning models. In this chapter, we will discuss how to apply deep learning models to time series forecasting problems.</p>"}, {"location": "time-series-deep-learning/#creating-dataset-for-deep-learning-models", "title": "Creating Dataset for Deep Learning Models", "text": "<p>Deep learning models usually require batches of data to train. For time series data, we need to slice along the time axis to create batches. In section The Time Delay Embedding Representation, we discussed methods to represent time series data. In this section, we provide an example.</p> <p>In our <code>ts_dl_utils</code> package, we provide a class called <code>DataFrameDataset</code>. This class moves along the time axis and cuts the time series into multiple data points.</p> <pre><code>from typing import Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom loguru import logger\nfrom torch.utils.data import Dataset\n\n\nclass DataFrameDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param history_length: length of input X in time dimension\n        in the final Dataset class.\n    :param horizon: number of steps to be forecasted.\n    :param gap: gap between input history and prediction\n    \"\"\"\n\n    def __init__(\n        self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.history_length = history_length\n        self.horzion = horizon\n        self.gap = gap\n        self.dataframe_rows = len(self.dataframe)\n        self.length = (\n            self.dataframe_rows - self.history_length - self.horzion - self.gap + 1\n        )\n\n    def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        x, y = (\n            self.dataframe[idx : self.history_length + idx].values,\n            self.dataframe[\n                self.history_length\n                + idx\n                + gap : self.history_length\n                + self.horzion\n                + idx\n                + gap\n            ].values,\n        )\n        return x, y\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [\n                self.moving_slicing(i, self.gap)\n                for i in range(idx.start, idx.stop, step)\n            ]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx, self.gap)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</code></pre> <p>For example, give a time series dataset,</p> index y 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 <p>The first data point of <code>DataFrameDataset(dataframe=df, history_length=10, horizon=1)</code> will be</p> <pre><code>(array([[0],\n         [1],\n         [2],\n         [3],\n         [4],\n         [5],\n         [6],\n         [7],\n         [8],\n         [9]]),\n  array([[10]]))\n</code></pre>"}, {"location": "time-series-deep-learning/timeseries.cnn/", "title": "Forecasting with Convolutional Neural Networks", "text": "", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.dataset.pendulum/", "title": "Pendulum Dataset", "text": "<p>We create a synthetic dataset based on the physical model called pendulum. The pendulum is modeled as a damped harmonic oscillator, i.e.,</p> \\[ \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t), \\] <p>where \\(\\theta(t)\\) is the angle of the pendulum at time \\(t\\). The period \\(p\\) is calculated using</p> \\[ p = 2 \\pi \\sqrt(L / g), \\] <p>with \\(L\\) being the length of the pendulum and \\(g\\) being the surface gravity.</p> Pendulum Angle Physics and Pytorch Dataset <p></p> <pre><code>import math\nfrom functools import cached_property\nfrom typing import Dict, List\n\nimport pandas as pd\n\nclass Pendulum:\n    \"\"\"Class for generating time series data for a pendulum.\n\n    The pendulum is modelled as a damped harmonic oscillator, i.e.,\n\n    $$\n    \\theta(t) = \\theta(0) \\cos(2 \\pi t / p)\\exp(-\\beta t),\n    $$\n\n    where $\\theta(t)$ is the angle of the pendulum at time $t$.\n    The period $p$ is calculated using\n\n    $$\n    p = 2 \\pi \\sqrt(L / g),\n    $$\n\n    with $L$ being the length of the pendulum\n    and $g$ being the surface gravity.\n\n    :param length: Length of the pendulum.\n    :param gravity: Acceleration due to gravity.\n    \"\"\"\n\n    def __init__(self, length: float, gravity: float = 9.81) -&gt; None:\n        self.length = length\n        self.gravity = gravity\n\n    @cached_property\n    def period(self) -&gt; float:\n        \"\"\"Calculate the period of the pendulum.\"\"\"\n        return 2 * math.pi * math.sqrt(self.length / self.gravity)\n\n    def __call__(\n        self,\n        num_periods: int,\n        num_samples_per_period: int,\n        initial_angle: float = 0.1,\n        beta: float = 0,\n    ) -&gt; Dict[str, List[float]]:\n        \"\"\"Generate time series data for the pendulum.\n\n        Returns a list of floats representing the angle\n        of the pendulum at each time step.\n\n        :param num_periods: Number of periods to generate.\n        :param num_samples_per_period: Number of samples per period.\n        :param initial_angle: Initial angle of the pendulum.\n        \"\"\"\n        time_step = self.period / num_samples_per_period\n        steps = []\n        time_series = []\n        for i in range(num_periods * num_samples_per_period):\n            t = i * time_step\n            angle = (\n                initial_angle\n                * math.cos(2 * math.pi * t / self.period)\n                * math.exp(-beta * t)\n            )\n            steps.append(t)\n            time_series.append(angle)\n\n        return {\"t\": steps, \"theta\": time_series}\n\npen = Pendulum(length=100)\ndf = pd.DataFrame(pen(10, 400, initial_angle=1, beta=0.001))\n\n_, ax = plt.subplots(figsize=(10, 6.18))\ndf.plot(x=\"t\", y=\"theta\", ax=ax)\n</code></pre> <p>We take this time series and ask our model to forecast the next step (forecast horizon is 1).</p> <p>PyTorch Dataset and Lightning DataModule</p> <p>In our tutorials, we will use Pytorch lightning excessively. We defined some useful modules in our  <code>ts_dl_utils</code> package and  this notebook.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/", "title": "TimeGrad Using Diffusion Model", "text": "<p>Rasul et al., (2021) proposed a probabilistic forecasting model using denoising diffusion models.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#autoregressive", "title": "Autoregressive", "text": "<p>Multivariate Forecasting Problem</p> <p>Given an input sequence \\(\\mathbf x_{t-K: t}\\), we forecast \\(\\mathbf x_{t+1:t+H}\\).</p> <p>See this section for more Time Series Forecasting Tasks.</p> <p>Notation</p> <p>We use \\(x^0\\) to denote the actual time series. The super script \\({}^{0}\\) will be used to represent the non-diffused values.</p> <p>To apply the denoising diffusion model in a multivariate forecasting problem, we define our forecasting task as the following autoregressive problem,</p> \\[ q(\\mathbf x^0_{t - K:t} \\vert \\mathbf x^0_{1:t_0 - 1}) = \\Pi_{t=t_0}^T q(\\mathbf x^0_t \\vert \\mathbf x^0_{1:t-1}). \\] <p></p> <p>At each time step \\(t\\), we build a denoising diffusion model.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#time-dynamics", "title": "Time Dynamics", "text": "<p>Note that in the denoising diffusion model, we minimize</p> \\[ \\operatorname{min}_\\theta \\mathbb E_{q(\\mathbf x^0)} \\left[ -\\log p_\\theta (\\mathbf x^0) \\right] \\] <p>The above loss becomes that of the denoising model for a single time step. Explicitly,</p> \\[ \\operatorname{min}_\\theta \\mathbb E_{q(\\mathbf x^0_t )} \\left[ -\\log p_\\theta (\\mathbf x^0_t) \\right]. \\] <p>Time dynamics can be easily captured by some RNN. To include the time dynamics, we use the RNN state built using the time series data of the previous time step \\(\\mathbf h_{t-1}\\)<sup>1</sup></p> \\[ \\operatorname{min}_\\theta \\mathbb E_{q(\\mathbf x^0_t )} \\left[ -\\log p_\\theta (\\mathbf x^0_t \\vert \\mathbf h_{t-1}) \\right]. \\] <p>Apart from the usual time dimension \\(t\\), the autoregressive denoising diffusion model has another dimension to optimize: the diffusion step \\(n\\) for each time \\(t\\).</p> <p>The loss for each time step \\(t\\) is<sup>1</sup></p> \\[ \\begin{equation} \\mathcal L_t = \\mathbb E_{\\mathbf x^0_t, \\epsilon, n} \\left[ \\lVert \\epsilon - \\epsilon_\\theta ( \\sqrt{\\bar \\alpha_n} \\mathbf x^0_t + \\sqrt{1-\\bar \\alpha_n}\\epsilon, \\mathbf h_{t-1}, n ) \\rVert^2  \\right]. \\label{eq:ddpm-loss} \\end{equation} \\] <p>That being said, we just need to minimize \\(\\mathcal L_t\\) for each time step \\(t\\).</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#training-algorithm", "title": "Training Algorithm", "text": "<p>The input data is sliced into fixed length time series \\(\\mathbf x_t^0\\). Since Eq \\eqref{eq:ddpm-loss} shows that a loss can be calculated for arbitrary \\(n\\) without depending on any previous diffusion steps \\(n-1\\), the training can be done by both random sampling in \\(\\mathbf x_t^0\\) and \\(n\\). See Rasul et al. (2021)<sup>1</sup>.</p> <p></p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#how-to-forecast", "title": "How to Forecast", "text": "<p>After training, we obtain the time dynamics encoding \\(\\mathbf h_T\\), with which the denoising steps can be calculated using the reverse process</p> \\[ \\mathbf x^{n-1}_{T+1} = \\frac{1}{\\alpha_n} \\left( \\mathbf x^n_{T+1} - \\frac{\\beta_n}{1 - \\bar\\alpha_n} \\epsilon_\\theta( \\mathbf x^n_{T+1}, \\mathbf h_{T}, n ) \\right) + \\sqrt{\\Sigma_\\theta} \\mathbf z, \\] <p>where \\(\\mathbf z \\sim \\mathcal N(\\mathbf 0, \\mathbf I)\\).</p> <p>For example,</p> \\[ \\mathbf x^{0}_{T+1} = \\frac{1}{\\alpha_1} \\left( \\mathbf x^1_{T+1} - \\frac{\\beta_1}{1 - \\bar\\alpha_1} \\epsilon_\\theta( \\mathbf x^1_{T+1}, \\mathbf h_{T}, 1 ) \\right) + \\sqrt{\\Sigma_\\theta} \\mathbf z. \\]", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#it-is-probabilistic", "title": "It is Probabilistic", "text": "<p>The quantiles is calculated by repeating many times for each forecasted time step<sup>1</sup>.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.deep-learning.timegrad/#code", "title": "Code", "text": "<p>An implementation of the model can be found in the package pytorch-ts <sup>2</sup>.</p> <ol> <li> <p>Rasul K, Seward C, Schuster I, Vollgraf R. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2101.12072 \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Rasul K. PyTorchTS. https://github.com/zalandoresearch/pytorch-ts.\u00a0\u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.feedforward/", "title": "Forecasting with Feedforward Neural Networks", "text": "<p> Jupyter Notebook Available</p> <p>We have a  notebook for this section which includes all the code used in this section.</p> <p> Introduction to Neural Networks</p> <p>We explain the theories of neural networks in this section. Please read it first if you are not familiar with neural networks.</p> <p>Feedforward neural networks are simple but powerful models for time series forecasting. In this section, we will build a simple feedforward neural network to forecast our pendulum physics data.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.feedforward/#feedforward-neural-network-model", "title": "Feedforward Neural Network Model", "text": "Feedforward Neural Network Code <p>We build a feedforward neural network with 5 hidden layers. The input is passed to the first hidden layer, and the output of the first hidden layer is passed to the second hidden layer. The output of the second hidden layer is passed to the third hidden layer, and so on. The output of the last hidden layer is passed to the output layer. The output layer outputs the forecasted values.</p> <pre><code>flowchart TD\n\ninput_layer[\"Input Layer (100)\"]\noutput_layer[\"Output Layer (1)\"]\n\nsubgraph hidden_layers[\"Hidden Layers\"]\n    hidden_layer_1[\"Hidden Layer (512)\"]\n    hidden_layer_2[\"Hidden Layer (256)\"]\n    hidden_layer_3[\"Hidden Layer (64)\"]\n    hidden_layer_4[\"Hidden Layer (256)\"]\n    hidden_layer_5[\"Hidden Layer (512)\"]\n\n    hidden_layer_1 --&gt; hidden_layer_2\n    hidden_layer_2 --&gt; hidden_layer_3\n    hidden_layer_3 --&gt; hidden_layer_4\n    hidden_layer_4 --&gt; hidden_layer_5\nend\n\ninput_layer --&gt; hidden_layers\nhidden_layers --&gt; output_layer</code></pre> <pre><code>from typing import Dict, List\n\nimport dataclasses\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch\n\n\n@dataclasses.dataclass\nclass TSFFNParams:\n    \"\"\"A dataclass to be served as our parameters for the model.\n\n    :param hidden_widths: list of dimensions for the hidden layers\n    \"\"\"\n\n    hidden_widths: List[int]\n\n\nclass TSFeedForward(nn.Module):\n    \"\"\"Feedforward networks for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param ffn_params: the parameters for the FFN network.\n    \"\"\"\n\n    def __init__(\n        self, history_length: int, horizon: int, ffn_params: TSFFNParams\n    ):\n        super().__init__()\n        self.ffn_params = ffn_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.regulate_input = nn.Linear(\n            self.history_length, self.ffn_params.hidden_widths[0]\n        )\n\n        self.hidden_layers = nn.Sequential(\n            *[\n                self._linear_block(dim_in, dim_out)\n                for dim_in, dim_out in\n                zip(\n                    self.ffn_params.hidden_widths[:-1],\n                    self.ffn_params.hidden_widths[1:]\n                )\n            ]\n        )\n\n        self.regulate_output = nn.Linear(\n            self.ffn_params.hidden_widths[-1], self.horizon\n        )\n\n    @property\n    def ffn_config(self) -&gt; Dict:\n        return dataclasses.asdict(self.ffn_params)\n\n    def _linear_block(self, dim_in, dim_out):\n        return nn.Sequential(*[nn.Linear(dim_in, dim_out), nn.ReLU()])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.regulate_input(x)\n        x = self.hidden_layers(x)\n\n        return self.regulate_output(x)\n</code></pre>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.feedforward/#results", "title": "Results", "text": "<p>We take 100 time steps as the input history and forecast 1 time step into the future, but with a gap of 10 time steps.</p> <p></p> <p>Why the Gap</p> <p>Since the differences between each steps are tiny, forecasting immediate next step is quite easy. We add a gap to make the forecasting problem a bit harder.</p> Training <p>The details for model training can be found in this  notebook. We will skip the details but show the loss curve here.</p> <p></p> <p>We plotted the forecasts for a test dataset that was held out from training. The forecasts are plotted in red and the ground truth is plotted in green. For a sense of goodness, we also added the naive forecast (forecasting the last observed value) in blue.</p> <p></p> <p>The feedforward neural network learned the damped sine wave pattern of the pendulum. To quantify the results, we compute a few metrics.</p> Metric FFN Naive Mean Absolute Error 0.017704 0.092666 Mean Squared Error 0.000571 0.010553 Symmetric Mean Absolute Percentage Error 0.010806 0.050442 <p>Since the differences between each time step are small, the naive forecast performs quite well.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.feedforward/#multi-horizon-forecasting", "title": "Multi-horizon Forecasting", "text": "<p>We perform a similar experiment but forecast 3 time steps into the future. We plot out some samples. In the plot, the orange shaded regions are the predictions. From these samples, we observe that the forecasts make sense.</p> <p></p> <p>To observe the quality of the whole time range, we plot out the first forecast step and the corresponding ground truth. The naive forecast plotted in blue has an obvious shift, while the feedforward neural network plotted in red is much closer to the ground truth.</p> <p></p> Metric FFN Naive Mean Absolute Error 0.024640 0.109485 Mean Squared Error 0.001116 0.014723 Symmetric Mean Absolute Percentage Error 0.015637 0.059591", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.flow/", "title": "Forecasting with Flow", "text": "", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.gan/", "title": "Forecasting with GAN", "text": "", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.neural-ode/", "title": "Time Series Forecasting with Neural ODE", "text": "<p> Jupyter Notebook Available</p> <p>We have a  notebook for this section which includes all the code used in this section.</p> <p> Introduction to Neural ODE</p> <p>We explain the theories of neuralode in this section. Please read it first if you are not familiar with neural ode.</p> <p>In the section Neural ODE, we have introduced the concept of neural ODE. In this section, we will show how to use neural ODE to do time series forecasting.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.neural-ode/#a-neural-ode-model", "title": "A Neural ODE Model", "text": "<p>We built a single hidden layer neural network as the field,</p> <pre><code>graph TD\n    input[\"Input (100)\"]\n    input_layer[\"Hidden Layer (100)\"]\n    output_layer[\"Output Layer (100)\"]\n    hidden_layer[\"Hidden Layer (256)\"]\n    output[\"Output (1)\"]\n\n    input --&gt; input_layer\n    input_layer --&gt; hidden_layer\n    hidden_layer --&gt; output_layer\n    output_layer --&gt; output</code></pre> <p>The model is built using the package called torchdyn <sup>1</sup>.</p> <p>Packages</p> <p>Apart from the torchdyn package we used here, there is another package called torchdiffeq <sup>2</sup> which is developed by the authors of neural ode.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.neural-ode/#single-step-forecasts", "title": "Single Step Forecasts", "text": "<p>We trained the model using a history length of 100 and only forecast one step (with a gap of 3 between the input and target). The result is shown below.</p> <p></p> <p>Neural ODE is a good forecaster for our pendulum dataset since the pendulum is simply generated by a differential equation. The metrics are also computed and listed below.</p> Metric Neural ODE Naive Mean Absolute Error 0.003052 0.092666 Mean Squared Error 0.000009 0.010553 Symmetric Mean Absolute Percentage Error 0.021231 0.376550 Training <p>The training loss is shown below.</p> <p></p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.neural-ode/#multi-step-forecasts", "title": "Multi-Step Forecasts", "text": "<p>We perform a similar experiment but forecast 3 steps.</p> <p></p> <p>We plot out some samples and shade the predictions using orange color. The plot below shows that the forecasts are mostly on the right trend.</p> <p></p> Metric Neural ODE Naive Mean Absolute Error 0.038421 0.109485 Mean Squared Error 0.001478 0.014723 Symmetric Mean Absolute Percentage Error 0.153392 0.423563 <ol> <li> <p>Poli M, Massaroli S, Yamashita A, Asama H, Park J, Ermon S. TorchDyn: Implicit Models and Neural Numerical Methods in PyTorch\u00a0\u21a9 <li> <p>Chen RTQ. torchdiffeq\u00a0\u21a9", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.rnn/", "title": "Forecasting with RNN", "text": "<p> Jupyter Notebook Available</p> <p>We have a  notebook for this section which includes all the code used in this section.</p> <p> Introduction to Neural Networks</p> <p>We explain the theories of neural networks in this section. Please read it first if you are not familiar with neural networks.</p> <p>In section Recurrent Neural Network we discussed the basics of RNN. In this section, we will build an RNN model to forecast our pendulum time series data.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.rnn/#rnn-model", "title": "RNN Model", "text": "RNN Model Description RNN Model Code <p>We build an RNN model with an input size of 96, a hidden size of 64 and one single RNN block. We use L1 loss in the trainings.</p> <pre><code>from typing import Dict\n\nimport dataclasses\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport torch\n\n\n@dataclasses.dataclass\nclass TSRNNParams:\n    \"\"\"A dataclass to be served as our parameters for the model.\n\n    :param hidden_size: number of dimensions in the hidden state\n    :param input_size: input dim\n    :param num_layers: number of units stacked\n    \"\"\"\n\n    input_size: int\n    hidden_size: int\n    num_layers: int = 1\n\n\nclass TSRNN(nn.Module):\n    \"\"\"RNN for univaraite time series modeling.\n\n    :param history_length: the length of the input history.\n    :param horizon: the number of steps to be forecasted.\n    :param rnn_params: the parameters for the RNN network.\n    \"\"\"\n\n    def __init__(self, history_length: int, horizon: int, rnn_params: TSRNNParams):\n        super().__init__()\n        self.rnn_params = rnn_params\n        self.history_length = history_length\n        self.horizon = horizon\n\n        self.regulate_input = nn.Linear(\n            self.history_length, self.rnn_params.input_size\n        )\n\n        self.rnn = nn.RNN(\n            input_size=self.rnn_params.input_size,\n            hidden_size=self.rnn_params.hidden_size,\n            num_layers=self.rnn_params.num_layers,\n            batch_first=True\n        )\n\n        self.regulate_output = nn.Linear(\n            self.rnn_params.hidden_size, self.horizon\n        )\n\n    @property\n    def rnn_config(self) -&gt; Dict:\n        return dataclasses.asdict(self.rnn_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.regulate_input(x)\n        x, _ = self.rnn(x)\n\n        return self.regulate_output(x)\n</code></pre>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.rnn/#one-step-forecasting", "title": "One Step Forecasting", "text": "<p>Similar to Forecasting with Feedforward Neural Networks, we take 100 time steps as the input history and forecast 1 time step into the future, but with a gap of 10 time steps.</p> Training <p>The details for model training can be found in this  notebook. We will skip the details but show the loss curve here.</p> <p></p> <p>With just a few seconds of training, our RNN model can capture the pattern of the pendulum time series data.</p> <p></p> <p>The metrics are listed in the following table.</p> Metric RNN Naive Mean Absolute Error 0.007229 0.092666 Mean Squared Error 0.000074 0.010553 Symmetric Mean Absolute Percentage Error 0.037245 0.376550", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.rnn/#multi-horizon-forecasting", "title": "Multi-Horizon Forecasting", "text": "<p>We also trained the same model to forecast 3 steps into the future and also with a gap of 10 time steps.</p> Training <p>The details for model training can be found in this  notebook. We will skip the details but show the loss curve here.</p> <p></p> <p>Visualizing a few examples of the forecasts, it looks reasonable in many cases.</p> <p></p> <p>Similar to the single step forecast, we visualize a specific time step in the forecasts and comparing it to the ground truths. Here we choose to visualize the second time step in the forecasts.</p> <p></p> <p>The metrics are listed in the following table.</p> Metric RNN Naive Mean Absolute Error 0.006714 0.109485 Mean Squared Error 0.000069 0.014723 Symmetric Mean Absolute Percentage Error 0.032914 0.423563", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/", "title": "Transformers for Time Series Forecasting", "text": "<p> Jupyter Notebook Available</p> <p>We have a  notebook for this section which includes all the code used in this section.</p> <p> Introduction to Transformers</p> <p>We explain the theories of transformers in this section. Please read it first if you are not familiar with transformers.</p> <p>Transformer is a good candidate for time series forecasting due to its sequence modeling capability<sup>1</sup><sup>2</sup>. In this section, we will introduce some basic ideas of transformer-based models for time series forecasting.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#transformer-for-univariate-time-series-forecasting", "title": "Transformer for Univariate Time Series Forecasting", "text": "<p>We take a simple Univariate time series forecasting task as an example. There are implementations of transformers for multivariate time series forecasting with all sorts of covariates, but we focus on univariate forecasting problem for simplicity.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#dataset", "title": "Dataset", "text": "<p>In this example, we use the pendulumn physics dataset.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#model", "title": "Model", "text": "<p>We built a naive transformer that only has an encoder. The input is passed to a linear layer to convert the tensor to the shape accepted by the encoder. The tensor is then passed to the encoder. The output of the encoder is passed to another linear layer to convert the tensor to the shape of the output.</p> <pre><code>flowchart TD\n\ninput_linear_layer[Linear Layer for Input]\npositional_encoder[Positional Encoder]\nencoder[Encoder]\noutput_linear_layer[Linear Layer for Output]\n\ninput_linear_layer --&gt; positional_encoder\npositional_encoder --&gt; encoder\nencoder --&gt; output_linear_layer</code></pre> Decoder is Good for Covariates <p>A decoder in a transformer model is good for capturing future covariates. In our problem, we do not have any covariates at all.</p> Positional Encoder <p>In this experiment, we do not include positional encoder as it introduces more complexities but it doesn't help that much in our case<sup>3</sup>.</p>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#evaluations", "title": "Evaluations", "text": "Training <p>The details for model training can be found in this  notebook. We will skip the details but show the loss curve here.</p> <p></p> <p>We trained the model using a history length of 50 and plotted the forecasts for a test dataset that was held out from training. The forecasts are plotted in red and the ground truth is plotted in blue.</p> <p></p> <p>The forecasts roughly captured the patterns of the pendulum. To quantify the results, we compute a few metrics.</p> Metric Vanilla Transformer Naive Mean Absolute Error 0.050232 0.092666 Mean Squared Error 0.003625 0.010553 Symmetric Mean Absolute Percentage Error 0.108245 0.376550", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#multi-horizon-forecasting", "title": "Multi-horizon Forecasting", "text": "<p>We perform a similar experiment for multi-horizon forecasting (horizon=3). We plot out some samples. In the plot, the orange-shaded regions are the predictions.</p> <p></p> <p>To verify that the forecasts make sense, we also plot out a few samples.</p> <p></p> <p>The following is a table of the metrics.</p> Metric Vanilla Transformer Naive Mean Absolute Error 0.057219 0.109485 Mean Squared Error 0.004241 0.014723 Symmetric Mean Absolute Percentage Error 0.112247 0.423563", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.transformer/#generalization", "title": "Generalization", "text": "<p>The vanilla transformer has its limitations. For example, it doesn't capture the correlations between the series that well. There are many variants of transformers that are designed just for time series forecasting<sup>4</sup><sup>5</sup><sup>6</sup><sup>7</sup><sup>8</sup><sup>9</sup>.</p> <p>A few forecasting packages implemented transformers for time series forecasting. For example, the neuralforecast package by Nixtla has implemented TFT, Informer, AutoFormer, FEDFormer, and PatchTST, as of November 2023. An alternative is darts. These packages provide documentation and we encourage the reader to check them out for more complicated use cases of transformer-based models.</p> <ol> <li> <p>Ahmed S, Nielsen IE, Tripathi A, Siddiqui S, Rasool G, Ramachandran RP. Transformers in time-series analysis: A tutorial. 2022. doi:10.1007/s00034-023-02454-8.\u00a0\u21a9</p> </li> <li> <p>Wen Q, Zhou T, Zhang C, Chen W, Ma Z, Yan J et al. Transformers in time series: A survey. 2022.http://arxiv.org/abs/2202.07125.\u00a0\u21a9</p> </li> <li> <p>Zhang Y, Jiang Q, Li S, Jin X, Ma X, Yan X. You may not need order in time series forecasting. arXiv [csLG] 2019.http://arxiv.org/abs/1910.09620.\u00a0\u21a9</p> </li> <li> <p>Lim B, Arik SO, Loeff N, Pfister T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. 2019.http://arxiv.org/abs/1912.09363.\u00a0\u21a9</p> </li> <li> <p>Wu H, Xu J, Wang J, Long M. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. 2021.https://github.com/thuml/Autoformer.\u00a0\u21a9</p> </li> <li> <p>Zhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H et al. Informer: Beyond efficient transformer for long sequence time-series forecasting. 2020.http://arxiv.org/abs/2012.07436.\u00a0\u21a9</p> </li> <li> <p>Nie Y, Nguyen NH, Sinthong P, Kalagnanam J. A time series is worth 64 words: Long-term forecasting with transformers. 2022.http://arxiv.org/abs/2211.14730.\u00a0\u21a9</p> </li> <li> <p>Zhou T, Ma Z, Wen Q, Wang X, Sun L, Jin R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. 2022.http://arxiv.org/abs/2201.12740.\u00a0\u21a9</p> </li> <li> <p>Liu Y, Hu T, Zhang H, Wu H, Wang S, Ma L et al. ITransformer: Inverted transformers are effective for time series forecasting. 2023.http://arxiv.org/abs/2310.06625.\u00a0\u21a9</p> </li> </ol>", "tags": ["WIP"]}, {"location": "time-series-deep-learning/timeseries.vae/", "title": "Forecasting with VAE", "text": "", "tags": ["WIP"]}, {"location": "transformers/transformers.vanilla/", "title": "Vanilla Transformers", "text": "<p>In the seminal paper Attention is All You Need, the legendary transformer architecture was born<sup>3</sup>.</p> <p>Quote from Attention Is All You Need</p> <p>\"... the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\"</p> <p>Transformer has evolved a lot in the past few years and there are a galaxy of variants<sup>4</sup>.</p> <p></p> <p>In this section, we will focus on the vanilla transformer. Jay Alammar wrote an excellent post, named The Illustrated Transformer<sup>1</sup>. We recommend the reader read the post. We won't cover everything in this section. However, for completeness, we will summarize some of the key ideas of transformers.</p> <p>Formal Algorithms</p> <p>For a formal description of the transformer-relevant algorithms, please refer to Phuong &amp; Hutter (2022)<sup>5</sup>.</p>"}, {"location": "transformers/transformers.vanilla/#the-vanilla-transformer", "title": "The Vanilla Transformer", "text": "<p>In the vanilla transformer, we can find three key components: Encoder-Decoder, the attention mechanism, and the positional encoding.</p>"}, {"location": "transformers/transformers.vanilla/#encoder-decoder", "title": "Encoder-Decoder", "text": "<p>It has an encoder-decoder architecture.</p> <p></p> <p>We assume that the input \\(\\mathbf X\\) is already embedded and converted to tensors.</p> <p>The encoder-decoder is simulating the induction-deduction framework of learning. Input \\(\\mathbf X\\) is first encoded into a representation \\(\\hat{\\mathbf X}\\) that should be able to capture the minimal sufficient statistics of the input. Then, the decoder takes this representation of minimal sufficient statistics \\(\\hat{\\mathbf X}\\) and perform deduction to create the output \\(\\hat{\\mathbf Y}\\).</p>"}, {"location": "transformers/transformers.vanilla/#attention", "title": "Attention", "text": "<p>The key to a transformer is its attention mechanism. It utilizes the attention mechanism to look into the relations of the embeddings<sup>3</sup><sup>6</sup>. To understand the attention mechanism, we need to understand the query, key, and value. In essence, the attention mechanism is a classifier that outputs the usefulness of the elements in the value, and the usefulness is represented using a matrix formed by the query and the key.</p> \\[ \\operatorname{Attention}(\\mathbf Q, \\mathbf K, \\mathbf V) = \\operatorname{softmax} \\left( \\frac{\\mathbf Q \\mathbf K^T}{\\sqrt{d_k}} \\right)\\mathbf V, \\] <p>where \\(d_k\\) is the dimension of the key \\(\\mathbf K\\). For example, we can construct the query, key, and value by applying a linear layer to the input \\(\\mathbf X\\).</p> <p>Conventions</p> <p>We follow the convention that the first index of \\(\\mathbf X\\) is the index for the input element. For example, if we have two words as our input, the \\(X_{0j}\\) is the representation of the first word and \\(X_{1j}\\) is that for the second.</p> <p>We also use Einstein notation in this section.</p> Name Definition Component Form Comment Query \\(\\mathbf Q\\) \\(\\mathbf Q=\\mathbf X \\mathbf W^Q\\) \\(Q_{ij} = X_{ik} W^{Q}_{kj}\\) Note that the weights \\(\\mathbf W^Q\\) can be used to adjust the size of the query. Key \\(\\mathbf K\\) \\(\\mathbf K=\\mathbf X \\mathbf W^K\\) \\(K_{ij} = X_{ik} W^{K}_{kj}\\) In the vanilla scaled-dot attention, the dimension of key is the same as the query. This is why \\(\\mathbf Q \\mathbf K^T\\) works. Value \\(\\mathbf V\\) \\(\\mathbf V = \\mathbf X \\mathbf W^V\\) \\(V_{ij} = X_{ik} W^{V}_{kj}\\) <p>The dot product \\(\\mathbf Q \\mathbf K^T\\) is</p> \\[ \\begin{align} \\mathbf A \\equiv \\mathbf Q \\mathbf K^T =&amp; \\mathbf X \\mathbf W^{Q} {\\mathbf W^{K}}^T \\mathbf X^T \\\\ =&amp; X_{im} X_{jn} W^{Q}_{mk} W^{K}_{nk}. \\end{align} \\] <p>which determines how the elements in the value tensor are mixed, \\(A_{ij}V_{jk}\\). For identity \\(\\mathbf A\\), we do not mix the rows of \\(\\mathbf V\\).</p> <p>Classifier</p> <p>The dot-product attention is like a classifier that outputs the usefulness of the elements in \\(\\mathbf V\\). After training, \\(\\mathbf A\\) should be able to make connections between the different input elements.</p> <p>We will provide a detailed example when discussing the applications to time series.</p>"}, {"location": "transformers/transformers.vanilla/#knowledge-of-positions", "title": "Knowledge of Positions", "text": "<p>Positional information, or time order information for time series input, is encoded by a positional encoder that shifts the embeddings. The simplest positional encoder uses the cyclic nature of trig functions<sup>3</sup>. By adding such positional information directly to the values before the data flows into the attention mechanism, we can encode the positional information into the attention mechanism<sup>2</sup>.</p> <ol> <li> <p>Alammar J. The Illustrated Transformer. In: Jay Alammar [Internet]. 27 Jun 2018 [cited 14 Jun 2023]. Available: http://jalammar.github.io/illustrated-transformer/ \u21a9</p> </li> <li> <p>Kazemnejad A. Transformer Architecture: The Positional Encoding. In: Amirhossein Kazemnejad\u2019s Blog [Internet]. 20 Sep 2019 [cited 7 Nov 2023]. Available: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ \u21a9</p> </li> <li> <p>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN et al. Attention is all you need. 2017.http://arxiv.org/abs/1706.03762.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Amatriain X. Transformer models: An introduction and catalog. arXiv [csCL] 2023. doi:10.48550/ARXIV.2302.07730.\u00a0\u21a9</p> </li> <li> <p>Phuong M, Hutter M. Formal algorithms for transformers. 2022. doi:10.48550/ARXIV.2207.09238.\u00a0\u21a9</p> </li> <li> <p>Zhang A, Lipton ZC, Li M, Smola AJ. Dive into deep learning. arXiv preprint arXiv:210611342 2021.\u00a0\u21a9</p> </li> </ol>"}, {"location": "trees/tree.basics/", "title": "Tree-Based Models", "text": "<p>Trees are still powerful machine-learning models for time series forecasting. We explain the basic ideas of trees in the following sections.</p>"}, {"location": "trees/tree.basics/#should-i-work-from-home", "title": "Should I Work from Home?", "text": "<p>We prepared a notebook for this section here .</p> <p>To illustrate the idea of trees, we use a simple classification task: Deciding whether a person will go to the office or work from home based on an artificial dataset.</p>"}, {"location": "trees/tree.basics/#definition-of-the-problem", "title": "Definition of the problem", "text": "<p>We will decide whether one should go to work today. In this demo project, we consider the following features.</p> feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday <p>Our prediction will be a binary result, 0 or 1, with 0 indicates staying at home and 1 indicates going to work.</p> <p>Notations</p> <p>For more compactness, we can use the abstract notation \\(\\{0,1\\}^3\\) to describe a set of three features each with 0 and 1 as possible values. In general, the notation \\(\\{0,1\\}^d\\) indicates \\(d\\) binary features.</p> <p>Meanwhile, the prediction can be denoted as \\(\\{0,1\\}^1\\).</p>"}, {"location": "trees/tree.basics/#how-to-describe-a-decision-tree", "title": "How to Describe a Decision Tree", "text": "<p>In theory, we would expect a decision tree of the following.</p> <pre><code>graph TD\n  A[health] --&gt; |feeling bad| E[stay home]\n  A[health] --&gt; |feeling good| B[weather]\n  B --&gt; |bad weather| E\n  B --&gt; |good weather| C[holiday]\n  C --&gt; |holiday| E\n  C --&gt; |not holiday| G[go to the office]</code></pre> <p>It is straightforward to prove that the max required depths and max required leaves of a model that maps \\(\\{0,1\\}^d\\) to \\(\\{0,1\\}^1\\) are \\(d+1\\) and \\(2^d\\). In our simple example, some of the branches are truncated based on our understanding of the problem. In principle, the branch \"feeling bad\" could also go on to the next level.</p>"}, {"location": "trees/tree.basics/#data", "title": "Data", "text": "<p>However, we are not always lucky enough to be able to forge trees using experience and common sense. It is more common to build the tree using data.</p> Artificial Dataset <p>To fit a model, we generated some artificial data using this notebook.</p> <p>When generating the data, we follow the rule that one only goes to the office, if and only if</p> <ul> <li>the person is healthy,</li> <li>the weather is good, and</li> <li>today is not a holiday.</li> </ul> <p>The following table shows a small sample of the dataset.</p> health weather holiday go_to_office 0 0 0 1 0 1 1 1 1 0 2 1 0 1 0 3 0 0 0 0 4 1 0 1 0"}, {"location": "trees/tree.basics/#build-a-tree", "title": "Build a Tree", "text": "<p>We use sklearn to build a decision tree, see  code here. We observed that the decision tree we get from the data is exactly what we expected.</p> <p></p> Reading the Decision Tree Chart <p>On each node of the tree, we read useful information.</p> <p>In the root, aka the first node on the top, the feature name and value range are denoted on the first row, i.e., weather&lt;= 0.5, which means that we are making decisions based on whether the value of the weather feature is less or equal to 0.5. If the value is less or equal to 0.5, we go to the left branch, otherwise, we go to the right branch. The following rows in the node are assuming the condition is satisfied.</p> <p>On the second row, we read the Gini impurity value. Gini impurity is a measure of the impurity of the data under the condition.</p> <p>On the third row, the number of samples of the given condition (weather &lt;= 0.5) is also given.</p> <p>Finally, we read the values of the samples. In this example, value = [93, 7], i.e., 93 of the samples have a target value 0, and 7 of the samples have a target value 1.</p> <p>This is a perfect result as it is the same as our theoretical expectations. This is because we have built our dataset using the rules. Surely we will get a perfect tree.</p> <p>In reality, our dataset is probabilistic or comes with noise. To see how the noise affects our decision tree, we can build a tree using a perturbed dataset. Here is an example.</p> <p></p> <p>A decision tree trained with a fake \"impure dataset\" with noise that doesn't always fit into our theoretical model. For example, on the leaves, aka, the bottom level, we see some with both going to the office and not going to the office, which corresponds to nonzero Gini impurity value. Though we take the majority target value when doing the predictions, we can already imagine that some of the data points will be misclassified.</p>"}, {"location": "trees/tree.basics/#how-was-the-model-built", "title": "How was the Model Built?", "text": "<p>Many different algorithms can build a decision tree from a given dataset. The Iterative Dichotomizer 3 algorithm, aka ID3 algorithm, is one of the famous implementations of the decision tree<sup>1</sup>. The following is the \"flowchart\" of the algorithm<sup>1</sup>.</p> <pre><code>graph TD\n  Leaf(\"Prepare samples in node\")\n  MajorityVote[\"Calculate majority vote\"]\n  Assign[Assign label to node]\n  Leaf --&gt; MajorityVote --&gt; Assign\n  Assign --&gt; Split1[Split on feature 1]\n  Assign --&gt; Splitdots[\"...\"]\n  Assign --&gt; Splitd[Split on feature d]\n  subgraph \"split on a subset of features\"\n  Split1 --&gt; |\"Split on feature 1\"|B1[\"Calculate gain of split\"]\n  Splitdots --&gt; |\"...\"| Bdots[\"...\"]\n  Splitd --&gt; |\"Split on feature d\"| Bd[\"Calculate gain of split\"]\n  end\n  B1 --&gt; C[\"Use the split with the largest gain\"]\n  Bdots --&gt; C\n  Bd --&gt; C\n  C --&gt; Left[\"Prepare samples in left node\"]\n  C --&gt; Right[\"Prepare samples in right node\"]\n\n  subgraph \"left node\"\n  MajorityVoteL[\"Calculate majority vote\"]\n  AssignL(Assign label to left node)\n  Left --&gt; MajorityVoteL --&gt; AssignL\n  end\n\n  subgraph \"right node\"\n  MajorityVoteR[\"Calculate majority vote\"]\n  Right --&gt;  MajorityVoteR\n  AssignR(Assign label to right node)\n  MajorityVoteR --&gt; AssignR\n  end</code></pre> <p>To \"calculate the gain of the split\", here we use Gini impurity. There are other \"gains\" such as information gain. For regression tasks, we can also have gains such as a MSE loss.</p>"}, {"location": "trees/tree.basics/#overfitting", "title": "Overfitting", "text": "<p>Fully grown trees will most likely to overfit the data since they always try to grow pure leaves. Besides, fully grown trees grow exponentially as the number of features grows which requires a lot of computation resources.</p> <p>Applying Occam's razor, we prefer smaller trees as long as the trees can explain the data well.</p> <p>To achieve this, we will either have to limit how the trees grow during training or prune the trees after the trees are built. Pruning of a tree is achieved by replacing subtrees at a node with a leaf if certain conditions are based on cost estimations.</p> <ol> <li> <p>Shalev-Shwartz S, Ben-David S. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014 doi:10.1017/CBO9781107298019.\u00a0\u21a9\u21a9</p> </li> </ol>"}, {"location": "trees/tree.darts/", "title": "Forecasting with Trees Using Darts", "text": "<p>Darts provides wrappers for tree-based models. In this section, we benchmark random forest and gradient-boosting decison tree (GBDT) on the famous air passenger dataset. Through the benchmarks, we will see the key advantage and disadvantage of tree-based models in forecasting.</p> <p>Just Run It</p> <p>The notebooks created to produce the results in this section can be found here for random forest  and here for gbdt .</p>"}, {"location": "trees/tree.darts/#the-simple-random-forest", "title": "The Simple Random Forest", "text": "<p>We will build different models to demonstrate the strength and weakness of random forest models. The focus will be in-sample and out-of-sample predictions. We know that trees are not quite good at extrapolating into realms where the out-of-sample distribution is different from the training data, due to the constant values assigned on each leaf. Time series forecasting in real world are often non-stationary and heteroscedastic, which implies that the distribution during test phase may be different from the distribution of the training data.</p>"}, {"location": "trees/tree.darts/#data", "title": "Data", "text": "<p>We choose the famous air passenger data. The dataset shows the number of air passengers in each month.</p> <p></p>"}, {"location": "trees/tree.darts/#baseline-model-simply-wrap-the-model-on-the-data", "title": "Baseline Model: \"Simply Wrap the Model on the Data\"", "text": "<p>A naive idea is to simply wrap a tree-based model on the data. Here is choose RandomForest from scikit-learn.</p> <p></p> <p>The predictions are quite off. However, if we look into the in-sample predictions, i.e., time range that the model has already seen during training, we would not have observed such bad predictions.</p> <p></p> <p>This indicates that there are some new patterns in the data to be forecasted. That is the distribution of the data is different, i.e., the level of the values is higher than before, and the variance is also higher. This is a typical case where trees are not good. However, trees can handle such cases if we preprocess the data to bring the data to the same level and also reduce the changes in variance.</p>"}, {"location": "trees/tree.darts/#detrend-but-also-a-clairvoyant-model", "title": "Detrend but also a Clairvoyant Model", "text": "<p>To confirm that this is due to the mismatch of the in-sample distribution and the out-of-sample distribution, we plot out the histograms of the training series and the test series.</p> <p></p> <p>This hints that we should at least detrend the data. In this example, we use a simple moving average while assuming multiplicative components to detrend the data. The detrended data is shown below. Without even training the model, we immediately see that forecasting for such simple patterns is easier than the patterns in the raw data.</p> <p></p> <p>To illustrate that detrending helps, we will cheat a bit to detrend the whole series to confirm that the forecasts are better.</p> <p></p> Distribution of Detrended Data <p></p>"}, {"location": "trees/tree.darts/#a-formal-model-to-use-detrending-and-without-information-leak", "title": "A Formal Model to Use Detrending and without Information Leak", "text": "<p>The above method leads to a great result, however, with information leakage during the detrending. Nevertheless, this indicates the performance of trees on out-of-sample predictions if we only predict on the cycle part of the series. In a real-world case, however, we have to predict the trend accurately for this to work. To better reconstruct the trend, we use Box-Cox transformations to stablize the variance first. The following plot shows the transformed data.</p> <p></p> <p>With the transformed data, we build a simple linear trend using the training dataset and extrapolate the trend to the dates of the prediction.</p> <p></p> <p>Finally, we fit a random forest model on the detrended data, i.e., Box-Cox transformed data - linear trend, then reconstruct the predictions, i.e., predictions + linear trend + Inverse Box-Cox transformation. We observed a much better performance than the first RF we built.</p> <p></p>"}, {"location": "trees/tree.darts/#comparisons-of-the-three-random-forest-models", "title": "Comparisons of the Three Random Forest Models", "text": "<p>Observations by eyes showed that cheating leads to the best result, followed by a simple linear detrend model.</p> <p></p> <p>To formally benchmark the results, we computed several metrics. For most of the metrics, the detrend (cheating) model is the best, which is expected since it is a clairvoyant model peeks into the future. The second best is the box-cox + linear trend model.</p> <p></p>"}, {"location": "trees/tree.darts/#gradient-boosted-trees", "title": "Gradient Boosted Trees", "text": "<p>Similar behavior is also observed for gradient-boosted decision trees (GBDT). We perform exactly the same steps as we did for random forest model. The results are shown below. For GBDT, we also tested the linear tree model too<sup>2</sup>.</p> <p></p> Why Linear Tree <p>For trees, the predictions are flat lines within a bucket of values. For example, we may get the same prediction for a given feature value 10 and 10.1 as the two values are so close to each other. However, without detrending, we expect the predictions to have a uplift trend in our data.</p> <p>To count for this, LightGBM has a parameter called <code>linear_tree</code><sup>2</sup>. This parameter allows the model to fit a linear model on the leaf nodes to capture the trend. This means that we do not need to detrend the data before fitting the model. In this specific example, we see that the linear tree model performs quite well.</p> <p>The benchmark metrics for the GBDT are shown below. As expected, linear tree and box-cox + linear trend models beat the baseline model.</p> <p></p>"}, {"location": "trees/tree.darts/#trees-are-powerful", "title": "Trees are Powerful", "text": "<p>Up to this point, we may get the feeling that trees are not the best choices for forecasting. As a matter of fact, trees are widely used in many competitions and have achieved a lot in forecasting<sup>3</sup>. Apart from being simple and robust, trees can also be made probabilistic. Trees are also attractive as our first model to try because they usually already work quite well out of the box<sup>1</sup>.</p> <ol> <li> <p>\"Out of the box\" sounds like something easy to do. However, if one ever reads the list of parameters of LightGBM, the thought of \"easy\" will immediately diminish.\u00a0\u21a9</p> </li> <li> <p>LightGBM. Parameters \u2014 LightGBM 4.3.0.99 documentation. In: LightGBM [Internet]. [cited 2 Feb 2024]. Available: https://lightgbm.readthedocs.io/en/latest/Parameters.html#linear_tree \u21a9\u21a9</p> </li> <li> <p>Januschowski T, Wang Y, Torkkola K, Erkkil\u00e4 T, Hasson H, Gasthaus J. Forecasting with trees. International journal of forecasting 2022; 38: 1473\u20131481.\u00a0\u21a9</p> </li> </ol>"}, {"location": "trees/tree.gbdt/", "title": "Gradient Boosted Trees", "text": "<p>Boosted trees is another ensemble method of trees. Similar to random forest, boosted trees makes prediction by combining the predictions from each tree. However, instead of performing average, boosted trees are additive models where the prediction \\(f(\\mathbf X)\\) is the additions of each predictions<sup>1</sup>,</p> \\[ f(\\mathbf X) = \\sum_t^T f_t(\\mathbf X), \\] <p>where \\(f_t(\\mathbf X)\\) is the prediction for tree \\(i\\) and \\(T\\) is the total number of trees. Given such a setup, the training becomes very different from random forests. As of 2023, there are two popular implementations of boosted trees, LightGBM and XGBoost. Training a boosted trees model finds a sequence of trees</p> \\[ \\{ f_1, f_2, \\cdots, f_t, \\cdots, f_T \\}. \\] <p>For a specified loss function \\(\\mathscr L(\\mathbf y, \\hat{\\mathbf y})\\), the sequence of trees helps reducing the loss step by step. At step \\(i\\), the loss is</p> \\[ \\mathscr L(y, f_1(\\mathbf X) + f_2(\\mathbf X) + \\cdots + f_i(\\mathbf X) ). \\] <p>To optimize the model, we have to add a tree that reduces the loss the most and approximations are applied for numerical computations<sup>2</sup>.</p> <p>The XGBoost documentation and the original paper on XGBoost explains the idea nicely with examples.</p> <p>Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1603.02754</p> <p>There are more than one realization of gradient boosted trees<sup>3</sup><sup>4</sup>.</p> <ol> <li> <p>Hastie T, Tibshirani R, Friedman J. The elements of statistical learning: Data mining, inference, and prediction. Springer Science   &amp; Business Media, 2013.\u00a0\u21a9</p> </li> <li> <p>Chen T, Guestrin C. XGBoost: A scalable tree boosting system. 2016.http://arxiv.org/abs/1603.02754.\u00a0\u21a9</p> </li> <li> <p>Ke G, Meng Q, Finley T, Wang T, Chen W, Ma W et al. LightGBM: A highly efficient gradient boosting decision tree. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S et al. (eds). Advances in neural information processing systems. Curran Associates, Inc., 2017https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf.\u00a0\u21a9</p> </li> <li> <p>Shi Y, Li J, Li Z. Gradient boosting with Piece-Wise linear regression trees. 2018.http://arxiv.org/abs/1802.05640.\u00a0\u21a9</p> </li> </ol>"}, {"location": "trees/tree.random-forest/", "title": "Random Forest", "text": "<p>From Ho TK. Random decision forests</p> <p>\"The essence of the method is to build multiple trees in randomly selected subspaces of the feature space.\"<sup>1</sup></p> <p>Random forest is an ensemble method based on decision trees which are dubbed as base-learners. Instead of using one single decision tree and model on all the features, we utilize a bunch of decision trees and each tree can model on a subset of features (feature subspace). To make predictions, the results from each tree are combined using some democratization.</p> <p>Translating to math language, given a proper dataset \\(\\mathscr D(\\mathbf X, \\mathbf y)\\), random forest or the ensemble of trees, denoted as \\(\\{f_i\\}\\), will predict an ensemble of results \\(\\{f_i(\\mathbf X_i)\\}\\), with \\(\\mathbf X_i \\subseteq \\mathbf X\\).</p> <p>A Good Reference for Random Forest</p> <p>Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Science &amp; Business Media; 2013. pp. 567\u2013567.</p> <p>However, random forest is not \"just\" ensembling. There are many different ensembling methods, e.g., bootstrapping, but suffer from correlations in the trees. Random forest has two levels of randomization:</p> <ul> <li>Bootstrapping the dataset by randomly selecting a subset of the training data;</li> <li>Random selection of the features to train a tree.</li> </ul> <p>We can already use the bootstrapping step to create many models to ensemble with, however, the randomization of features is also key to a random forest model as it helps reduce the correlations between the trees<sup>2</sup>. In this section, we ask ourselves the following questions.</p> <ol> <li>How to democratize the ensemble of results from each tree?</li> <li>What determines the quality of the predictions?</li> <li>Why does it even work?</li> </ol>"}, {"location": "trees/tree.random-forest/#margin-strength-and-correlations", "title": "Margin, Strength, and Correlations", "text": "<p>The margin of the model, the strength of the trees, and the correlation between the trees can help us understand how random forest work.</p>"}, {"location": "trees/tree.random-forest/#margin", "title": "Margin", "text": "<p>The margin of the tree is defined as<sup>3</sup><sup>4</sup></p> \\[ M(\\mathbf X, \\mathbf y) = {\\color{green}P (\\{f_i(\\mathbf X)=\\mathbf y \\})} - \\operatorname{max}_{\\mathbf j\\neq \\mathbf y} {\\color{red}P (\\{ f_i(\\mathbf X) = \\mathbf j \\})}. \\] <p>Terms in the Margin Definition</p> <p>The first term, \\({\\color{green}P (\\{f_i(\\mathbf X)=\\mathbf y \\})}\\) is the probability of predicting the exact value in the dataset. In a random forest model, it can be calculated using</p> \\[ {\\color{green}P (\\{f_i(\\mathbf X)=\\mathbf y \\})} = \\frac{\\sum_{i=1}^N I(f_i(\\mathbf X) = \\mathbf y)}{N}, \\] <p>where \\(I\\) is the indicator function that maps the correct predictions to 1 and the incorrect predictions to 0. The summation is over all the trees.</p> <p>The term \\({\\color{red}P (\\{f_i(\\mathbf X) = \\mathbf j \\})}\\) is the probability of predicting values \\(\\mathbf j\\). The second term \\(\\operatorname{max}_{\\mathbf j\\neq \\mathbf y} {\\color{red}P ( \\{f_i(\\mathbf X) = \\mathbf j\\})}\\) finds the highest misclassification probabilities, i.e., the max probabilities of predicting values \\(\\mathbf j\\) other than \\(\\mathbf y\\).</p> <p>Raw Margin</p> <p>We can also think of the indicator function itself is also a measure of how well the predictions are. Instead of looking into the whole forest and probabilities, the raw margin of a single tree is defined as<sup>3</sup></p> \\[ M_{R,i}(\\mathbf X, \\mathbf y) = I (f_i(\\mathbf X)=\\mathbf y ) - \\operatorname{max}_{\\mathbf j\\neq \\mathbf y} I ( f_i(\\mathbf X) = \\mathbf j ). \\] <p>The margin is the expected value of this raw margin over each classifier.</p> <p>To make it easier to interpret this quantity, we only consider two possible predictions:</p> <ul> <li>\\(M(\\mathbf X, \\mathbf y) \\to 1\\): We will always predict the true value, for all the trees.</li> <li>\\(M(\\mathbf X, \\mathbf y) \\to -1\\): We will always predict the wrong value, for all the trees.</li> <li>\\(M(\\mathbf X, \\mathbf y) \\to 0\\), we have an equal probability of predicting the correct value and the wrong value.</li> </ul> <p>In general, we prefer a model with higher \\(M(\\mathbf X, \\mathbf y)\\).</p>"}, {"location": "trees/tree.random-forest/#strength", "title": "Strength", "text": "<p>However, the margin of the same model is different in different problems. The same model for one problem may give us margin 1 but it might not work that well for a different problem. This can be seen in our decision tree examples.</p> <p>To bring the idea of margin to a specific problem, Breiman defined the strength \\(s\\) as the expected value of the margin over the dataset fed into the trees<sup>3</sup><sup>4</sup>,</p> \\[ s = E_{\\mathscr D}[M(\\mathbf X, \\mathbf y)]. \\] <p>Dataset Fed into the Trees</p> <p>This may be different in different models since there are different randomization and data selection methods. For example, in bagging, the dataset fed into the trees would be random selections of the training data.</p>"}, {"location": "trees/tree.random-forest/#correlation", "title": "Correlation", "text": "<p>Naively speaking, we expect each tree takes care of different factors and spit out a different result, for ensembling to provide benefits. To quantify this idea, we define the correlation of raw margin between the trees<sup>3</sup></p> \\[ \\rho_{ij} = \\operatorname{corr}(M_{R,i}, M_{R,j}) = \\frac{\\operatorname{cov}(M_{R,i}, M_{R,j})}{\\sigma_{M_{R,i}} \\sigma_{M_{R,j}}}  = \\frac{E[(M_{R,i} - \\bar M_{R,i})(M_{R,j} - \\bar M_{R,j})]}{\\sigma_{M_{R,i}} \\sigma_{M_{R,j}}}. \\] <p>Since the raw margin tells us how likely we can predict the correct value, the correlation defined above indicates how likely two trees are functioning. If all trees are similar, the correlation is high, and ensembling won't provide much in this situation.</p> <p>To get a scalar value of the whole model, the average correlation \\(\\bar \\rho\\) over all the possible pairs is calculated.</p>"}, {"location": "trees/tree.random-forest/#predicting-power", "title": "Predicting Power", "text": "<p>The higher the generalization power, the better the model is at new predictions. To measure the goodness of a random forest, the population error can be used,</p> \\[ P_{err} = P_{\\mathscr D}(M(\\mathbf X, \\mathbf y)&lt; 0). \\] <p>It has been proved that the error almost converges in the random forest as the number of trees gets large<sup>3</sup>. The upper bound of the population error is related to the strength and the mean correlation<sup>3</sup>,</p> \\[ P_{err} \\leq \\frac{\\bar \\rho (1-s^2) }{s^2}. \\] <p>To get a grasp of this upper bound, we plot out the heatmap as a function of \\(\\bar \\rho\\) and \\(s\\).</p> <p></p> <p>We observe that</p> <ol> <li>The stronger the strength, the lower the population error upper bound.</li> <li>The smaller the correlation, the lower the population error upper bound.</li> <li>If the strength is too low, it is very hard for the model to avoid errors.</li> <li>If the correlation is very high, it is still possible to get a decent model if the strength is high.</li> </ol>"}, {"location": "trees/tree.random-forest/#random-forest-regressor", "title": "Random Forest Regressor", "text": "<p>Similar to decision trees, random forest can also be used as regressors. The random forest regressor population error is capped by the average population error of trees multiplied by the correlation of trees<sup>3</sup>.</p> <p>To see how the regressor works with data, we construct an artificial problem. The code can be accessed here .</p> Sinusoid DataSinusoid Data with NoiseComparing Tow Scenarios <p>A random forest with 1600 estimators can estimate the following sin data. Note that this is in-sample fitting and prediction to demonstrate the capability of representing sin data.</p> <p></p> <p>One observation is that not all the trees spit out the same values. We observe some quite dispersed predictions from the trees but the ensemble result is very close to the true values.</p> <p></p> <p>We generate a new dataset by adding some noise to the sin dataset. By adding uniform random noise, we introduce some variance but not much bias in the data. We are cheating a bit here because this kind of data is what random forest is good at.</p> <p>We train a random forest model with 1300 estimators using this noise data. Note that this is in-sample fitting and prediction to demonstrate the representation capability.</p> <p></p> <p>One observation is that not all the trees spit out the same values. The predictions from the trees are sometimes dispersed and not even bell-like, the ensemble result reflects the values of the true sin data. The ensemble results are even located at the center of the noisy data where the true sin values should be. However, we will see that the distribution of the predictions is more dispersed than the model trained without noise (see the tab \"Comparing Tow Scenarios\").</p> <p></p> <p>The following two charts show the boxes for the two trainings.</p> <p></p> <p></p> <p>To see the differences between the box sizes for in a more quantitive way, we plot out the box plot of the box sizes for each training.</p> <p></p> <ol> <li> <p>Ho TK. Random decision forests. In: Proceedings of 3rd international conference on document analysis and recognition. 1995, pp 278\u2013282 vol.1.\u00a0\u21a9</p> </li> <li> <p>Hastie T, Tibshirani R, Friedman J. The elements of statistical learning: Data mining, inference, and prediction. Springer Science   &amp; Business Media, 2013.\u00a0\u21a9</p> </li> <li> <p>Breiman L. Random forests. Machine learning 2001; 45: 5\u201332.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Bernard S, Heutte L, Adam S. A study of strength and correlation in random forests. In: Advanced intelligent computing theories and applications. Springer Berlin Heidelberg, 2010, pp 186\u2013191.\u00a0\u21a9\u21a9</p> </li> </ol>"}, {"location": "utilities/notebooks-and-utilities/", "title": "Notebooks and Utilities for Tutorials", "text": "<p>All the notebooks are located in the folder <code>dl/notebooks</code>. To run these notebooks, we need to set up our Python environment first. We use poetry to manage our Python environment. For the argument of this choice, please refer to Engineering Tips.</p>"}, {"location": "utilities/notebooks-and-utilities/#install-requirements-and-create-jupyter-kernel", "title": "Install Requirements and Create Jupyter Kernel", "text": "<p>First of all, we need to install all the requirements,</p> <pre><code>poetry install\n</code></pre> <p>or install certain groups using</p> <pre><code>poetry install --with notebook,visualization,torch,darts\n</code></pre> <p>To create a Jupyter kernel for the notebooks, run</p> <pre><code>poetry run ipython kernel install --user --name=deep-learning\n</code></pre> <p>and a Jupyter kernel named <code>deep-learning</code> will be created.</p>"}, {"location": "utilities/notebooks-and-utilities/#utilities", "title": "Utilities", "text": "<p>We have a few utilities that we use in our tutorials. Most of them are located in the package <code>ts_dl_utils</code> located in the folder <code>dl/notebooks/ts_dl_utils</code>.</p> <p>In principle, the notebooks we provided should work without installing this package. The package is also installed in the environment if you run <code>poetry install</code> just in case one uses the kernel <code>deep-learning</code> created above to run some personal notebooks located in other folders.</p>"}, {"location": "tags/", "title": "Tags", "text": "<p>Following is a list of tags:</p>"}, {"location": "tags/#wip", "title": "WIP", "text": "<ul> <li>Convolutional Neural Networks</li> <li>f-GAN</li> <li>GAN</li> <li>InfoGAN</li> <li>Adversarial Models</li> <li>Contrastive Predictive Coding</li> <li>Deep Infomax</li> <li>Introduction</li> <li>AE</li> <li>Autoregressive</li> <li>Flow</li> <li>Introduction</li> <li>MADE</li> <li>MAF</li> <li>VAE</li> <li>Creating Synthetic Dataset</li> <li>Forecasting with CNN</li> <li>Pendulum Dataset</li> <li>Forecasting with Diffusion Models</li> <li>Forecasting with MLP</li> <li>Forecasting with Flow</li> <li>Forecasting with GAN</li> <li>Forecasting with Neural ODE</li> <li>Forecasting with RNN</li> <li>Forecasting with Transformers</li> <li>Forecasting with VAE</li> </ul>"}]}