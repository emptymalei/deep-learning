@online{Liu2020-yh,
title         = "Self-supervised Learning: Generative or Contrastive",
author        = "Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu
                 and Mian, Li and Zhang, Jing and Tang, Jie",
abstract      = "Deep supervised learning has achieved great success in the
                 last decade. However, its deficiencies of dependence on
                 manual labels and vulnerability to attacks have driven
                 people to explore a better solution. As an alternative,
                 self-supervised learning attracts many researchers for its
                 soaring performance on representation learning in the last
                 several years. Self-supervised representation learning
                 leverages input data itself as supervision and benefits
                 almost all types of downstream tasks. In this survey, we
                 take a look into new self-supervised learning methods for
                 representation in computer vision, natural language
                 processing, and graph learning. We comprehensively review
                 the existing empirical methods and summarize them into three
                 main categories according to their objectives: generative,
                 contrastive, and generative-contrastive (adversarial). We
                 further investigate related theoretical analysis work to
                 provide deeper thoughts on how self-supervised learning
                 works. Finally, we briefly discuss open problems and future
                 directions for self-supervised learning. An outline slide
                 for the survey is provided.",
month         =  "15~" # jun,
year          =  2020,
url           = "http://arxiv.org/abs/2006.08218",
file          = "All Papers/LIU/Liu et al. 2020 - Self-supervised Learning - Generative or Contrastive.pdf",
archivePrefix = "arXiv",
eprint        = "2006.08218",
primaryClass  = "cs.LG",
arxivid       = "2006.08218"
}


@online{Chen2018-mp,
  title         = "Neural Ordinary Differential Equations",
  author        = "Chen, Ricky T Q and Rubanova, Yulia and Bettencourt, Jesse
                   and Duvenaud, David",
  abstract      = "We introduce a new family of deep neural network models.
                   Instead of specifying a discrete sequence of hidden layers,
                   we parameterize the derivative of the hidden state using a
                   neural network. The output of the network is computed using
                   a black-box differential equation solver. These
                   continuous-depth models have constant memory cost, adapt
                   their evaluation strategy to each input, and can explicitly
                   trade numerical precision for speed. We demonstrate these
                   properties in continuous-depth residual networks and
                   continuous-time latent variable models. We also construct
                   continuous normalizing flows, a generative model that can
                   train by maximum likelihood, without partitioning or
                   ordering the data dimensions. For training, we show how to
                   scalably backpropagate through any ODE solver, without
                   access to its internal operations. This allows end-to-end
                   training of ODEs within larger models.",
  month         =  "19~" # jun,
  year          =  2018,
  url           = "http://arxiv.org/abs/1806.07366",
  file          = "All Papers/CHEN/Chen et al. 2018 - Neural Ordinary Differential Equations.pdf",
  archivePrefix = "arXiv",
  eprint        = "1806.07366",
  primaryClass  = "cs.LG",
  arxivid       = "1806.07366"
}

@online{Le_Cun2006-ta,
  title  = "A Tutorial on {Energy-Based} Learning",
  author = "Le Cun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato,
            Marc'aurelio and Huang, Fu Jie",
  year   =  2006,
  file   = "All Papers/LE CUN/Le Cun et al. 2006 - A Tutorial on Energy-Based Learning.pdf"
}


@online{Luo2022-hz,
  title         = "Understanding Diffusion Models: A Unified Perspective",
  author        = "Luo, Calvin",
  abstract      = "Diffusion models have shown incredible capabilities as
                   generative models; indeed, they power the current
                   state-of-the-art models on text-conditioned image generation
                   such as Imagen and DALL-E 2. In this work we review,
                   demystify, and unify the understanding of diffusion models
                   across both variational and score-based perspectives. We
                   first derive Variational Diffusion Models (VDM) as a special
                   case of a Markovian Hierarchical Variational Autoencoder,
                   where three key assumptions enable tractable computation and
                   scalable optimization of the ELBO. We then prove that
                   optimizing a VDM boils down to learning a neural network to
                   predict one of three potential objectives: the original
                   source input from any arbitrary noisification of it, the
                   original source noise from any arbitrarily noisified input,
                   or the score function of a noisified input at any arbitrary
                   noise level. We then dive deeper into what it means to learn
                   the score function, and connect the variational perspective
                   of a diffusion model explicitly with the Score-based
                   Generative Modeling perspective through Tweedie's Formula.
                   Lastly, we cover how to learn a conditional distribution
                   using diffusion models via guidance.",
  month         =  "25~" # aug,
  year          =  2022,
  url           = "http://arxiv.org/abs/2208.11970",
  file          = "All Papers/LUO/Luo 2022 - Understanding Diffusion Models - A Unified Perspective.pdf",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2208.11970",
  primaryClass  = "cs.LG",
  arxivid       = "2208.11970"
}



@online{Sohl-Dickstein2015-th,
  title         = "Deep Unsupervised Learning using Nonequilibrium
                   Thermodynamics",
  author        = "Sohl-Dickstein, Jascha and Weiss, Eric A and
                   Maheswaranathan, Niru and Ganguli, Surya",
  abstract      = "A central problem in machine learning involves modeling
                   complex data-sets using highly flexible families of
                   probability distributions in which learning, sampling,
                   inference, and evaluation are still analytically or
                   computationally tractable. Here, we develop an approach that
                   simultaneously achieves both flexibility and tractability.
                   The essential idea, inspired by non-equilibrium statistical
                   physics, is to systematically and slowly destroy structure
                   in a data distribution through an iterative forward
                   diffusion process. We then learn a reverse diffusion process
                   that restores structure in data, yielding a highly flexible
                   and tractable generative model of the data. This approach
                   allows us to rapidly learn, sample from, and evaluate
                   probabilities in deep generative models with thousands of
                   layers or time steps, as well as to compute conditional and
                   posterior probabilities under the learned model. We
                   additionally release an open source reference implementation
                   of the algorithm.",
  month         =  "12~" # mar,
  year          =  2015,
  url           = "http://arxiv.org/abs/1503.03585",
  file          = "All Papers/SOHL-DICKSTEIN/Sohl-Dickstein et al. 2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf",
  archivePrefix = "arXiv",
  eprint        = "1503.03585",
  primaryClass  = "cs.LG",
  arxivid       = "1503.03585"
}


@online{Ho2020-er,
  title         = "Denoising Diffusion Probabilistic Models",
  author        = "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
  abstract      = "We present high quality image synthesis results using
                   diffusion probabilistic models, a class of latent variable
                   models inspired by considerations from nonequilibrium
                   thermodynamics. Our best results are obtained by training on
                   a weighted variational bound designed according to a novel
                   connection between diffusion probabilistic models and
                   denoising score matching with Langevin dynamics, and our
                   models naturally admit a progressive lossy decompression
                   scheme that can be interpreted as a generalization of
                   autoregressive decoding. On the unconditional CIFAR10
                   dataset, we obtain an Inception score of 9.46 and a
                   state-of-the-art FID score of 3.17. On 256x256 LSUN, we
                   obtain sample quality similar to ProgressiveGAN. Our
                   implementation is available at
                   https://github.com/hojonathanho/diffusion",
  month         =  "19~" # jun,
  year          =  2020,
  url           = "http://arxiv.org/abs/2006.11239",
  archivePrefix = "arXiv",
  eprint        = "2006.11239",
  primaryClass  = "cs.LG",
  arxivid       = "2006.11239"
}

@book{shalev-shwartz_ben-david_2014, place={Cambridge}, title={Understanding Machine Learning: From Theory to Algorithms}, DOI={10.1017/CBO9781107298019}, publisher={Cambridge University Press}, author={Shalev-Shwartz, Shai and Ben-David, Shai}, year={2014}}


@PHDTHESIS{Roelofs2019-dm,
  title    = "Measuring Generalization and Overfitting in Machine Learning",
  author   = "Roelofs, Rebecca",
  abstract = "Author(s): Roelofs, Rebecca | Advisor(s): Recht, Benjamin;
              Demmel, James | Abstract: Due to the prevalence of machine
              learning algorithms and the potential for their decisions to
              profoundly impact billions of human lives, it is crucial that
              they are robust, reliable, and understandable. This thesis
              examines key theoretical pillars of machine learning surrounding
              generalization and overfitting, and tests the extent to which
              empirical behavior matches existing theory. We develop novel
              methods for measuring overfitting and generalization, and we
              characterize how reproducible observed behavior is across
              differences in optimization algorithm, dataset, task, evaluation
              metric, and domain.First, we examine how optimization algorithms
              bias machine learning models towards solutions with varying
              generalization properties. We show that adaptive gradient methods
              empirically find solutions with inferior generalization behavior
              compared to those found by stochastic gradient descent. We then
              construct an example using a simple overparameterized model that
              corroborates the algorithms' empirical behavior on neural
              networks. Next, we study the extent to which machine learning
              models have overfit to commonly reused datasets in both academic
              benchmarks and machine learning competitions. We build new test
              sets for the CIFAR-10 and ImageNet datasets and evaluate a broad
              range of classification models on the new datasets. All models
              experience a drop in accuracy, which indicates that current
              accuracy numbers are susceptible to even minute natural
              variations in the data distribution. Surprisingly, despite
              several years of adaptively selecting the models to perform well
              on these competitive benchmarks, we find no evidence of
              overfitting. We then analyze data from the machine learning
              platform Kaggle and find little evidence of substantial
              overfitting in ML competitions. These findings speak to the
              robustness of the holdout method across different data domains,
              loss functions, model classes, and human analysts.Overall, our
              work suggests that the true concern for robust machine learning
              is distribution shift rather than overfitting, and designing
              models that still work reliably in dynamic environments is a
              challenging but necessary undertaking.",
  year     =  2019,
  url      = "https://escholarship.org/uc/item/6j01x9mz",
  file     = "All Papers/ROELOFS/Roelofs 2019 - Measuring Generalization and Overfitting in Machine Learning.pdf",
  school   = "UC Berkeley"
}


@BOOK{Hastie2013-tt,
  title     = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract  = "During the past decade there has been an explosion in
               computation and information technology. With it have come vast
               amounts of data in a variety of fields such as medicine,
               biology, finance, and marketing. The challenge of understanding
               these data has led to the development of new tools in the field
               of statistics, and spawned new areas such as data mining,
               machine learning, and bioinformatics. Many of these tools have
               common underpinnings but are often expressed with different
               terminology. This book describes the important ideas in these
               areas in a common conceptual framework. While the approach is
               statistical, the emphasis is on concepts rather than
               mathematics. Many examples are given, with a liberal use of
               color graphics. It is a valuable resource for statisticians and
               anyone interested in data mining in science or industry. The
               book's coverage is broad, from supervised learning (prediction)
               to unsupervised learning. The many topics include neural
               networks, support vector machines, classification trees and
               boosting---the first comprehensive treatment of this topic in
               any book. This major new edition features many topics not
               covered in the original, including graphical models, random
               forests, ensemble methods, least angle regression \& path
               algorithms for the lasso, non-negative matrix factorization, and
               spectral clustering. There is also a chapter on methods for
               ``wide'' data (p bigger than n), including multiple testing and
               false discovery rates. Trevor Hastie, Robert Tibshirani, and
               Jerome Friedman are professors of statistics at Stanford
               University. They are prominent researchers in this area: Hastie
               and Tibshirani developed generalized additive models and wrote a
               popular book of that title. Hastie co-developed much of the
               statistical modeling software and environment in R/S-PLUS and
               invented principal curves and surfaces. Tibshirani proposed the
               lasso and is co-author of the very successful An Introduction to
               the Bootstrap. Friedman is the co-inventor of many data-mining
               tools including CART, MARS, projection pursuit and gradient
               boosting.",
  publisher = "Springer Science \& Business Media",
  volume    =  99,
  pages     = "567--567",
  month     =  "11~" # nov,
  year      =  2013,
  url       = "https://play.google.com/store/books/details?id=yPfZBwAAQBAJ",
  file      = "All Papers/HASTIE/Hastie et al. 2013 - The Elements of Statistical Learning - Data Mining, Inference, and Prediction.pdf",
  language  = "en",
  isbn      = "9780387216065"
}

@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}


@BOOK{Vapnik2006-pn,
  title     = "Estimation of dependences based on empirical data",
  author    = "Vapnik, Vladimir",
  publisher = "Springer",
  address   = "New York, NY",
  edition   =  1,
  series    = "Information science and statistics",
  month     =  "1~" # mar,
  year      =  2006,
  url       = "https://sci-hub.se/https://link.springer.com/book/10.1007/0-387-34239-7",
  file      = "All Papers/VAPNIK/Vapnik 2006 - Estimation of dependences based on empirical data.pdf;All Papers/VAPNIK/Vapnik 2006 - Estimation of dependences based on empirical data.pdf",
  doi       = "10.1007/0-387-34239-7",
  isbn      = "9780387308654,9780387342399",
  issn      = "1613-9011,2197-4128",
  language  = "en"
}

@BOOK{Vapnik2010-xf,
  title     = "The nature of statistical learning theory",
  author    = "Vapnik, Vladimir",
  publisher = "Springer",
  address   = "New York, NY",
  series    = "Information Science and Statistics",
  month     =  "21~" # oct,
  year      =  2010,
  url       = "https://www.springer.com/gp/book/9780387987804",
  file      = "All Papers/VAPNIK/Vapnik 2010 - The nature of statistical learning theory.pdf",
  doi       = "10.1007/978-1-4757-3264-1",
  isbn      = "9781441931603,9781475732641",
  language  = "en"
}

@BOOK{Abu-Mostafa2012-rk,
  title     = "Learning from data: A short course",
  author    = "Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien",
  publisher = "AMLBook",
  month     =  "1~" # jan,
  year      =  2012,
  url       = "https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698",
  file      = "All Papers/ABU-MOSTAFA/Abu-Mostafa et al. 2012 - Learning from data - A short course.pdf",
  isbn      =  9781600490064,
  language  = "en"
}

@ARTICLE{Domingos2012-wn,
  title     = "A few useful things to know about machine learning",
  author    = "Domingos, Pedro",
  journal   = "Communications of the ACM",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  55,
  number    =  10,
  pages     = "78--87",
  abstract  = "Tapping into the ``folk knowledge'' needed to advance machine
               learning applications.",
  month     =  oct,
  year      =  2012,
  url       = "https://dl.acm.org/doi/10.1145/2347736.2347755",
  file      = "All Papers/DOMINGOS/Domingos 2012 - A few useful things to know about machine learning.pdf",
  doi       = "10.1145/2347736.2347755",
  issn      = "0001-0782,1557-7317",
  language  = "en"
}


@BOOK{Hassoun2021-sh,
  title     = "Fundamentals of Artificial Neural Networks",
  author    = "Hassoun, Mohamad",
  publisher = "The MIT Press, Massachusetts Institute of Technology",
  abstract  = "Hassoun provides the first systematic account of artificial
               neural network paradigms by identifying clearly the fundamental
               concepts and major methodologies ...",
  month     =  "1~" # dec,
  year      =  2021,
  url       = "https://mitpress.mit.edu/9780262514675/fundamentals-of-artificial-neural-networks/",
  language  = "en"
}

@ARTICLE{Hornik1989-nc,
  title     = "Multilayer feedforward networks are universal approximators",
  author    = "Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert",
  journal   = "Neural networks: the official journal of the International Neural
               Network Society",
  publisher = "Elsevier BV",
  volume    =  2,
  number    =  5,
  pages     = "359--366",
  abstract  = "This paper rigorously establishes that standard multilayer
               feedforward networks with as few as one hidden layer using
               arbitrary squashing functions are capable of approximating any
               Borel measurable function from one finite dimensional space to
               another to any desired degree of accuracy, provided sufficiently
               many hidden units are available. In this sense, multilayer
               feedforward networks are a class of universal approximators.",
  month     =  jan,
  year      =  1989,
  url       = "https://linkinghub.elsevier.com/retrieve/pii/0893608089900208",
  file      = "All Papers/HORNIK/Hornik et al. 1989 - Multilayer feedforward networks are universal approximators.pdf",
  doi       = "10.1016/0893-6080(89)90020-8",
  issn      = "0893-6080,1879-2782",
  language  = "en"
}

@ARTICLE{Cybenko1989-ro,
  title    = "Approximation by superpositions of a sigmoidal function",
  author   = "Cybenko, G",
  journal  = "Mathematics of Control, Signals, and Systems",
  volume   =  2,
  number   =  4,
  pages    = "303--314",
  abstract = "In this paper we study the degree of approximation by
              superpositions of a sigmoidal function. We mainly consider the
              univariate case. If f is a continuous function, we prove that for
              any bounded sigmoidal function σ, {Mathematical expression}. For
              the Heaviside function H(x), we prove that {Mathematical
              expression}. If f is a continuous function of bounded variation,
              we prove that {Mathematical expression} and {Mathematical
              expression}. For he Heaviside function, the coefficient 1 and the
              approximation orders are the best possible. We compare these
              results with the classical Jackson and Bernstein theorems, and
              make some conjectures for further study. © 1993 Springer.",
  month    =  dec,
  year     =  1989,
  url      = "http://link.springer.com/10.1007/BF02551274",
  file     = "All Papers/CYBENKO/Cybenko 1989 - Approximation by superpositions of a sigmoidal function.pdf",
  keywords = "approximation; completeness; neural networks",
  doi      = "10.1007/BF02551274",
  issn     = "0932-4194"
}


@online{Srivastava2015-te,
  title         = "Highway Networks",
  author        = "Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber,
                   Jürgen",
  abstract      = "There is plenty of theoretical and empirical evidence that
                   depth of neural networks is a crucial ingredient for their
                   success. However, network training becomes more difficult
                   with increasing depth and training of very deep networks
                   remains an open problem. In this extended abstract, we
                   introduce a new architecture designed to ease gradient-based
                   training of very deep networks. We refer to networks with
                   this architecture as highway networks, since they allow
                   unimpeded information flow across several layers on
                   ``information highways''. The architecture is characterized
                   by the use of gating units which learn to regulate the flow
                   of information through a network. Highway networks with
                   hundreds of layers can be trained directly using stochastic
                   gradient descent and with a variety of activation functions,
                   opening up the possibility of studying extremely deep and
                   efficient architectures.",
  month         =  "2~" # may,
  year          =  2015,
  url           = "http://arxiv.org/abs/1505.00387",
  file          = "All Papers/SRIVASTAVA/Srivastava et al. 2015 - Highway Networks.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1505.00387"
}


@online{He2015-ie,
  title         = "Deep Residual Learning for Image Recognition",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Deeper neural networks are more difficult to train. We
                   present a residual learning framework to ease the training of
                   networks that are substantially deeper than those used
                   previously. We explicitly reformulate the layers as learning
                   residual functions with reference to the layer inputs,
                   instead of learning unreferenced functions. We provide
                   comprehensive empirical evidence showing that these residual
                   networks are easier to optimize, and can gain accuracy from
                   considerably increased depth. On the ImageNet dataset we
                   evaluate residual nets with a depth of up to 152 layers---8x
                   deeper than VGG nets but still having lower complexity. An
                   ensemble of these residual nets achieves 3.57\% error on the
                   ImageNet test set. This result won the 1st place on the
                   ILSVRC 2015 classification task. We also present analysis on
                   CIFAR-10 with 100 and 1000 layers. The depth of
                   representations is of central importance for many visual
                   recognition tasks. Solely due to our extremely deep
                   representations, we obtain a 28\% relative improvement on the
                   COCO object detection dataset. Deep residual nets are
                   foundations of our submissions to ILSVRC \& COCO 2015
                   competitions, where we also won the 1st places on the tasks
                   of ImageNet detection, ImageNet localization, COCO detection,
                   and COCO segmentation.",
  month         =  "10~" # dec,
  year          =  2015,
  url           = "http://arxiv.org/abs/1512.03385",
  file          = "All Papers/HE/He et al. 2015 - Deep Residual Learning for Image Recognition.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.03385"
}


@incollection{Hochreiter2008,
  added-at = {2008-02-26T12:05:08.000+0100},
  author = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
  biburl = {https://www.bibsonomy.org/bibtex/279df6721c014a00bfac62abd7d5a9968/schaul},
  booktitle = {A Field Guide to Dynamical Recurrent Neural Networks},
  citeulike-article-id = {2374777},
  description = {idsia},
  editor = {Kremer, S. C. and Kolen, J. F.},
  interhash = {485c1bd6a99186c9414c6b9ddaed42c9},
  intrahash = {79df6721c014a00bfac62abd7d5a9968},
  keywords = {daanbib},
  priority = {2},
  publisher = {IEEE Press},
  timestamp = {2008-02-26T12:07:01.000+0100},
  title = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  year = 2001
}

@online{Huang2016-uz,
  title         = "Deep networks with stochastic depth",
  author        = "Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and
                   Weinberger, Kilian",
  abstract      = "Very deep convolutional networks with hundreds of layers have
                   led to significant reductions in error on competitive
                   benchmarks. Although the unmatched expressiveness of the many
                   layers can be highly desirable at test time, training very
                   deep networks comes with its own set of challenges. The
                   gradients can vanish, the forward flow often diminishes, and
                   the training time can be painfully slow. To address these
                   problems, we propose stochastic depth, a training procedure
                   that enables the seemingly contradictory setup to train short
                   networks and use deep networks at test time. We start with
                   very deep networks but during training, for each mini-batch,
                   randomly drop a subset of layers and bypass them with the
                   identity function. This simple approach complements the
                   recent success of residual networks. It reduces training time
                   substantially and improves the test error significantly on
                   almost all data sets that we used for evaluation. With
                   stochastic depth we can increase the depth of residual
                   networks even beyond 1200 layers and still yield meaningful
                   improvements in test error (4.91\% on CIFAR-10).",
  month         =  "30~" # mar,
  year          =  2016,
  url           = "http://arxiv.org/abs/1603.09382",
  file          = "All Papers/HUANG/Huang et al. 2016 - Deep networks with stochastic depth.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1603.09382"
}


@ARTICLE{Grossberg:2013,
AUTHOR = {Grossberg, S. },
TITLE   = {{R}ecurrent neural networks},
YEAR    = {2013},
JOURNAL = {Scholarpedia},
VOLUME  = {8},
NUMBER  = {2},
PAGES   = {1888},
DOI     = {10.4249/scholarpedia.1888},
NOTE    = {revision \#138057}
}

@ARTICLE{Hochreiter1997-gm,
  title     = "Long short-term memory",
  author    = "Hochreiter, S and Schmidhuber, J",
  journal   = "Neural computation",
  publisher = "MIT Press - Journals",
  volume    =  9,
  number    =  8,
  pages     = "1735--1780",
  abstract  = "Learning to store information over extended time intervals by
               recurrent backpropagation takes a very long time, mostly because
               of insufficient, decaying error backflow. We briefly review
               Hochreiter's (1991) analysis of this problem, then address it by
               introducing a novel, efficient, gradient-based method called long
               short-term memory (LSTM). Truncating the gradient where this does
               not do harm, LSTM can learn to bridge minimal time lags in excess
               of 1000 discrete-time steps by enforcing constant error flow
               through constant error carousels within special units.
               Multiplicative gate units learn to open and close access to the
               constant error flow. LSTM is local in space and time; its
               computational complexity per time step and weight is O(1). Our
               experiments with artificial data involve local, distributed,
               real-valued, and noisy pattern representations. In comparisons
               with real-time recurrent learning, back propagation through time,
               recurrent cascade correlation, Elman nets, and neural sequence
               chunking, LSTM leads to many more successful runs, and learns
               much faster. LSTM also solves complex, artificial long-time-lag
               tasks that have never been solved by previous recurrent network
               algorithms.",
  month     =  "15~" # nov,
  year      =  1997,
  url       = "http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735",
  file      = "All Papers/HOCHREITER/Hochreiter and Schmidhuber 1997 - Long short-term memory.pdf",
  doi       = "10.1162/neco.1997.9.8.1735",
  pmid      =  9377276,
  issn      = "0899-7667,1530-888X",
  language  = "en"
}

@online{Pascanu2012-qv,
  title         = "On the difficulty of training Recurrent Neural Networks",
  author        = "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
  abstract      = "There are two widely known issues with properly training
                   Recurrent Neural Networks, the vanishing and the exploding
                   gradient problems detailed in Bengio et al. (1994). In this
                   paper we attempt to improve the understanding of the
                   underlying issues by exploring these problems from an
                   analytical, a geometric and a dynamical systems perspective.
                   Our analysis is used to justify a simple yet effective
                   solution. We propose a gradient norm clipping strategy to
                   deal with exploding gradients and a soft constraint for the
                   vanishing gradients problem. We validate empirically our
                   hypothesis and proposed solutions in the experimental
                   section.",
  month         =  "21~" # nov,
  year          =  2012,
  url           = "http://arxiv.org/abs/1211.5063",
  file          = "All Papers/PASCANU/Pascanu et al. 2012 - On the difficulty of training Recurrent Neural Networks.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1211.5063"
}


@MISC{Jaeger2013-vx,
  title  = "A tutorial on training recurrent neural networks, covering {BPTT},
            {RTRL}, {EKF} and the ``echo state network'' approach",
  author = "Jaeger, Herbert and Kirkpatrick, James",
  url    = "https://www.ai.rug.nl/minds/uploads/ESNTutorialRev.pdf",
  file   = "All Papers/JAEGER/Jaeger and Kirkpatrick - A tutorial on training recurrent neural networks, covering BPTT, RTRL, EKF and the 'echo state network' approach.pdf"
}
