@online{Liu2020-yh,
title         = "Self-supervised Learning: Generative or Contrastive",
author        = "Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu
                 and Mian, Li and Zhang, Jing and Tang, Jie",
abstract      = "Deep supervised learning has achieved great success in the
                 last decade. However, its deficiencies of dependence on
                 manual labels and vulnerability to attacks have driven
                 people to explore a better solution. As an alternative,
                 self-supervised learning attracts many researchers for its
                 soaring performance on representation learning in the last
                 several years. Self-supervised representation learning
                 leverages input data itself as supervision and benefits
                 almost all types of downstream tasks. In this survey, we
                 take a look into new self-supervised learning methods for
                 representation in computer vision, natural language
                 processing, and graph learning. We comprehensively review
                 the existing empirical methods and summarize them into three
                 main categories according to their objectives: generative,
                 contrastive, and generative-contrastive (adversarial). We
                 further investigate related theoretical analysis work to
                 provide deeper thoughts on how self-supervised learning
                 works. Finally, we briefly discuss open problems and future
                 directions for self-supervised learning. An outline slide
                 for the survey is provided.",
month         =  "15~" # jun,
year          =  2020,
url           = "http://arxiv.org/abs/2006.08218",
file          = "All Papers/LIU/Liu et al. 2020 - Self-supervised Learning - Generative or Contrastive.pdf",
archivePrefix = "arXiv",
eprint        = "2006.08218",
primaryClass  = "cs.LG",
arxivid       = "2006.08218"
}


@online{Chen2018-mp,
  title         = "Neural Ordinary Differential Equations",
  author        = "Chen, Ricky T Q and Rubanova, Yulia and Bettencourt, Jesse
                   and Duvenaud, David",
  abstract      = "We introduce a new family of deep neural network models.
                   Instead of specifying a discrete sequence of hidden layers,
                   we parameterize the derivative of the hidden state using a
                   neural network. The output of the network is computed using
                   a black-box differential equation solver. These
                   continuous-depth models have constant memory cost, adapt
                   their evaluation strategy to each input, and can explicitly
                   trade numerical precision for speed. We demonstrate these
                   properties in continuous-depth residual networks and
                   continuous-time latent variable models. We also construct
                   continuous normalizing flows, a generative model that can
                   train by maximum likelihood, without partitioning or
                   ordering the data dimensions. For training, we show how to
                   scalably backpropagate through any ODE solver, without
                   access to its internal operations. This allows end-to-end
                   training of ODEs within larger models.",
  month         =  "19~" # jun,
  year          =  2018,
  url           = "http://arxiv.org/abs/1806.07366",
  file          = "All Papers/CHEN/Chen et al. 2018 - Neural Ordinary Differential Equations.pdf",
  archivePrefix = "arXiv",
  eprint        = "1806.07366",
  primaryClass  = "cs.LG",
  arxivid       = "1806.07366"
}
