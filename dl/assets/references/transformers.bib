@online{Phuong2022-im,
  title         = "Formal Algorithms for Transformers",
  author        = "Phuong, Mary and Hutter, Marcus",
  abstract      = "This document aims to be a self-contained, mathematically
                   precise overview of transformer architectures and algorithms
                   (*not* results). It covers what transformers are, how they
                   are trained, what they are used for, their key architectural
                   components, and a preview of the most prominent models. The
                   reader is assumed to be familiar with basic ML terminology
                   and simpler neural network architectures such as MLPs.",
  month         =  "19~" # jul,
  year          =  2022,
  url           = "http://arxiv.org/abs/2207.09238",
  file          = "All Papers/PHUONG/Phuong and Hutter 2022 - Formal Algorithms for Transformers.pdf",
  archivePrefix = "arXiv",
  eprint        = "2207.09238",
  primaryClass  = "cs.LG",
  arxivid       = "2207.09238",
  doi           = "10.48550/ARXIV.2207.09238"
}


@ARTICLE{Zeng2022-ku,
  title         = "Are Transformers Effective for Time Series Forecasting?",
  author        = "Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang",
  abstract      = "Recently, there has been a surge of Transformer-based
                   solutions for the long-term time series forecasting (LTSF)
                   task. Despite the growing performance over the past few
                   years, we question the validity of this line of research in
                   this work. Specifically, Transformers is arguably the most
                   successful solution to extract the semantic correlations
                   among the elements in a long sequence. However, in time
                   series modeling, we are to extract the temporal relations in
                   an ordered set of continuous points. While employing
                   positional encoding and using tokens to embed sub-series in
                   Transformers facilitate preserving some ordering
                   information, the nature of the
                   \textbackslashemph\{permutation-invariant\} self-attention
                   mechanism inevitably results in temporal information loss.
                   To validate our claim, we introduce a set of embarrassingly
                   simple one-layer linear models named LTSF-Linear for
                   comparison. Experimental results on nine real-life datasets
                   show that LTSF-Linear surprisingly outperforms existing
                   sophisticated Transformer-based LTSF models in all cases,
                   and often by a large margin. Moreover, we conduct
                   comprehensive empirical studies to explore the impacts of
                   various design elements of LTSF models on their temporal
                   relation extraction capability. We hope this surprising
                   finding opens up new research directions for the LTSF task.
                   We also advocate revisiting the validity of
                   Transformer-based solutions for other time series analysis
                   tasks (e.g., anomaly detection) in the future. Code is
                   available at:
                   \textbackslashurl\{https://github.com/cure-lab/LTSF-Linear\}.",
  month         =  "26~" # may,
  year          =  2022,
  url           = "http://arxiv.org/abs/2205.13504",
  file          = "All Papers/ZENG/Zeng et al. 2022 - Are Transformers Effective for Time Series Forecasting.pdf",
  archivePrefix = "arXiv",
  eprint        = "2205.13504",
  primaryClass  = "cs.AI",
  arxivid       = "2205.13504"
}


@ARTICLE{Micheli2022-mf,
  title         = "Transformers are {Sample-Efficient} World Models",
  author        = "Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c c}ois",
  abstract      = "Deep reinforcement learning agents are notoriously sample
                   inefficient, which considerably limits their application to
                   real-world problems. Recently, many model-based methods have
                   been designed to address this issue, with learning in the
                   imagination of a world model being one of the most prominent
                   approaches. However, while virtually unlimited interaction
                   with a simulated environment sounds appealing, the world
                   model has to be accurate over extended periods of time.
                   Motivated by the success of Transformers in sequence
                   modeling tasks, we introduce IRIS, a data-efficient agent
                   that learns in a world model composed of a discrete
                   autoencoder and an autoregressive Transformer. With the
                   equivalent of only two hours of gameplay in the Atari 100k
                   benchmark, IRIS achieves a mean human normalized score of
                   1.046, and outperforms humans on 10 out of 26 games, setting
                   a new state of the art for methods without lookahead search.
                   To foster future research on Transformers and world models
                   for sample-efficient reinforcement learning, we release our
                   code and models at https://github.com/eloialonso/iris.",
  month         =  "1~" # sep,
  year          =  2022,
  url           = "http://arxiv.org/abs/2209.00588",
  file          = "All Papers/MICHELI/Micheli et al. 2022 - Transformers are Sample-Efficient World Models.pdf",
  archivePrefix = "arXiv",
  eprint        = "2209.00588",
  primaryClass  = "cs.LG",
  arxivid       = "2209.00588"
}


@online{Vaswani2017-yg,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  "12~" # jun,
  year          =  2017,
  url           = "http://arxiv.org/abs/1706.03762",
  file          = "All Papers/VASWANI/Vaswani et al. 2017 - Attention Is All You Need.pdf",
  archivePrefix = "arXiv",
  eprint        = "1706.03762",
  primaryClass  = "cs.CL",
  arxivid       = "1706.03762"
}


@ARTICLE{Amatriain2023-of,
  title         = "Transformer models: an introduction and catalog",
  author        = "Amatriain, Xavier",
  abstract      = "In the past few years we have seen the meteoric appearance
                   of dozens of models of the Transformer family, all of which
                   have funny, but not self-explanatory, names. The goal of
                   this paper is to offer a somewhat comprehensive but simple
                   catalog and classification of the most popular Transformer
                   models. The paper also includes an introduction to the most
                   important aspects and innovation in Transformer models.",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  month         =  "12~" # feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.07730",
  file          = "All Papers/AMATRIAIN/Amatriain 2023 - Transformer models - an introduction and catalog.pdf",
  archivePrefix = "arXiv",
  eprint        = "2302.07730",
  primaryClass  = "cs.CL",
  arxivid       = "2302.07730",
  doi           = "10.48550/ARXIV.2302.07730"
}
