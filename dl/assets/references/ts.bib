
@online{Wen2020-ez,
title         = "Time Series Data Augmentation for Deep Learning: A Survey",
author        = "Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin
                 and Gao, Jingkun and Wang, Xue and Xu, Huan",
abstract      = "Deep learning performs remarkably well on many time series
                 analysis tasks recently. The superior performance of deep
                 neural networks relies heavily on a large number of training
                 data to avoid overfitting. However, the labeled data of many
                 real-world time series applications may be limited such as
                 classification in medical time series and anomaly detection
                 in AIOps. As an effective way to enhance the size and
                 quality of the training data, data augmentation is crucial
                 to the successful application of deep learning models on
                 time series data. In this paper, we systematically review
                 different data augmentation methods for time series. We
                 propose a taxonomy for the reviewed methods, and then
                 provide a structured review for these methods by
                 highlighting their strengths and limitations. We also
                 empirically compare different data augmentation methods for
                 different tasks including time series classification,
                 anomaly detection, and forecasting. Finally, we discuss and
                 highlight five future directions to provide useful research
                 guidance.",
month         =  "27~" # feb,
year          =  2020,
url           = "http://arxiv.org/abs/2002.12478",
file          = "All Papers/WEN/Wen et al. 2020 - Time Series Data Augmentation for Deep Learning - A Survey.pdf",
archivePrefix = "arXiv",
eprint        = "2002.12478",
primaryClass  = "cs.LG",
arxivid       = "2002.12478"
}


@INPROCEEDINGS{Le_Guennec2016-zi,
title     = "Data augmentation for time series classification using
             convolutional neural networks",
booktitle = "{ECML/PKDD} workshop on advanced analytics and learning on
             temporal data",
author    = "Le Guennec, Arthur and Malinowski, Simon and Tavenard, Romain",
year      =  2016,
url       = "https://halshs.archives-ouvertes.fr/halshs-01357973/document",
file      = "All Papers/LE GUENNEC/Le Guennec et al. 2016 - Data augmentation for time series classification using convolutional neural networks.pdf"
}


@ARTICLE{Shorten2019-ty,
  title     = "A survey on Image Data Augmentation for Deep Learning",
  author    = "Shorten, Connor and Khoshgoftaar, Taghi M",
  abstract  = "Deep convolutional neural networks have performed remarkably
               well on many Computer Vision tasks. However, these networks are
               heavily reliant on big data to avoid overfitting. Overfitting
               refers to the phenomenon when a network learns a function with
               very high variance such as to perfectly model the training data.
               Unfortunately, many application domains do not have access to
               big data, such as medical image analysis. This survey focuses on
               Data Augmentation, a data-space solution to the problem of
               limited data. Data Augmentation encompasses a suite of
               techniques that enhance the size and quality of training
               datasets such that better Deep Learning models can be built
               using them. The image augmentation algorithms discussed in this
               survey include geometric transformations, color space
               augmentations, kernel filters, mixing images, random erasing,
               feature space augmentation, adversarial training, generative
               adversarial networks, neural style transfer, and meta-learning.
               The application of augmentation methods based on GANs are
               heavily covered in this survey. In addition to augmentation
               techniques, this paper will briefly discuss other
               characteristics of Data Augmentation such as test-time
               augmentation, resolution impact, final dataset size, and
               curriculum learning. This survey will present existing methods
               for Data Augmentation, promising developments, and meta-level
               decisions for implementing Data Augmentation. Readers will
               understand how Data Augmentation can improve the performance of
               their models and expand limited datasets to take advantage of
               the capabilities of big data.",
  journal   = "Journal of Big Data",
  publisher = "SpringerOpen",
  volume    =  6,
  number    =  1,
  pages     = "1--48",
  month     =  "6~" # jul,
  year      =  2019,
  url       = "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0",
  file      = "All Papers/SHORTEN/Shorten and Khoshgoftaar 2019 - A survey on Image Data Augmentation for Deep Learning.pdf",
  language  = "en",
  issn      = "2196-1115, 2196-1115",
  doi       = "10.1186/s40537-019-0197-0"
}


@online{Hasibi2019-in,
  title         = "Augmentation Scheme for Dealing with Imbalanced Network
                   Traffic Classification Using Deep Learning",
  author        = "Hasibi, Ramin and Shokri, Matin and Dehghan, Mehdi",
  abstract      = "One of the most important tasks in network management is
                   identifying different types of traffic flows. As a result, a
                   type of management service, called Network Traffic
                   Classifier (NTC), has been introduced. One type of NTCs that
                   has gained huge attention in recent years applies deep
                   learning on packets in order to classify flows. Internet is
                   an imbalanced environment i.e., some classes of applications
                   are a lot more populated than others e.g., HTTP.
                   Additionally, one of the challenges in deep learning methods
                   is that they do not perform well in imbalanced environments
                   in terms of evaluation metrics such as precision, recall,
                   and $\mathrm\{F_1\}$ measure. In order to solve this
                   problem, we recommend the use of augmentation methods to
                   balance the dataset. In this paper, we propose a novel data
                   augmentation approach based on the use of Long Short Term
                   Memory (LSTM) networks for generating traffic flow patterns
                   and Kernel Density Estimation (KDE) for replicating the
                   numerical features of each class. First, we use the LSTM
                   network in order to learn and generate the sequence of
                   packets in a flow for classes with less population. Then, we
                   complete the features of the sequence with generating random
                   values based on the distribution of a certain feature, which
                   will be estimated using KDE. Finally, we compare the
                   training of a Convolutional Recurrent Neural Network (CRNN)
                   in large-scale imbalanced, sampled, and augmented datasets.
                   The contribution of our augmentation scheme is then
                   evaluated on all of the datasets through measurements of
                   precision, recall, and F1 measure for every class of
                   application. The results demonstrate that our scheme is well
                   suited for network traffic flow datasets and improves the
                   performance of deep learning algorithms when it comes to
                   above-mentioned metrics.",
  month         =  "1~" # jan,
  year          =  2019,
  url           = "http://arxiv.org/abs/1901.00204",
  file          = "All Papers/HASIBI/Hasibi et al. 2019 - Augmentation Scheme for Dealing with Imbalanced Network Traffic Classification Using Deep Learning.pdf",
  archivePrefix = "arXiv",
  eprint        = "1901.00204",
  primaryClass  = "cs.NI",
  arxivid       = "1901.00204"
}


@online{Iwana2020-oc,
  title         = "An Empirical Survey of Data Augmentation for Time Series
                   Classification with Neural Networks",
  author        = "Iwana, Brian Kenji and Uchida, Seiichi",
  abstract      = "In recent times, deep artificial neural networks have
                   achieved many successes in pattern recognition. Part of this
                   success can be attributed to the reliance on big data to
                   increase generalization. However, in the field of time
                   series recognition, many datasets are often very small. One
                   method of addressing this problem is through the use of data
                   augmentation. In this paper, we survey data augmentation
                   techniques for time series and their application to time
                   series classification with neural networks. We propose a
                   taxonomy and outline the four families in time series data
                   augmentation, including transformation-based methods,
                   pattern mixing, generative models, and decomposition
                   methods. Furthermore, we empirically evaluate 12 time series
                   data augmentation methods on 128 time series classification
                   datasets with six different types of neural networks.
                   Through the results, we are able to analyze the
                   characteristics, advantages and disadvantages, and
                   recommendations of each data augmentation method. This
                   survey aims to help in the selection of time series data
                   augmentation for neural network applications.",
  month         =  "31~" # jul,
  year          =  2020,
  url           = "http://arxiv.org/abs/2007.15951",
  file          = "All Papers/IWANA/Iwana and Uchida 2020 - An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "2007.15951",
  primaryClass  = "cs.LG",
  arxivid       = "2007.15951"
}



@online{Gao2020-qr,
  title         = "{RobustTAD}: Robust Time Series Anomaly Detection via
                   Decomposition and Convolutional Neural Networks",
  author        = "Gao, Jingkun and Song, Xiaomin and Wen, Qingsong and Wang,
                   Pichao and Sun, Liang and Xu, Huan",
  abstract      = "The monitoring and management of numerous and diverse time
                   series data at Alibaba Group calls for an effective and
                   scalable time series anomaly detection service. In this
                   paper, we propose RobustTAD, a Robust Time series Anomaly
                   Detection framework by integrating robust seasonal-trend
                   decomposition and convolutional neural network for time
                   series data. The seasonal-trend decomposition can
                   effectively handle complicated patterns in time series, and
                   meanwhile significantly simplifies the architecture of the
                   neural network, which is an encoder-decoder architecture
                   with skip connections. This architecture can effectively
                   capture the multi-scale information from time series, which
                   is very useful in anomaly detection. Due to the limited
                   labeled data in time series anomaly detection, we
                   systematically investigate data augmentation methods in both
                   time and frequency domains. We also introduce label-based
                   weight and value-based weight in the loss function by
                   utilizing the unbalanced nature of the time series anomaly
                   detection problem. Compared with the widely used
                   forecasting-based anomaly detection algorithms,
                   decomposition-based algorithms, traditional statistical
                   algorithms, as well as recent neural network based
                   algorithms, RobustTAD performs significantly better on
                   public benchmark datasets. It is deployed as a public online
                   service and widely adopted in different business scenarios
                   at Alibaba Group.",
  month         =  "21~" # feb,
  year          =  2020,
  url           = "http://arxiv.org/abs/2002.09545",
  file          = "All Papers/GAO/Gao et al. 2020 - RobustTAD - Robust Time Series Anomaly Detection via Decomposition and Convolutional Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "2002.09545",
  primaryClass  = "cs.LG",
  arxivid       = "2002.09545"
}


@online{Takahashi2017-yz,
  title         = "{AENet}: Learning Deep Audio Features for Video Analysis",
  author        = "Takahashi, Naoya and Gygli, Michael and Van Gool, Luc",
  abstract      = "We propose a new deep network for audio event recognition,
                   called AENet. In contrast to speech, sounds coming from
                   audio events may be produced by a wide variety of sources.
                   Furthermore, distinguishing them often requires analyzing an
                   extended time period due to the lack of clear sub-word units
                   that are present in speech. In order to incorporate this
                   long-time frequency structure of audio events, we introduce
                   a convolutional neural network (CNN) operating on a large
                   temporal input. In contrast to previous works this allows us
                   to train an audio event detection system end-to-end. The
                   combination of our network architecture and a novel data
                   augmentation outperforms previous methods for audio event
                   detection by 16\%. Furthermore, we perform transfer learning
                   and show that our model learnt generic audio features,
                   similar to the way CNNs learn generic features on vision
                   tasks. In video analysis, combining visual features and
                   traditional audio features such as MFCC typically only leads
                   to marginal improvements. Instead, combining visual features
                   with our AENet features, which can be computed efficiently
                   on a GPU, leads to significant performance improvements on
                   action recognition and video highlight detection. In video
                   highlight detection, our audio features improve the
                   performance by more than 8\% over visual features alone.",
  month         =  "3~" # jan,
  year          =  2017,
  url           = "http://arxiv.org/abs/1701.00599",
  file          = "All Papers/TAKAHASHI/Takahashi et al. 2017 - AENet - Learning Deep Audio Features for Video Analysis.pdf",
  archivePrefix = "arXiv",
  eprint        = "1701.00599",
  primaryClass  = "cs.MM",
  arxivid       = "1701.00599"
}


@INCOLLECTION{Stock2016-mh,
  title     = "Chapter 8 - Dynamic Factor Models, {Factor-Augmented} Vector
               Autoregressions, and Structural Vector Autoregressions in
               Macroeconomics",
  booktitle = "Handbook of Macroeconomics",
  author    = "Stock, J H and Watson, M W",
  editor    = "Taylor, John B and Uhlig, Harald",
  abstract  = "This chapter provides an overview of and user's guide to dynamic
               factor models (DFMs), their estimation, and their uses in
               empirical macroeconomics. It also surveys recent developments in
               methods for identifying and estimating SVARs, an area that has
               seen important developments over the past 15 years. The chapter
               begins by introducing DFMs and the associated statistical tools,
               both parametric (state-space forms) and nonparametric (principal
               components and related methods). After reviewing two mature
               applications of DFMs, forecasting and macroeconomic monitoring,
               the chapter lays out the use of DFMs for analysis of structural
               shocks, a special case of which is factor-augmented vector
               autoregressions (FAVARs). A main focus of the chapter is how to
               extend methods for identifying shocks in structural vector
               autoregression (SVAR) to structural DFMs. The chapter provides a
               unification of SVARs, FAVARs, and structural DFMs and shows both
               in theory and through an empirical application to oil shocks how
               the same identification strategies can be applied to each type
               of model.",
  publisher = "Elsevier",
  volume    =  2,
  pages     = "415--525",
  month     =  "1~" # jan,
  year      =  2016,
  url       = "https://www.sciencedirect.com/science/article/pii/S1574004816300027",
  file      = "All Papers/STOCK/Stock and Watson 2016 - Chapter 8 - Dynamic Factor Models, Factor-Augmen ... oregressions, and Structural Vector Autoregressions in Macroeconomics.pdf",
  keywords  = "State-space models; Structural vector autoregressions;
               Factor-augmented vector autoregressions; Principal components;
               Large-model forecasting; Nowcasting; Structural shocks",
  doi       = "10.1016/bs.hesmac.2016.04.002"
}


@INPROCEEDINGS{Cui2014-de,
  title     = "Data Augmentation for deep neural network acoustic modeling",
  booktitle = "2014 {IEEE} International Conference on Acoustics, Speech and
               Signal Processing ({ICASSP})",
  author    = "Cui, Xiaodong and Goel, Vaibhava and Kingsbury, Brian",
  abstract  = "Data augmentation using label preserving transformations has
               been shown to be effective for neural network training to make
               invariant predictions. In this paper we focus on data
               augmentation approaches to acoustic modeling using deep neural
               networks (DNNs) for automatic speech recognition (ASR). We first
               investigate a modified version of a previously studied approach
               using vocal tract length perturbation (VTLP) and then propose a
               novel data augmentation approach based on stochastic feature
               mapping (SFM) in a speaker adaptive feature space. Experiments
               were conducted on Bengali and Assamese limited language packs
               (LLPs) from the IARPA Babel program. Improved recognition
               performance has been observed after both cross-entropy (CE) and
               state-level minimum Bayes risk (sMBR) training of DNN models.",
  pages     = "5582--5586",
  month     =  may,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/ICASSP.2014.6854671",
  file      = "All Papers/CUI/Cui et al. 2014 - Data Augmentation for deep neural network acoustic modeling.pdf",
  keywords  = "Training;Hidden Markov models;Speech;Acoustics;Neural
               networks;Data models;Training data;deep neural networks;data
               augmentation;vocal tract length perturbation;stochastic feature
               mapping;automatic speech recognition",
  issn      = "2379-190X",
  doi       = "10.1109/ICASSP.2014.6854671"
}


@ARTICLE{Cao2014-mt,
  title    = "A parsimonious mixture of Gaussian trees model for oversampling
              in imbalanced and multimodal time-series classification",
  author   = "Cao, Hong and Tan, Vincent Y F and Pang, John Z F",
  abstract = "We propose a novel framework of using a parsimonious statistical
              model, known as mixture of Gaussian trees, for modeling the
              possibly multimodal minority class to solve the problem of
              imbalanced time-series classification. By exploiting the fact
              that close-by time points are highly correlated due to smoothness
              of the time-series, our model significantly reduces the number of
              covariance parameters to be estimated from O(d(2)) to O(Ld),
              where L is the number of mixture components and d is the
              dimensionality. Thus, our model is particularly effective for
              modeling high-dimensional time-series with limited number of
              instances in the minority positive class. In addition, the
              computational complexity for learning the model is only of the
              order O(Ln+d(2)) where n+ is the number of positively labeled
              samples. We conduct extensive classification experiments based on
              several well-known time-series data sets (both single- and
              multimodal) by first randomly generating synthetic instances from
              our learned mixture model to correct the imbalance. We then
              compare our results with several state-of-the-art oversampling
              techniques and the results demonstrate that when our proposed
              model is used in oversampling, the same support vector machines
              classifier achieves much better classification accuracy across
              the range of data sets. In fact, the proposed method achieves the
              best average performance 30 times out of 36 multimodal data sets
              according to the F-value metric. Our results are also highly
              competitive compared with nonoversampling-based classifiers for
              dealing with imbalanced time-series data sets.",
  journal  = "IEEE transactions on neural networks and learning systems",
  volume   =  25,
  number   =  12,
  pages    = "2226--2239",
  month    =  dec,
  year     =  2014,
  url      = "http://dx.doi.org/10.1109/TNNLS.2014.2308321",
  file     = "All Papers/CAO/Cao et al. 2014 - A parsimonious mixture of Gaussian trees model for oversampling in imbalanced and multimodal time-series classification.pdf",
  language = "en",
  issn     = "2162-2388, 2162-237X",
  pmid     = "25420245",
  doi      = "10.1109/TNNLS.2014.2308321"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bergmeir2016-eh,
  title     = "Bagging exponential smoothing methods using {STL} decomposition
               and {Box--Cox} transformation",
  author    = "Bergmeir, Christoph and Hyndman, Rob J and Ben{\'\i}tez,
               Jos{\'e} M",
  abstract  = "Exponential smoothing is one of the most popular forecasting
               methods. We present a technique for the bootstrap aggregation
               (bagging) of exponential smoothing methods, which results in
               significant improvements in the forecasts. The bagging uses a
               Box--Cox transformation followed by an STL decomposition to
               separate the time series into the trend, seasonal part, and
               remainder. The remainder is then bootstrapped using a moving
               block bootstrap, and a new series is assembled using this
               bootstrapped remainder. An ensemble of exponential smoothing
               models is then estimated on the bootstrapped series, and the
               resulting point forecasts are combined. We evaluate this new
               method on the M3 data set, and show that it outperforms the
               original exponential smoothing models consistently. On the
               monthly data, we achieve better results than any of the original
               M3 participants.",
  journal   = "International journal of forecasting",
  publisher = "Elsevier BV",
  volume    =  32,
  number    =  2,
  pages     = "303--312",
  month     =  "1~" # apr,
  year      =  2016,
  url       = "https://www.sciencedirect.com/science/article/pii/S0169207015001120",
  file      = "All Papers/BERGMEIR/Bergmeir et al. 2016 - Bagging exponential smoothing methods using STL decomposition and Boxâ€“Cox transformation.pdf",
  keywords  = "Bagging; Bootstrapping; Exponential smoothing; STL decomposition",
  language  = "en",
  issn      = "0169-2070, 1872-8200",
  doi       = "10.1016/j.ijforecast.2015.07.002"
}



@online{Um2017-oq,
  title         = "Data Augmentation of Wearable Sensor Data for Parkinson's
                   Disease Monitoring using Convolutional Neural Networks",
  author        = "Um, Terry Taewoong and Pfister, Franz Michael Josef and
                   Pichler, Daniel and Endo, Satoshi and Lang, Muriel and
                   Hirche, Sandra and Fietzek, Urban and Kuli{\'c}, Dana",
  abstract      = "While convolutional neural networks (CNNs) have been
                   successfully applied to many challenging classification
                   applications, they typically require large datasets for
                   training. When the availability of labeled data is limited,
                   data augmentation is a critical preprocessing step for CNNs.
                   However, data augmentation for wearable sensor data has not
                   been deeply investigated yet. In this paper, various data
                   augmentation methods for wearable sensor data are proposed.
                   The proposed methods and CNNs are applied to the
                   classification of the motor state of Parkinson's Disease
                   patients, which is challenging due to small dataset size,
                   noisy labels, and large intra-class variability. Appropriate
                   augmentation improves the classification performance from
                   77.54\% to 86.88\%.",
  month         =  "2~" # jun,
  year          =  2017,
  url           = "http://arxiv.org/abs/1706.00527",
  file          = "All Papers/UM/Um et al. 2017 - Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring using Convolutional Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "1706.00527",
  primaryClass  = "cs.CV",
  arxivid       = "1706.00527"
}



@online{Kang2019-cl,
  title         = "{GRATIS}: {GeneRAting} {TIme} Series with diverse and
                   controllable characteristics",
  author        = "Kang, Yanfei and Hyndman, Rob J and Li, Feng",
  abstract      = "The explosion of time series data in recent years has
                   brought a flourish of new time series analysis methods, for
                   forecasting, clustering, classification and other tasks. The
                   evaluation of these new methods requires either collecting
                   or simulating a diverse set of time series benchmarking data
                   to enable reliable comparisons against alternative
                   approaches. We propose GeneRAting TIme Series with diverse
                   and controllable characteristics, named GRATIS, with the use
                   of mixture autoregressive (MAR) models. We simulate sets of
                   time series using MAR models and investigate the diversity
                   and coverage of the generated time series in a time series
                   feature space. By tuning the parameters of the MAR models,
                   GRATIS is also able to efficiently generate new time series
                   with controllable features. In general, as a costless
                   surrogate to the traditional data collection approach,
                   GRATIS can be used as an evaluation tool for tasks such as
                   time series forecasting and classification. We illustrate
                   the usefulness of our time series generation process through
                   a time series forecasting application.",
  month         =  "7~" # mar,
  year          =  2019,
  url           = "http://arxiv.org/abs/1903.02787",
  file          = "All Papers/KANG/Kang et al. 2019 - GRATIS - GeneRAting TIme Series with diverse and controllable characteristics.pdf",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1903.02787",
  primaryClass  = "stat.ML",
  arxivid       = "1903.02787"
}


@ARTICLE{Petitjean2011-sj,
  title    = "A global averaging method for dynamic time warping, with
              applications to clustering",
  author   = "Petitjean, Fran{\c c}ois and Ketterlin, Alain and Gan{\c c}arski,
              Pierre",
  abstract = "Mining sequential data is an old topic that has been revived in
              the last decade, due to the increasing availability of sequential
              datasets. Most works in this field are centred on the definition
              and use of a distance (or, at least, a similarity measure)
              between sequences of elements. A measure called dynamic time
              warping (DTW) seems to be currently the most relevant for a large
              panel of applications. This article is about the use of DTW in
              data mining algorithms, and focuses on the computation of an
              average of a set of sequences. Averaging is an essential tool for
              the analysis of data. For example, the K-means clustering
              algorithm repeatedly computes such an average, and needs to
              provide a description of the clusters it forms. Averaging is here
              a crucial step, which must be sound in order to make algorithms
              work accurately. When dealing with sequences, especially when
              sequences are compared with DTW, averaging is not a trivial task.
              Starting with existing techniques developed around DTW, the
              article suggests an analysis framework to classify averaging
              techniques. It then proceeds to study the two major questions
              lifted by the framework. First, we develop a global technique for
              averaging a set of sequences. This technique is original in that
              it avoids using iterative pairwise averaging. It is thus
              insensitive to ordering effects. Second, we describe a new
              strategy to reduce the length of the resulting average sequence.
              This has a favourable impact on performance, but also on the
              relevance of the results. Both aspects are evaluated on standard
              datasets, and the evaluation shows that they compare favourably
              with existing methods. The article ends by describing the use of
              averaging in clustering. The last section also introduces a new
              application domain, namely the analysis of satellite image time
              series, where data mining techniques provide an original
              approach.",
  journal  = "Pattern recognition",
  volume   =  44,
  number   =  3,
  pages    = "678--693",
  month    =  "1~" # mar,
  year     =  2011,
  url      = "https://www.sciencedirect.com/science/article/pii/S003132031000453X",
  file     = "All Papers/PETITJEAN/Petitjean et al. 2011 - A global averaging method for dynamic time warping, with applications to clustering.pdf",
  keywords = "Sequence analysis; Time series clustering; Dynamic time warping;
              Distance-based clustering; Time series averaging; DTW barycenter
              averaging; Global averaging; Satellite image time series",
  issn     = "0031-3203",
  doi      = "10.1016/j.patcog.2010.09.013"
}


@online{Hewamalage2019-tv,
  title         = "Recurrent Neural Networks for Time Series Forecasting:
                   Current Status and Future Directions",
  author        = "Hewamalage, Hansika and Bergmeir, Christoph and Bandara,
                   Kasun",
  abstract      = "Recurrent Neural Networks (RNN) have become competitive
                   forecasting methods, as most notably shown in the winning
                   method of the recent M4 competition. However, established
                   statistical models such as ETS and ARIMA gain their
                   popularity not only from their high accuracy, but they are
                   also suitable for non-expert users as they are robust,
                   efficient, and automatic. In these areas, RNNs have still a
                   long way to go. We present an extensive empirical study and
                   an open-source software framework of existing RNN
                   architectures for forecasting, that allow us to develop
                   guidelines and best practices for their use. For example, we
                   conclude that RNNs are capable of modelling seasonality
                   directly if the series in the dataset possess homogeneous
                   seasonal patterns, otherwise we recommend a
                   deseasonalization step. Comparisons against ETS and ARIMA
                   demonstrate that the implemented (semi-)automatic RNN models
                   are no silver bullets, but they are competitive alternatives
                   in many situations.",
  month         =  "2~" # sep,
  year          =  2019,
  url           = "http://arxiv.org/abs/1909.00590",
  file          = "All Papers/HEWAMALAGE/Hewamalage et al. 2019 - Recurrent Neural Networks for Time Series Forecasting - Current Status and Future Directions.pdf",
  archivePrefix = "arXiv",
  eprint        = "1909.00590",
  primaryClass  = "cs.LG",
  arxivid       = "1909.00590"
}


@online{Bandara2020-yp,
  title         = "Improving the Accuracy of Global Forecasting Models using
                   Time Series Data Augmentation",
  author        = "Bandara, Kasun and Hewamalage, Hansika and Liu, Yuan-Hao and
                   Kang, Yanfei and Bergmeir, Christoph",
  abstract      = "Forecasting models that are trained across sets of many time
                   series, known as Global Forecasting Models (GFM), have shown
                   recently promising results in forecasting competitions and
                   real-world applications, outperforming many state-of-the-art
                   univariate forecasting techniques. In most cases, GFMs are
                   implemented using deep neural networks, and in particular
                   Recurrent Neural Networks (RNN), which require a sufficient
                   amount of time series to estimate their numerous model
                   parameters. However, many time series databases have only a
                   limited number of time series. In this study, we propose a
                   novel, data augmentation based forecasting framework that is
                   capable of improving the baseline accuracy of the GFM models
                   in less data-abundant settings. We use three time series
                   augmentation techniques: GRATIS, moving block bootstrap
                   (MBB), and dynamic time warping barycentric averaging (DBA)
                   to synthetically generate a collection of time series. The
                   knowledge acquired from these augmented time series is then
                   transferred to the original dataset using two different
                   approaches: the pooled approach and the transfer learning
                   approach. When building GFMs, in the pooled approach, we
                   train a model on the augmented time series alongside the
                   original time series dataset, whereas in the transfer
                   learning approach, we adapt a pre-trained model to the new
                   dataset. In our evaluation on competition and real-world
                   time series datasets, our proposed variants can
                   significantly improve the baseline accuracy of GFM models
                   and outperform state-of-the-art univariate forecasting
                   methods.",
  month         =  "6~" # aug,
  year          =  2020,
  url           = "http://arxiv.org/abs/2008.02663",
  file          = "All Papers/BANDARA/Bandara et al. 2020 - Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation.pdf",
  archivePrefix = "arXiv",
  eprint        = "2008.02663",
  primaryClass  = "cs.LG",
  arxivid       = "2008.02663"
}


@INPROCEEDINGS{Forestier2017-uk,
  title     = "Generating Synthetic Time Series to Augment Sparse Datasets",
  booktitle = "2017 {IEEE} International Conference on Data Mining ({ICDM})",
  author    = "Forestier, Germain and Petitjean, Fran{\c c}ois and Dau, Hoang
               Anh and Webb, Geoffrey I and Keogh, Eamonn",
  abstract  = "In machine learning, data augmentation is the process of
               creating synthetic examples in order to augment a dataset used
               to learn a model. One motivation for data augmentation is to
               reduce the variance of a classifier, thereby reducing error. In
               this paper, we propose new data augmentation techniques
               specifically designed for time series classification, where the
               space in which they are embedded is induced by Dynamic Time
               Warping (DTW). The main idea of our approach is to average a set
               of time series and use the average time series as a new
               synthetic example. The proposed methods rely on an extension of
               DTW Barycentric Averaging (DBA), the averaging technique that is
               specifically developed for DTW. In this paper, we extend DBA to
               be able to calculate a weighted average of time series under
               DTW. In this case, instead of each time series contributing
               equally to the final average, some can contribute more than
               others. This extension allows us to generate an infinite number
               of new examples from any set of given time series. To this end,
               we propose three methods that choose the weights associated to
               the time series of the dataset. We carry out experiments on the
               85 datasets of the UCR archive and demonstrate that our method
               is particularly useful when the number of available examples is
               limited (e.g. 2 to 6 examples per class) using a 1-NN DTW
               classifier. Furthermore, we show that augmenting full datasets
               is beneficial in most cases, as we observed an increase of
               accuracy on 56 datasets, no effect on 7 and a slight decrease on
               only 22.",
  pages     = "865--870",
  month     =  nov,
  year      =  2017,
  url       = "http://dx.doi.org/10.1109/ICDM.2017.106",
  file      = "All Papers/FORESTIER/Forestier et al. 2017 - Generating Synthetic Time Series to Augment Sparse Datasets.pdf",
  keywords  = "Time series analysis;Training;Data models;Heuristic
               algorithms;Manifolds;Conferences;Data mining;time series
               classification;dynamic time warping;data augmentation",
  issn      = "2374-8486",
  doi       = "10.1109/ICDM.2017.106"
}
