
@online{Wen2020-ez,
title         = "Time Series Data Augmentation for Deep Learning: A Survey",
author        = "Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin
                 and Gao, Jingkun and Wang, Xue and Xu, Huan",
abstract      = "Deep learning performs remarkably well on many time series
                 analysis tasks recently. The superior performance of deep
                 neural networks relies heavily on a large number of training
                 data to avoid overfitting. However, the labeled data of many
                 real-world time series applications may be limited such as
                 classification in medical time series and anomaly detection
                 in AIOps. As an effective way to enhance the size and
                 quality of the training data, data augmentation is crucial
                 to the successful application of deep learning models on
                 time series data. In this paper, we systematically review
                 different data augmentation methods for time series. We
                 propose a taxonomy for the reviewed methods, and then
                 provide a structured review for these methods by
                 highlighting their strengths and limitations. We also
                 empirically compare different data augmentation methods for
                 different tasks including time series classification,
                 anomaly detection, and forecasting. Finally, we discuss and
                 highlight five future directions to provide useful research
                 guidance.",
month         =  "27~" # feb,
year          =  2020,
url           = "http://arxiv.org/abs/2002.12478",
file          = "All Papers/WEN/Wen et al. 2020 - Time Series Data Augmentation for Deep Learning - A Survey.pdf",
archivePrefix = "arXiv",
eprint        = "2002.12478",
primaryClass  = "cs.LG",
arxivid       = "2002.12478",
journal       = "arXiv"
}


@INPROCEEDINGS{Le_Guennec2016-zi,
title     = "Data augmentation for time series classification using
             convolutional neural networks",
booktitle = "{ECML/PKDD} workshop on advanced analytics and learning on
             temporal data",
author    = "Le Guennec, Arthur and Malinowski, Simon and Tavenard, Romain",
year      =  2016,
url       = "https://halshs.archives-ouvertes.fr/halshs-01357973/document",
file      = "All Papers/LE GUENNEC/Le Guennec et al. 2016 - Data augmentation for time series classification using convolutional neural networks.pdf"
}


@ARTICLE{Shorten2019-ty,
  title     = "A survey on Image Data Augmentation for Deep Learning",
  author    = "Shorten, Connor and Khoshgoftaar, Taghi M",
  abstract  = "Deep convolutional neural networks have performed remarkably
               well on many Computer Vision tasks. However, these networks are
               heavily reliant on big data to avoid overfitting. Overfitting
               refers to the phenomenon when a network learns a function with
               very high variance such as to perfectly model the training data.
               Unfortunately, many application domains do not have access to
               big data, such as medical image analysis. This survey focuses on
               Data Augmentation, a data-space solution to the problem of
               limited data. Data Augmentation encompasses a suite of
               techniques that enhance the size and quality of training
               datasets such that better Deep Learning models can be built
               using them. The image augmentation algorithms discussed in this
               survey include geometric transformations, color space
               augmentations, kernel filters, mixing images, random erasing,
               feature space augmentation, adversarial training, generative
               adversarial networks, neural style transfer, and meta-learning.
               The application of augmentation methods based on GANs are
               heavily covered in this survey. In addition to augmentation
               techniques, this paper will briefly discuss other
               characteristics of Data Augmentation such as test-time
               augmentation, resolution impact, final dataset size, and
               curriculum learning. This survey will present existing methods
               for Data Augmentation, promising developments, and meta-level
               decisions for implementing Data Augmentation. Readers will
               understand how Data Augmentation can improve the performance of
               their models and expand limited datasets to take advantage of
               the capabilities of big data.",
  journal   = "Journal of Big Data",
  publisher = "SpringerOpen",
  volume    =  6,
  number    =  1,
  pages     = "1--48",
  month     =  "6~" # jul,
  year      =  2019,
  url       = "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0",
  file      = "All Papers/SHORTEN/Shorten and Khoshgoftaar 2019 - A survey on Image Data Augmentation for Deep Learning.pdf",
  language  = "en",
  issn      = "2196-1115, 2196-1115",
  doi       = "10.1186/s40537-019-0197-0"
}


@online{Hasibi2019-in,
  title         = "Augmentation Scheme for Dealing with Imbalanced Network
                   Traffic Classification Using Deep Learning",
  author        = "Hasibi, Ramin and Shokri, Matin and Dehghan, Mehdi",
  abstract      = "One of the most important tasks in network management is
                   identifying different types of traffic flows. As a result, a
                   type of management service, called Network Traffic
                   Classifier (NTC), has been introduced. One type of NTCs that
                   has gained huge attention in recent years applies deep
                   learning on packets in order to classify flows. Internet is
                   an imbalanced environment i.e., some classes of applications
                   are a lot more populated than others e.g., HTTP.
                   Additionally, one of the challenges in deep learning methods
                   is that they do not perform well in imbalanced environments
                   in terms of evaluation metrics such as precision, recall,
                   and $\mathrm\{F_1\}$ measure. In order to solve this
                   problem, we recommend the use of augmentation methods to
                   balance the dataset. In this paper, we propose a novel data
                   augmentation approach based on the use of Long Short Term
                   Memory (LSTM) networks for generating traffic flow patterns
                   and Kernel Density Estimation (KDE) for replicating the
                   numerical features of each class. First, we use the LSTM
                   network in order to learn and generate the sequence of
                   packets in a flow for classes with less population. Then, we
                   complete the features of the sequence with generating random
                   values based on the distribution of a certain feature, which
                   will be estimated using KDE. Finally, we compare the
                   training of a Convolutional Recurrent Neural Network (CRNN)
                   in large-scale imbalanced, sampled, and augmented datasets.
                   The contribution of our augmentation scheme is then
                   evaluated on all of the datasets through measurements of
                   precision, recall, and F1 measure for every class of
                   application. The results demonstrate that our scheme is well
                   suited for network traffic flow datasets and improves the
                   performance of deep learning algorithms when it comes to
                   above-mentioned metrics.",
  month         =  "1~" # jan,
  year          =  2019,
  url           = "http://arxiv.org/abs/1901.00204",
  file          = "All Papers/HASIBI/Hasibi et al. 2019 - Augmentation Scheme for Dealing with Imbalanced Network Traffic Classification Using Deep Learning.pdf",
  archivePrefix = "arXiv",
  eprint        = "1901.00204",
  primaryClass  = "cs.NI",
  arxivid       = "1901.00204"
}


@online{Iwana2020-oc,
  title         = "An Empirical Survey of Data Augmentation for Time Series
                   Classification with Neural Networks",
  author        = "Iwana, Brian Kenji and Uchida, Seiichi",
  abstract      = "In recent times, deep artificial neural networks have
                   achieved many successes in pattern recognition. Part of this
                   success can be attributed to the reliance on big data to
                   increase generalization. However, in the field of time
                   series recognition, many datasets are often very small. One
                   method of addressing this problem is through the use of data
                   augmentation. In this paper, we survey data augmentation
                   techniques for time series and their application to time
                   series classification with neural networks. We propose a
                   taxonomy and outline the four families in time series data
                   augmentation, including transformation-based methods,
                   pattern mixing, generative models, and decomposition
                   methods. Furthermore, we empirically evaluate 12 time series
                   data augmentation methods on 128 time series classification
                   datasets with six different types of neural networks.
                   Through the results, we are able to analyze the
                   characteristics, advantages and disadvantages, and
                   recommendations of each data augmentation method. This
                   survey aims to help in the selection of time series data
                   augmentation for neural network applications.",
  month         =  "31~" # jul,
  year          =  2020,
  url           = "http://arxiv.org/abs/2007.15951",
  file          = "All Papers/IWANA/Iwana and Uchida 2020 - An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "2007.15951",
  primaryClass  = "cs.LG",
  arxivid       = "2007.15951"
}



@online{Gao2020-qr,
  title         = "{RobustTAD}: Robust Time Series Anomaly Detection via
                   Decomposition and Convolutional Neural Networks",
  author        = "Gao, Jingkun and Song, Xiaomin and Wen, Qingsong and Wang,
                   Pichao and Sun, Liang and Xu, Huan",
  abstract      = "The monitoring and management of numerous and diverse time
                   series data at Alibaba Group calls for an effective and
                   scalable time series anomaly detection service. In this
                   paper, we propose RobustTAD, a Robust Time series Anomaly
                   Detection framework by integrating robust seasonal-trend
                   decomposition and convolutional neural network for time
                   series data. The seasonal-trend decomposition can
                   effectively handle complicated patterns in time series, and
                   meanwhile significantly simplifies the architecture of the
                   neural network, which is an encoder-decoder architecture
                   with skip connections. This architecture can effectively
                   capture the multi-scale information from time series, which
                   is very useful in anomaly detection. Due to the limited
                   labeled data in time series anomaly detection, we
                   systematically investigate data augmentation methods in both
                   time and frequency domains. We also introduce label-based
                   weight and value-based weight in the loss function by
                   utilizing the unbalanced nature of the time series anomaly
                   detection problem. Compared with the widely used
                   forecasting-based anomaly detection algorithms,
                   decomposition-based algorithms, traditional statistical
                   algorithms, as well as recent neural network based
                   algorithms, RobustTAD performs significantly better on
                   public benchmark datasets. It is deployed as a public online
                   service and widely adopted in different business scenarios
                   at Alibaba Group.",
  month         =  "21~" # feb,
  year          =  2020,
  url           = "http://arxiv.org/abs/2002.09545",
  file          = "All Papers/GAO/Gao et al. 2020 - RobustTAD - Robust Time Series Anomaly Detection via Decomposition and Convolutional Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "2002.09545",
  primaryClass  = "cs.LG",
  arxivid       = "2002.09545"
}


@online{Takahashi2017-yz,
  title         = "{AENet}: Learning Deep Audio Features for Video Analysis",
  author        = "Takahashi, Naoya and Gygli, Michael and Van Gool, Luc",
  abstract      = "We propose a new deep network for audio event recognition,
                   called AENet. In contrast to speech, sounds coming from
                   audio events may be produced by a wide variety of sources.
                   Furthermore, distinguishing them often requires analyzing an
                   extended time period due to the lack of clear sub-word units
                   that are present in speech. In order to incorporate this
                   long-time frequency structure of audio events, we introduce
                   a convolutional neural network (CNN) operating on a large
                   temporal input. In contrast to previous works this allows us
                   to train an audio event detection system end-to-end. The
                   combination of our network architecture and a novel data
                   augmentation outperforms previous methods for audio event
                   detection by 16\%. Furthermore, we perform transfer learning
                   and show that our model learnt generic audio features,
                   similar to the way CNNs learn generic features on vision
                   tasks. In video analysis, combining visual features and
                   traditional audio features such as MFCC typically only leads
                   to marginal improvements. Instead, combining visual features
                   with our AENet features, which can be computed efficiently
                   on a GPU, leads to significant performance improvements on
                   action recognition and video highlight detection. In video
                   highlight detection, our audio features improve the
                   performance by more than 8\% over visual features alone.",
  month         =  "3~" # jan,
  year          =  2017,
  url           = "http://arxiv.org/abs/1701.00599",
  file          = "All Papers/TAKAHASHI/Takahashi et al. 2017 - AENet - Learning Deep Audio Features for Video Analysis.pdf",
  archivePrefix = "arXiv",
  eprint        = "1701.00599",
  primaryClass  = "cs.MM",
  arxivid       = "1701.00599"
}


@INCOLLECTION{Stock2016-mh,
  title     = "Chapter 8 - Dynamic Factor Models, {Factor-Augmented} Vector
               Autoregressions, and Structural Vector Autoregressions in
               Macroeconomics",
  booktitle = "Handbook of Macroeconomics",
  author    = "Stock, J H and Watson, M W",
  editor    = "Taylor, John B and Uhlig, Harald",
  abstract  = "This chapter provides an overview of and user's guide to dynamic
               factor models (DFMs), their estimation, and their uses in
               empirical macroeconomics. It also surveys recent developments in
               methods for identifying and estimating SVARs, an area that has
               seen important developments over the past 15 years. The chapter
               begins by introducing DFMs and the associated statistical tools,
               both parametric (state-space forms) and nonparametric (principal
               components and related methods). After reviewing two mature
               applications of DFMs, forecasting and macroeconomic monitoring,
               the chapter lays out the use of DFMs for analysis of structural
               shocks, a special case of which is factor-augmented vector
               autoregressions (FAVARs). A main focus of the chapter is how to
               extend methods for identifying shocks in structural vector
               autoregression (SVAR) to structural DFMs. The chapter provides a
               unification of SVARs, FAVARs, and structural DFMs and shows both
               in theory and through an empirical application to oil shocks how
               the same identification strategies can be applied to each type
               of model.",
  publisher = "Elsevier",
  volume    =  2,
  pages     = "415--525",
  month     =  "1~" # jan,
  year      =  2016,
  url       = "https://www.sciencedirect.com/science/article/pii/S1574004816300027",
  file      = "All Papers/STOCK/Stock and Watson 2016 - Chapter 8 - Dynamic Factor Models, Factor-Augmen ... oregressions, and Structural Vector Autoregressions in Macroeconomics.pdf",
  keywords  = "State-space models; Structural vector autoregressions;
               Factor-augmented vector autoregressions; Principal components;
               Large-model forecasting; Nowcasting; Structural shocks",
  doi       = "10.1016/bs.hesmac.2016.04.002"
}


@INPROCEEDINGS{Cui2014-de,
  title     = "Data Augmentation for deep neural network acoustic modeling",
  booktitle = "2014 {IEEE} International Conference on Acoustics, Speech and
               Signal Processing ({ICASSP})",
  author    = "Cui, Xiaodong and Goel, Vaibhava and Kingsbury, Brian",
  abstract  = "Data augmentation using label preserving transformations has
               been shown to be effective for neural network training to make
               invariant predictions. In this paper we focus on data
               augmentation approaches to acoustic modeling using deep neural
               networks (DNNs) for automatic speech recognition (ASR). We first
               investigate a modified version of a previously studied approach
               using vocal tract length perturbation (VTLP) and then propose a
               novel data augmentation approach based on stochastic feature
               mapping (SFM) in a speaker adaptive feature space. Experiments
               were conducted on Bengali and Assamese limited language packs
               (LLPs) from the IARPA Babel program. Improved recognition
               performance has been observed after both cross-entropy (CE) and
               state-level minimum Bayes risk (sMBR) training of DNN models.",
  pages     = "5582--5586",
  month     =  may,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/ICASSP.2014.6854671",
  file      = "All Papers/CUI/Cui et al. 2014 - Data Augmentation for deep neural network acoustic modeling.pdf",
  keywords  = "Training;Hidden Markov models;Speech;Acoustics;Neural
               networks;Data models;Training data;deep neural networks;data
               augmentation;vocal tract length perturbation;stochastic feature
               mapping;automatic speech recognition",
  issn      = "2379-190X",
  doi       = "10.1109/ICASSP.2014.6854671"
}


@ARTICLE{Cao2014-mt,
  title    = "A parsimonious mixture of Gaussian trees model for oversampling
              in imbalanced and multimodal time-series classification",
  author   = "Cao, Hong and Tan, Vincent Y F and Pang, John Z F",
  abstract = "We propose a novel framework of using a parsimonious statistical
              model, known as mixture of Gaussian trees, for modeling the
              possibly multimodal minority class to solve the problem of
              imbalanced time-series classification. By exploiting the fact
              that close-by time points are highly correlated due to smoothness
              of the time-series, our model significantly reduces the number of
              covariance parameters to be estimated from O(d(2)) to O(Ld),
              where L is the number of mixture components and d is the
              dimensionality. Thus, our model is particularly effective for
              modeling high-dimensional time-series with limited number of
              instances in the minority positive class. In addition, the
              computational complexity for learning the model is only of the
              order O(Ln+d(2)) where n+ is the number of positively labeled
              samples. We conduct extensive classification experiments based on
              several well-known time-series data sets (both single- and
              multimodal) by first randomly generating synthetic instances from
              our learned mixture model to correct the imbalance. We then
              compare our results with several state-of-the-art oversampling
              techniques and the results demonstrate that when our proposed
              model is used in oversampling, the same support vector machines
              classifier achieves much better classification accuracy across
              the range of data sets. In fact, the proposed method achieves the
              best average performance 30 times out of 36 multimodal data sets
              according to the F-value metric. Our results are also highly
              competitive compared with nonoversampling-based classifiers for
              dealing with imbalanced time-series data sets.",
  journal  = "IEEE transactions on neural networks and learning systems",
  volume   =  25,
  number   =  12,
  pages    = "2226--2239",
  month    =  dec,
  year     =  2014,
  url      = "http://dx.doi.org/10.1109/TNNLS.2014.2308321",
  file     = "All Papers/CAO/Cao et al. 2014 - A parsimonious mixture of Gaussian trees model for oversampling in imbalanced and multimodal time-series classification.pdf",
  language = "en",
  issn     = "2162-2388, 2162-237X",
  pmid     = "25420245",
  doi      = "10.1109/TNNLS.2014.2308321"
}
