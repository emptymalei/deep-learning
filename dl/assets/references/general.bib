@article{dimensionsai,
 abstract = {Dimensions is a new scholarly search database that focuses on the broader set of use cases that academics now face. By including awarded grants, patents, and clinical trials alongside publication and Altmetric attention data, Dimensions goes beyond the standard publication-citation ecosystem to give the user a much greater sense of context of a piece of research. All entities in the graph may be linked to all other entities. Thus, a patent may be linked to a grant, if an appropriate reference is made. Books, book chapters, and conference proceedings are included in the publication index. All entities are treated as first-class objects and are mapped to a database of research institutions and a standard set of research classifications via machine-learning techniques. This article gives an overview of the methodology of construction of the Dimensions dataset and user interface.},
 author = {Hook, Daniel W. and Porter, Simon J. and Herzog, Christian},
 doi = {10.3389/frma.2018.00023},
 journal = {Frontiers in Research Metrics and Analytics},
 keywords = {},
 note = {https://www.frontiersin.org/articles/10.3389/frma.2018.00023/pdf},
 number = {},
 pages = {23},
 title = {Dimensions: Building Context for Search and Evaluation},
 url = {https://app.dimensions.ai/details/publication/pub.1106289502},
 volume = {3},
 year = {2018}
}


@online{Millidge2020-mr,
  title         = "Predictive Coding Approximates Backprop along Arbitrary
                   Computation Graphs",
  author        = "Millidge, Beren and Tschantz, Alexander and Buckley,
                   Christopher L",
  abstract      = "Backpropagation of error (backprop) is a powerful algorithm
                   for training machine learning architectures through
                   end-to-end differentiation. However, backprop is often
                   criticised for lacking biological plausibility. Recently, it
                   has been shown that backprop in multilayer-perceptrons
                   (MLPs) can be approximated using predictive coding, a
                   biologically-plausible process theory of cortical
                   computation which relies only on local and Hebbian updates.
                   The power of backprop, however, lies not in its
                   instantiation in MLPs, but rather in the concept of
                   automatic differentiation which allows for the optimisation
                   of any differentiable program expressed as a computation
                   graph. Here, we demonstrate that predictive coding converges
                   asymptotically (and in practice rapidly) to exact backprop
                   gradients on arbitrary computation graphs using only local
                   learning rules. We apply this result to develop a
                   straightforward strategy to translate core machine learning
                   architectures into their predictive coding equivalents. We
                   construct predictive coding CNNs, RNNs, and the more complex
                   LSTMs, which include a non-layer-like branching internal
                   graph structure and multiplicative interactions. Our models
                   perform equivalently to backprop on challenging machine
                   learning benchmarks, while utilising only local and (mostly)
                   Hebbian plasticity. Our method raises the potential that
                   standard machine learning algorithms could in principle be
                   directly implemented in neural circuitry, and may also
                   contribute to the development of completely distributed
                   neuromorphic architectures.",
  month         =  "7~" # jun,
  year          =  2020,
  url           = "http://arxiv.org/abs/2006.04182",
  file          = "All Papers/MILLIDGE/Millidge et al. 2020 - Predictive Coding Approximates Backprop along Arbitrary Computation Graphs.pdf",
  archivePrefix = "arXiv",
  eprint        = "2006.04182",
  primaryClass  = "cs.LG",
  arxivid       = "2006.04182"
}


@ARTICLE{Makridakis2022-hb,
  title    = "{M5} accuracy competition: Results, findings, and conclusions",
  author   = "Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos,
              Vassilios",
  abstract = "In this study, we present the results of the M5 ``Accuracy''
              competition, which was the first of two parallel challenges in
              the latest M competition with the aim of advancing the theory and
              practice of forecasting. The main objective in the M5
              ``Accuracy'' competition was to accurately predict 42,840 time
              series representing the hierarchical unit sales for the largest
              retail company in the world by revenue, Walmart. The competition
              required the submission of 30,490 point forecasts for the lowest
              cross-sectional aggregation level of the data, which could then
              be summed up accordingly to estimate forecasts for the remaining
              upward levels. We provide details of the implementation of the M5
              ``Accuracy'' challenge, as well as the results and best
              performing methods, and summarize the major findings and
              conclusions. Finally, we discuss the implications of these
              findings and suggest directions for future research.",
  journal  = "International journal of forecasting",
  volume   =  38,
  number   =  4,
  pages    = "1346--1364",
  month    =  "1~" # oct,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S0169207021001874",
  file     = "All Papers/MAKRIDAKIS/Makridakis et al. 2022 - M5 accuracy competition - Results, findings, and conclusions.pdf",
  keywords = "Forecasting competitions; M competitions; Accuracy; Time series;
              Machine learning; Retail sales forecasting",
  issn     = "0169-2070",
  doi      = "10.1016/j.ijforecast.2021.11.013"
}
